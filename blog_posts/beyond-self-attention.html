<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>transformer-experiments - Beyond Self-Attention: How a Small Language Model Predicts the Next Token</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="transformer-experiments - Beyond Self-Attention: How a Small Language Model Predicts the Next Token">
<meta property="og:description" content="Some experiments on transformer models">
<meta property="og:site_name" content="transformer-experiments">
<meta name="twitter:title" content="transformer-experiments - Beyond Self-Attention: How a Small Language Model Predicts the Next Token">
<meta name="twitter:description" content="Some experiments on transformer models">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">transformer-experiments</span>
    </a>
  </div>
        <div class="quarto-navbar-tools">
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../blog_posts/beyond-self-attention.html">blog_posts</a></li><li class="breadcrumb-item"><a href="../blog_posts/beyond-self-attention.html">Beyond Self-Attention: How a Small Language Model Predicts the Next Token</a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">transformer-experiments</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">analyses</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../analyses/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../analyses/clustering_block_intermediates.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Interpreting the Projection and Feed-Forward Layers in a Self-Attention Block</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../analyses/cosine_sim_intermediates.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Investigation of Cosine Similarity of Block Intermediates</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../analyses/similar_strings_deep_dive.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Dive into Similar Strings Progress through the Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../analyses/widening_similar_space.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Widening the Space of Similar Values</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../analyses/approximation_details.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Approximation Interpretation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../analyses/combining_token_subspaces.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Combining Token Subspaces</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../analyses/embedding_adjustments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Embedding Adjustments</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">blog_posts</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../blog_posts/beyond-self-attention.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Beyond Self-Attention: How a Small Language Model Predicts the Next Token</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">common</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../common/databatcher.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">databatcher</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../common/environments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">environments</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../common/substring-generator.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">substring-generator</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../common/svd-helpers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">svd-helpers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../common/text-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">text-analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../common/utils.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">utils</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">datasets</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../datasets/tinyshakespeare.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">tinyshakespeare.ipynb</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">experiments</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/alternate-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">alternate-models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/block-internals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">block-internals</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/cosine-sims.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">cosine-sims</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/final_ffwd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">final-ffwd</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/learn-embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">learn-embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/logit-lens.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">logit-lens</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../experiments/similar-strings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">similar-strings</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../models/transformer-helpers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">transformer-helpers.ipynb</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../models/transformer-training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">transformer-training</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../models/transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">transformer</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
 <span class="menu-text">tokenizers</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tokenizers/char-tokenizer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">char-tokenizer.ipynb</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
 <span class="menu-text">trained_models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../trained_models/tinyshakespeare-transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">tinyshakespeare-transformer</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">
 <span class="menu-text">training</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../training/dataset-split.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">dataset-split</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../training/training-utils.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">training-utils.ipynb</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-model-and-setup" id="toc-the-model-and-setup" class="nav-link active" data-scroll-target="#the-model-and-setup">The Model and Setup</a>
  <ul class="collapse">
  <li><a href="#model-overview" id="toc-model-overview" class="nav-link" data-scroll-target="#model-overview">Model Overview</a></li>
  <li><a href="#transformer-block-structure" id="toc-transformer-block-structure" class="nav-link" data-scroll-target="#transformer-block-structure">Transformer Block Structure</a></li>
  </ul></li>
  <li><a href="#demo-my-proposal-in-action" id="toc-demo-my-proposal-in-action" class="nav-link" data-scroll-target="#demo-my-proposal-in-action">Demo: My Proposal In Action</a></li>
  <li><a href="#implementation-approximating-the-transformer-output-with-feed-forward-network-outputs" id="toc-implementation-approximating-the-transformer-output-with-feed-forward-network-outputs" class="nav-link" data-scroll-target="#implementation-approximating-the-transformer-output-with-feed-forward-network-outputs">Implementation: Approximating the Transformer Output with Feed-forward Network Outputs</a>
  <ul class="collapse">
  <li><a href="#procedure-setup" id="toc-procedure-setup" class="nav-link" data-scroll-target="#procedure-setup">Procedure Setup</a></li>
  <li><a href="#procedure-walkthrough" id="toc-procedure-walkthrough" class="nav-link" data-scroll-target="#procedure-walkthrough">Procedure Walkthrough</a></li>
  <li><a href="#similarity-thresholds" id="toc-similarity-thresholds" class="nav-link" data-scroll-target="#similarity-thresholds">Similarity Thresholds</a></li>
  <li><a href="#going-beyond-the-first-block" id="toc-going-beyond-the-first-block" class="nav-link" data-scroll-target="#going-beyond-the-first-block">Going Beyond the First Block</a></li>
  <li><a href="#extending-to-all-20000-prompts" id="toc-extending-to-all-20000-prompts" class="nav-link" data-scroll-target="#extending-to-all-20000-prompts">Extending to All 20,000 Prompts</a></li>
  </ul></li>
  <li><a href="#evaluating-the-approximation" id="toc-evaluating-the-approximation" class="nav-link" data-scroll-target="#evaluating-the-approximation">Evaluating the Approximation</a></li>
  <li><a href="#interpretation-why-does-the-approximation-work" id="toc-interpretation-why-does-the-approximation-work" class="nav-link" data-scroll-target="#interpretation-why-does-the-approximation-work">Interpretation: Why Does the Approximation Work?</a>
  <ul class="collapse">
  <li><a href="#the-model-is-a-series-of-transformations" id="toc-the-model-is-a-series-of-transformations" class="nav-link" data-scroll-target="#the-model-is-a-series-of-transformations">The Model is a Series of Transformations</a></li>
  <li><a href="#transformation-via-vector-addition" id="toc-transformation-via-vector-addition" class="nav-link" data-scroll-target="#transformation-via-vector-addition">Transformation via Vector Addition</a></li>
  <li><a href="#token-subspaces" id="toc-token-subspaces" class="nav-link" data-scroll-target="#token-subspaces">Token Subspaces</a></li>
  <li><a href="#putting-it-all-together" id="toc-putting-it-all-together" class="nav-link" data-scroll-target="#putting-it-all-together">Putting it All Together</a></li>
  <li><a href="#final-summary-of-correspondence-between-transformer-and-approximation" id="toc-final-summary-of-correspondence-between-transformer-and-approximation" class="nav-link" data-scroll-target="#final-summary-of-correspondence-between-transformer-and-approximation">Final Summary of Correspondence Between Transformer and Approximation</a></li>
  <li><a href="#what-about-attention" id="toc-what-about-attention" class="nav-link" data-scroll-target="#what-about-attention">What About Attention?</a></li>
  </ul></li>
  <li><a href="#closing-thoughts" id="toc-closing-thoughts" class="nav-link" data-scroll-target="#closing-thoughts">Closing Thoughts</a></li>
  <li><a href="#appendices" id="toc-appendices" class="nav-link" data-scroll-target="#appendices">Appendices</a>
  <ul class="collapse">
  <li><a href="#i-model-details" id="toc-i-model-details" class="nav-link" data-scroll-target="#i-model-details">I: Model Details</a></li>
  <li><a href="#ii-evaluation-of-main-model-vs-3-alternate-models" id="toc-ii-evaluation-of-main-model-vs-3-alternate-models" class="nav-link" data-scroll-target="#ii-evaluation-of-main-model-vs-3-alternate-models">II: Evaluation of Main Model vs 3 Alternate Models</a></li>
  <li><a href="#iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm" id="toc-iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm" class="nav-link" data-scroll-target="#iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">III: The Output Embedding’s Norm Doesn’t Matter Because of the Final LayerNorm</a></li>
  <li><a href="#iv-summary-of-experiment-on-relative-impact-of-self-attention-and-feed-forward-network-outputs" id="toc-iv-summary-of-experiment-on-relative-impact-of-self-attention-and-feed-forward-network-outputs" class="nav-link" data-scroll-target="#iv-summary-of-experiment-on-relative-impact-of-self-attention-and-feed-forward-network-outputs">IV: Summary of Experiment on Relative Impact of Self-Attention and Feed Forward Network Outputs</a></li>
  <li><a href="#v-performing-svd-to-get-a-linear-approximation-of-a-token-subspace" id="toc-v-performing-svd-to-get-a-linear-approximation-of-a-token-subspace" class="nav-link" data-scroll-target="#v-performing-svd-to-get-a-linear-approximation-of-a-token-subspace">V: Performing SVD to Get a Linear Approximation of a Token Subspace</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/spather/transformer-experiments/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../blog_posts/beyond-self-attention.html">blog_posts</a></li><li class="breadcrumb-item"><a href="../blog_posts/beyond-self-attention.html">Beyond Self-Attention: How a Small Language Model Predicts the Next Token</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Beyond Self-Attention: How a Small Language Model Predicts the Next Token</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<p>I trained a small (~10 million parameter) <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">transformer</a> following <a href="https://karpathy.ai/">Andrej Karpathy</a>’s excellent tutorial, <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Let’s build GPT: from scratch, in code, spelled out</a>. After getting it working, I wanted to understand, as deeply as possible, what it was doing internally and how it produced its results.</p>
<p>The <a href="https://arxiv.org/abs/1706.03762">original paper</a>, as well every transformer tutorial I found, focuses primarily on <a href="https://machinelearningmastery.com/the-transformer-attention-mechanism/">multi-head self-attention</a>, the mechanism by which transformers learn multiple relationships between tokens without relying on recurrences or convolution. But none of the papers or tutorials I encountered give a satisfying explanation of what happens <em>after attention</em>: <strong>how exactly do the results of the attention computation turn into accurate predictions for the next token?</strong></p>
<p>I thought I could run a few example prompts through the small but working transformer I’d trained, examine the internal states, and figure this out. What I thought would be a quick investigation turned out to be a 6-month deep dive, but yielded some results I think are worth sharing. Specifically, I have a working theory that explains how the transformer produces its predictions and some empirical evidence that suggests this explanation is at least plausible.</p>
<p>For those readers familiar with transformers and eager for the punchline, here it is: Each transformer block (containing a multi-head self-attention layer and feed-forward network) learns weights that associate a given prompt with a class of strings found in the training corpus. <strong>The distribution of tokens that follow those strings in the training corpus is, approximately, what the block outputs as its predictions for the next token.</strong> Each block may associate the same prompt with a different class of training corpus strings, resulting in a different distribution of next tokens and thus different predictions. The final transformer output is a linear combination of each block’s predictions.</p>
<p>I implemented imperative code that does what I’m proposing the transformer is doing. It produces outputs very similar to the transformer, which I’ll review in detail in a <a href="#evaluating-the-approximation">later section</a>.</p>
<p>In this post, I’m going to briefly introduce the model and training data, demo some evidence for my proposed explanation, give a detailed walkthrough of the imperative code implementation of it, and present the supporting evidence I have for my theory. I’ve tried to keep the main narrative succinct, with links to relevant technical details and justifications in the <a href="#appendices">appendices</a> or other notebooks in the <a href="https://github.com/spather/transformer-experiments">repo</a>.</p>
<blockquote class="blockquote">
<p>This project is my first foray into this type of open-ended ML research. I’m sure I have made errors or omissions that would be obvious to more experienced researchers. I welcome any feedback on this work at <code>shyam.pather at gmail dot com</code>.</p>
</blockquote>
<blockquote class="blockquote">
<p>This post was <a href="https://news.ycombinator.com/item?id=39251909">discussed on HackerNews</a> on February 4, 2024.</p>
</blockquote>
<section id="the-model-and-setup" class="level2">
<h2 class="anchored" data-anchor-id="the-model-and-setup">The Model and Setup</h2>
<blockquote class="blockquote">
<h3 id="disclaimer" class="anchored">Disclaimer</h3>
<p>I want to start by saying upfront: the code for the model I trained isn’t mine. It came from <a href="https://karpathy.ai/">Andrej Karpathy</a>’s video, <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Let’s build GPT: from scratch, in code, spelled out</a> (highly recommend).</p>
<p>I typed in the code by copying what I saw on the screen as I watched the video. For things that weren’t clear onscreen, I referenced the <a href="https://github.com/karpathy/ng-video-lecture">GitHub repo for the video</a> and the <a href="https://github.com/karpathy/nanoGPT">nanoGPT repo</a>. After getting it working, I made only minor changes to make it work with the rest of the code in/structure of <a href="https://github.com/spather/transformer-experiments">my repository</a>, resulting in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer.ipynb">this implementation</a>. In summary: the core language model is Andrej Karpathy’s work, not mine. The analysis and all the supporting code behind it are my original contributions. I’ll acknowledge and cite influential papers, posts, tutorials, and other resources in the relevant places.</p>
</blockquote>
<section id="model-overview" class="level3">
<h3 class="anchored" data-anchor-id="model-overview">Model Overview</h3>
<p>The model is a 6-block, decoder-only <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">transformer</a>:</p>
<div id="cell-5" class="cell">
<details class="code-fold">
<summary>Drawing code for architecture diagram</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ArchDiagram(Scene):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> construct(<span class="va">self</span>):</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Instantiate the model and tokenizer, which we'll use to</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># embed prompts.</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        ts <span class="op">=</span> TinyShakespeareDataSet(cache_file<span class="op">=</span>environment.code_root <span class="op">/</span> <span class="st">'nbs/artifacts/input.txt'</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        m, tokenizer <span class="op">=</span> create_model_and_tokenizer(</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>            saved_model_filename<span class="op">=</span>environment.code_root <span class="op">/</span> <span class="st">'nbs/artifacts/shakespeare-20231112.pt'</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>            dataset<span class="op">=</span>ts,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>            device<span class="op">=</span>device,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        encoding_helpers <span class="op">=</span> EncodingHelpers(tokenizer, device)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">=</span> <span class="st">'ROMEO:'</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> MobjectTable(</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>            [[Text(<span class="ss">f'</span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">'</span>, font<span class="op">=</span><span class="st">'Arial'</span>) <span class="cf">for</span> t <span class="kw">in</span> tokenizer.encode(prompt)]],</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>            include_outer_lines<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        ).scale(<span class="fl">0.25</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> VGroup(</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>            Text(<span class="st">'Input Tokens'</span>, font<span class="op">=</span><span class="st">'Arial'</span>).scale(<span class="fl">0.4</span>).next_to(tokens, UP, buff<span class="op">=</span><span class="fl">0.1</span>),</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>            tokens,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        token_emb <span class="op">=</span> m.token_embedding_table(encoding_helpers.tokenize_string(prompt))</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        token_emb <span class="op">=</span> MobjectTable(</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>            [[Text(<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">:.1f}</span><span class="ss">'</span>, font<span class="op">=</span><span class="st">'Arial'</span>) <span class="cf">for</span> x <span class="kw">in</span> token_emb[<span class="dv">0</span>, :, row]] <span class="cf">for</span> row <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">2</span>)]</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>            <span class="op">+</span> [[Text(<span class="st">'...'</span>, font<span class="op">=</span><span class="st">'Arial'</span>).rotate(PI<span class="op">/</span><span class="dv">2</span>) <span class="cf">for</span> _ <span class="kw">in</span> prompt]]</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>            <span class="op">+</span> [</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>                [Text(<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">:.1f}</span><span class="ss">'</span>, font<span class="op">=</span><span class="st">'Arial'</span>) <span class="cf">for</span> x <span class="kw">in</span> token_emb[<span class="dv">0</span>, :, row]]</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> row <span class="kw">in</span> <span class="bu">range</span>(n_embed <span class="op">-</span> <span class="dv">2</span>, n_embed)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>            ],</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>            include_outer_lines<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        ).scale(<span class="fl">0.25</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        pos_emb <span class="op">=</span> m.position_embedding_table(</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>            torch.arange(<span class="dv">0</span>, <span class="bu">len</span>(prompt), device<span class="op">=</span>device)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        pos_emb <span class="op">=</span> MobjectTable(</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>            [[Text(<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">:.1f}</span><span class="ss">'</span>, font<span class="op">=</span><span class="st">'Arial'</span>) <span class="cf">for</span> x <span class="kw">in</span> pos_emb[:, row]] <span class="cf">for</span> row <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">2</span>)]</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>            <span class="op">+</span> [[Text(<span class="st">'...'</span>, font<span class="op">=</span><span class="st">'Arial'</span>).rotate(PI<span class="op">/</span><span class="dv">2</span>) <span class="cf">for</span> _ <span class="kw">in</span> prompt]]</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>            <span class="op">+</span> [</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>                [Text(<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">:.1f}</span><span class="ss">'</span>, font<span class="op">=</span><span class="st">'Arial'</span>) <span class="cf">for</span> x <span class="kw">in</span> pos_emb[:, row]]</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> row <span class="kw">in</span> <span class="bu">range</span>(n_embed <span class="op">-</span> <span class="dv">2</span>, n_embed)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>            ],</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>            include_outer_lines<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        ).scale(<span class="fl">0.25</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>        embs <span class="op">=</span> VGroup(</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>            token_emb,</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>            pos_emb,</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        ).arrange(RIGHT, buff<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>        embs <span class="op">=</span> VGroup(</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>            tokens,</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>            embs,</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>        ).arrange(DOWN, buff<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>        emb_labels <span class="op">=</span> VGroup(</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>            Text(<span class="st">'Token Embeddings'</span>, font<span class="op">=</span><span class="st">'Arial'</span>)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>            .scale(<span class="fl">0.4</span>)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>            .next_to(token_emb, DOWN, buff<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>            .align_to(token_emb, LEFT),</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>            Text(<span class="st">'Position Embeddings'</span>, font<span class="op">=</span><span class="st">'Arial'</span>)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>            .scale(<span class="fl">0.4</span>)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>            .next_to(pos_emb, DOWN, buff<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>            .align_to(pos_emb, RIGHT),</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>        blocks1_2 <span class="op">=</span> [Rectangle(height<span class="op">=</span><span class="fl">0.4</span>, width<span class="op">=</span><span class="dv">3</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)]</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>        vert_ellipsis <span class="op">=</span> VGroup(</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span>[Circle(radius<span class="op">=</span><span class="fl">0.025</span>, color<span class="op">=</span>WHITE) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>)],</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>        ).arrange(direction<span class="op">=</span>DOWN, buff<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>        block6 <span class="op">=</span> Rectangle(height<span class="op">=</span><span class="fl">0.4</span>, width<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>        blocks <span class="op">=</span> VGroup(</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span>blocks1_2,</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>            vert_ellipsis,</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>            block6,</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>        ).arrange(DOWN, buff<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>        empty_rect <span class="op">=</span> Rectangle(height<span class="op">=</span><span class="fl">0.25</span>, width<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span>BLACK)</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>        out_blocks <span class="op">=</span> [Rectangle(height<span class="op">=</span><span class="fl">0.4</span>, width<span class="op">=</span><span class="dv">3</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>)]</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>        arch <span class="op">=</span> VGroup(</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>            embs,</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>            emb_labels,</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>            blocks,</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>            empty_rect,</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span>out_blocks,</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>            empty_rect.copy()</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>        ).arrange(DOWN, buff<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(arch)</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add annotations</span></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Lines and arrows between tokens and embedding tables</span></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>        left_line <span class="op">=</span> Line(start<span class="op">=</span>tokens[<span class="dv">1</span>].get_left(), end<span class="op">=</span>(tokens[<span class="dv">1</span>].get_left() <span class="op">*</span> [<span class="fl">0.</span>, <span class="fl">1.</span>, <span class="fl">0.</span>]) <span class="op">+</span> (token_emb.get_center() <span class="op">*</span> [<span class="fl">1.</span>, <span class="fl">0.</span>, <span class="fl">0.</span>]))</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(left_line)</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>        left_arrow_down <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.15</span>)</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>        left_arrow_down.put_start_and_end_on(</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>left_line.get_left(),</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>left_line.get_left() <span class="op">*</span> [<span class="fl">1.</span>, <span class="fl">0.</span>, <span class="fl">0.</span>] <span class="op">+</span> token_emb.get_top() <span class="op">*</span> [<span class="fl">0.</span>, <span class="fl">1.</span>, <span class="fl">0.</span>],</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(left_arrow_down)</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>        right_line <span class="op">=</span> Line(start<span class="op">=</span>tokens[<span class="dv">1</span>].get_right(), end<span class="op">=</span>(tokens[<span class="dv">1</span>].get_right() <span class="op">*</span> [<span class="fl">0.</span>, <span class="fl">1.</span>, <span class="fl">0.</span>]) <span class="op">+</span> (pos_emb.get_center() <span class="op">*</span> [<span class="fl">1.</span>, <span class="fl">0.</span>, <span class="fl">0.</span>]))</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(right_line)</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>        right_arrow_down <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.15</span>)</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>        right_arrow_down.put_start_and_end_on(</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>right_line.get_right(),</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>right_line.get_right() <span class="op">*</span> [<span class="fl">1.</span>, <span class="fl">0.</span>, <span class="fl">0.</span>] <span class="op">+</span> pos_emb.get_top() <span class="op">*</span> [<span class="fl">0.</span>, <span class="fl">1.</span>, <span class="fl">0.</span>],</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(right_arrow_down)</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Circle with the plus between the embedding tables</span></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>        circle <span class="op">=</span> (</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>            Circle(radius<span class="op">=</span><span class="fl">0.4</span>, color<span class="op">=</span>WHITE)</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>            .move_to((token_emb.get_right() <span class="op">+</span> pos_emb.get_left()) <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(circle)</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>        plus <span class="op">=</span> Text(<span class="st">'+'</span>, font<span class="op">=</span><span class="st">'Arial'</span>).move_to(circle.get_center())</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(plus)</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrows from embedding tables to circled plus</span></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>        right_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.15</span>)</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>        right_arrow.put_start_and_end_on(</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>token_emb.get_right(),</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>circle.get_left()</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(right_arrow)</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>        left_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.15</span>)</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>        left_arrow.put_start_and_end_on(</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>pos_emb.get_left(),</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>circle.get_right()</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(left_arrow)</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow down to the first block</span></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>        down_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.15</span>)</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>        down_arrow.put_start_and_end_on(</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>circle.get_bottom(),</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>blocks1_2[<span class="dv">0</span>].get_top(),</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(down_arrow)</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Block labels</span></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>        blocks1_2_labels <span class="op">=</span> [Text(<span class="ss">f'Block </span><span class="sc">{</span>b<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>, font<span class="op">=</span><span class="st">'Arial'</span>).move_to(blocks1_2[b].get_center()).scale(<span class="fl">0.4</span>) <span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)]</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(<span class="op">*</span>blocks1_2_labels)</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>        block6_label <span class="op">=</span> Text(<span class="st">'Block 6'</span>, font<span class="op">=</span><span class="st">'Arial'</span>).move_to(block6.get_center()).scale(<span class="fl">0.4</span>)</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(block6_label)</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output blocks labels</span></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> [<span class="st">'Layer Norm'</span>, <span class="st">'Linear'</span>, <span class="st">'Softmax'</span>]</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>        out_block_labels <span class="op">=</span> [Text(<span class="ss">f'</span><span class="sc">{</span>l<span class="sc">}</span><span class="ss">'</span>, font<span class="op">=</span><span class="st">'Arial'</span>).move_to(out_blocks[b].get_center()).scale(<span class="fl">0.4</span>) <span class="cf">for</span> b, l <span class="kw">in</span> <span class="bu">enumerate</span>(labels)]</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(<span class="op">*</span>out_block_labels)</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow between last block and output blocks</span></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>        out_block_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>)</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>        out_block_arrow.put_start_and_end_on(</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>block6.get_bottom(),</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>out_blocks[<span class="dv">0</span>].get_top(),</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(out_block_arrow)</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Text at the bottom</span></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>        out_prob_text <span class="op">=</span> Text(<span class="st">'Output Probabilities'</span>, font<span class="op">=</span><span class="st">'Arial'</span>).scale(<span class="fl">0.4</span>).next_to(out_blocks[<span class="op">-</span><span class="dv">1</span>], DOWN, buff<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(out_prob_text)</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow between softmax and output probs</span></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>        out_prob_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>)</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>        out_prob_arrow.put_start_and_end_on(</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>out_blocks[<span class="op">-</span><span class="dv">1</span>].get_bottom(),</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>out_prob_text.get_top(),</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(out_prob_arrow)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-6" class="cell">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It’s trained on the <a href="https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt">TinyShakespeare data set</a> which contains 40,000 lines of Shakespeare’s plays. After about an hour of training on an RTX 4000 GPU, it is able to produce reasonable-looking faux Shakespeare.</p>
<p>Given a prompt, the model predicts tokens that it thinks should follow. Let’s look at an example: starting with the prompt, <code>ROMEO:</code>, and sampling 500 tokens from the model’s predictions, we get:</p>
<div id="cell-9" class="cell">
<details class="code-fold">
<summary>Code to spin up model and generate example output from prompt, ‘ROMEO:’</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>ts <span class="op">=</span> TinyShakespeareDataSet(cache_file<span class="op">=</span>environment.code_root <span class="op">/</span> <span class="st">'nbs/artifacts/input.txt'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>m, tokenizer <span class="op">=</span> create_model_and_tokenizer(</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    saved_model_filename<span class="op">=</span>environment.code_root <span class="op">/</span> <span class="st">'nbs/artifacts/shakespeare-20231112.pt'</span>,</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    dataset<span class="op">=</span>ts,</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>device,</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>encoding_helpers <span class="op">=</span> EncodingHelpers(tokenizer, device)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>accessors <span class="op">=</span> TransformerAccessors(m, device)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">2321</span>) <span class="co"># Keep the output deterministic across runs</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">'ROMEO:'</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> encoding_helpers.tokenize_string(prompt)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.decode(m.generate(tokens, max_new_tokens<span class="op">=</span><span class="dv">500</span>)[<span class="dv">0</span>].tolist()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>ROMEO:
If thou wilt triumphant be virtue, and since from any
bold virtue that is made a bawd of earth, then the
duke desires of patience and perish:
take up the other husband, dislike his tent
back.

First Citizen:
Ourself goes, go back: you have no consul, but the disguised gods.

Second Citizen:
We choose him in the world, he did runk itself.

First Citizen:
Sir, I am I a man changed him and thriving, I have heard the
king.

CORIOLANUS:
Consider him!

AUFIDIUS:
Most gracious irice, and you must danc</code></pre>
</div>
</div>
<p>It’s not Shakespeare but structurally, it’s plausible Shakespeare. It looks like the script for a play, the language sounds archaic, the character names/titles come from real Shakespeare plays. Most of the words are English words. Punctuation and capitalization are mostly sensible. Clearly, none of the text actually makes sense, but still, it’s not bad for an hour of training.</p>
<p>The <strong>tokens in the model are characters</strong>, not words. Given a prompt, the model predicts a probability distribution for the next character. For example, given the prompt <code>'my most gr</code>, the model predicts these probabilities for the next token:</p>
<div id="cell-12" class="cell">
<details class="code-fold">
<summary>Code to display probabilities for next token after <code>my most gr</code></summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">'my most gr'</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> encoding_helpers.tokenize_string(prompt)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>logits, _ <span class="op">=</span> m(tokens)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> LogitsWrapper(logits.detach(), tokenizer)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> token, prob <span class="kw">in</span> logits.topk_tokens(k<span class="op">=</span><span class="dv">10</span>)[<span class="dv">0</span>][<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span><span class="bu">repr</span>(token)<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>prob<span class="sc">:.3f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>'a' 0.819
'e' 0.081
'i' 0.059
'o' 0.036
'u' 0.004
'y' 0.001
'w' 0.000
'r' 0.000
'g' 0.000
's' 0.000</code></pre>
</div>
</div>
<p><a href="#i-model-details">Appendix I</a> provides a few more details about the model. Beyond that, if you want to know more, <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer.ipynb">the code</a> and <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Andrej’s video</a> are the best resources.</p>
</section>
<section id="transformer-block-structure" class="level3">
<h3 class="anchored" data-anchor-id="transformer-block-structure">Transformer Block Structure</h3>
<p>Each of the 6 blocks in the architecture diagram above contains two significant sub-components: a multi-head self-attention layer and a feed-forward network, wired together via a mix of direct and residual connections as follows:</p>
<div id="cell-16" class="cell">
<details class="code-fold">
<summary>Drawing code for block internals architecture diagram</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BlockArchDiagram(Scene):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> construct(<span class="va">self</span>):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        input_emb <span class="op">=</span> Rectangle(height<span class="op">=</span><span class="fl">0.4</span>, width<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        input_emb_label <span class="op">=</span> (</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>            Text(<span class="st">'Block Input'</span>, font<span class="op">=</span><span class="st">'Arial'</span>).move_to(input_emb.get_center()).scale(<span class="fl">0.4</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        input_emb <span class="op">=</span> VGroup(input_emb, input_emb_label)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        ln1 <span class="op">=</span> Rectangle(height<span class="op">=</span><span class="fl">0.4</span>, width<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        ln1_label <span class="op">=</span> Text(<span class="st">'Layer Norm'</span>, font<span class="op">=</span><span class="st">'Arial'</span>).move_to(ln1.get_center()).scale(<span class="fl">0.4</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        ln1 <span class="op">=</span> VGroup(ln1, ln1_label)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        sa <span class="op">=</span> Rectangle(height<span class="op">=</span><span class="fl">0.6</span>, width<span class="op">=</span><span class="fl">3.5</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        sa_label <span class="op">=</span> Text(<span class="st">'Multi-Head Self-Attention'</span>, font<span class="op">=</span><span class="st">'Arial'</span>).move_to(sa.get_center()).scale(<span class="fl">0.4</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        sa <span class="op">=</span> VGroup(sa, sa_label)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        stack1 <span class="op">=</span> VGroup(ln1, sa).arrange(DOWN, buff<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        first_plus <span class="op">=</span> <span class="va">self</span>.circled_plus(radius<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        ln2 <span class="op">=</span> Rectangle(height<span class="op">=</span><span class="fl">0.4</span>, width<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>        ln2_label <span class="op">=</span> Text(<span class="st">'Layer Norm'</span>, font<span class="op">=</span><span class="st">'Arial'</span>).move_to(ln2.get_center()).scale(<span class="fl">0.4</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>        ln2 <span class="op">=</span> VGroup(ln2, ln2_label)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>        ffwd <span class="op">=</span> Rectangle(height<span class="op">=</span><span class="fl">0.6</span>, width<span class="op">=</span><span class="fl">3.5</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>        ffwd_label <span class="op">=</span> Text(<span class="st">'Feed Forward'</span>, font<span class="op">=</span><span class="st">'Arial'</span>).move_to(ffwd.get_center()).scale(<span class="fl">0.4</span>)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>        ffwd <span class="op">=</span> VGroup(ffwd, ffwd_label)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>        stack2 <span class="op">=</span> VGroup(ln2, ffwd).arrange(DOWN, buff<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>        empty_rect <span class="op">=</span> Rectangle(height<span class="op">=</span><span class="fl">0.2</span>, width<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span>BLACK)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>        second_plus <span class="op">=</span> <span class="va">self</span>.circled_plus(radius<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>            VGroup(</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>                empty_rect,</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>                input_emb,</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>                stack1,</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>                first_plus,</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>                empty_rect.copy(),</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>                stack2,</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>                second_plus,</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>                empty_rect.copy(),</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>            ).arrange(DOWN, buff<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>        stack1.shift(LEFT <span class="op">*</span> <span class="dv">2</span>)</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>        stack2.shift(LEFT <span class="op">*</span> <span class="dv">2</span>)</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add annotations</span></span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>        arrow_stroke_width <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow from input emb to ln1</span></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.arrow_via_points(</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>                [</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>                    input_emb.get_left(),</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>                    ln1.get_top() <span class="op">*</span> [<span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>                    <span class="op">+</span> input_emb.get_left() <span class="op">*</span> [<span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>],</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>                    ln1.get_top(),</span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>                ],</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>                stroke_width<span class="op">=</span>arrow_stroke_width,</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow from ln1 to sa</span></span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(</span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.arrow_via_points(</span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a>                [</span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a>                    ln1.get_bottom(),</span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a>                    sa.get_top(),</span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a>                ],</span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a>                stroke_width<span class="op">=</span><span class="dv">2</span> <span class="op">*</span> arrow_stroke_width,</span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow from sa to first plus</span></span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(</span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.arrow_via_points(</span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a>                [</span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a>                    sa.get_bottom(),</span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a>                    sa.get_bottom() <span class="op">*</span> [<span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]</span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a>                    <span class="op">+</span> first_plus.get_left() <span class="op">*</span> [<span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>],</span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a>                    first_plus.get_left(),</span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a>                ],</span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a>                stroke_width<span class="op">=</span>arrow_stroke_width,</span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow from input emb to circled plus</span></span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(</span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.arrow_via_points(</span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a>                [</span>
<span id="cb6-94"><a href="#cb6-94" aria-hidden="true" tabindex="-1"></a>                    input_emb.get_right(),</span>
<span id="cb6-95"><a href="#cb6-95" aria-hidden="true" tabindex="-1"></a>                    ln1.get_top() <span class="op">*</span> [<span class="op">-</span><span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]</span>
<span id="cb6-96"><a href="#cb6-96" aria-hidden="true" tabindex="-1"></a>                    <span class="op">+</span> input_emb.get_right() <span class="op">*</span> [<span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>],</span>
<span id="cb6-97"><a href="#cb6-97" aria-hidden="true" tabindex="-1"></a>                    ln1.get_top() <span class="op">*</span> [<span class="op">-</span><span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]</span>
<span id="cb6-98"><a href="#cb6-98" aria-hidden="true" tabindex="-1"></a>                    <span class="op">+</span> first_plus.get_right() <span class="op">*</span> [<span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>],</span>
<span id="cb6-99"><a href="#cb6-99" aria-hidden="true" tabindex="-1"></a>                    first_plus.get_right(),</span>
<span id="cb6-100"><a href="#cb6-100" aria-hidden="true" tabindex="-1"></a>                ],</span>
<span id="cb6-101"><a href="#cb6-101" aria-hidden="true" tabindex="-1"></a>                stroke_width<span class="op">=</span>arrow_stroke_width,</span>
<span id="cb6-102"><a href="#cb6-102" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb6-103"><a href="#cb6-103" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-104"><a href="#cb6-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-105"><a href="#cb6-105" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow from first plus to ln2</span></span>
<span id="cb6-106"><a href="#cb6-106" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(</span>
<span id="cb6-107"><a href="#cb6-107" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.arrow_via_points(</span>
<span id="cb6-108"><a href="#cb6-108" aria-hidden="true" tabindex="-1"></a>                [</span>
<span id="cb6-109"><a href="#cb6-109" aria-hidden="true" tabindex="-1"></a>                    first_plus.get_bottom(),</span>
<span id="cb6-110"><a href="#cb6-110" aria-hidden="true" tabindex="-1"></a>                    first_plus.get_bottom()</span>
<span id="cb6-111"><a href="#cb6-111" aria-hidden="true" tabindex="-1"></a>                    <span class="op">+</span> (ln2.get_top() <span class="op">-</span> first_plus.get_bottom()) <span class="op">*</span> [<span class="fl">0.0</span>, <span class="fl">0.5</span>, <span class="fl">0.0</span>],</span>
<span id="cb6-112"><a href="#cb6-112" aria-hidden="true" tabindex="-1"></a>                    ln2.get_top()</span>
<span id="cb6-113"><a href="#cb6-113" aria-hidden="true" tabindex="-1"></a>                    <span class="op">-</span> (ln2.get_top() <span class="op">-</span> first_plus.get_bottom()) <span class="op">*</span> [<span class="fl">0.0</span>, <span class="fl">0.5</span>, <span class="fl">0.0</span>],</span>
<span id="cb6-114"><a href="#cb6-114" aria-hidden="true" tabindex="-1"></a>                    ln2.get_top(),</span>
<span id="cb6-115"><a href="#cb6-115" aria-hidden="true" tabindex="-1"></a>                ],</span>
<span id="cb6-116"><a href="#cb6-116" aria-hidden="true" tabindex="-1"></a>                stroke_width<span class="op">=</span>arrow_stroke_width,</span>
<span id="cb6-117"><a href="#cb6-117" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb6-118"><a href="#cb6-118" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-119"><a href="#cb6-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-120"><a href="#cb6-120" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow from ln2 to ffwd</span></span>
<span id="cb6-121"><a href="#cb6-121" aria-hidden="true" tabindex="-1"></a>        ln2_ffwd_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>)</span>
<span id="cb6-122"><a href="#cb6-122" aria-hidden="true" tabindex="-1"></a>        ln2_ffwd_arrow.put_start_and_end_on(</span>
<span id="cb6-123"><a href="#cb6-123" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>ln2.get_bottom(),</span>
<span id="cb6-124"><a href="#cb6-124" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>ffwd.get_top(),</span>
<span id="cb6-125"><a href="#cb6-125" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-126"><a href="#cb6-126" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(ln2_ffwd_arrow)</span>
<span id="cb6-127"><a href="#cb6-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-128"><a href="#cb6-128" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow from ffwd to second plus</span></span>
<span id="cb6-129"><a href="#cb6-129" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(</span>
<span id="cb6-130"><a href="#cb6-130" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.arrow_via_points(</span>
<span id="cb6-131"><a href="#cb6-131" aria-hidden="true" tabindex="-1"></a>                [</span>
<span id="cb6-132"><a href="#cb6-132" aria-hidden="true" tabindex="-1"></a>                    ffwd.get_bottom(),</span>
<span id="cb6-133"><a href="#cb6-133" aria-hidden="true" tabindex="-1"></a>                    ffwd.get_bottom() <span class="op">*</span> [<span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>]</span>
<span id="cb6-134"><a href="#cb6-134" aria-hidden="true" tabindex="-1"></a>                    <span class="op">+</span> second_plus.get_left() <span class="op">*</span> [<span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>],</span>
<span id="cb6-135"><a href="#cb6-135" aria-hidden="true" tabindex="-1"></a>                    second_plus.get_left(),</span>
<span id="cb6-136"><a href="#cb6-136" aria-hidden="true" tabindex="-1"></a>                ],</span>
<span id="cb6-137"><a href="#cb6-137" aria-hidden="true" tabindex="-1"></a>                stroke_width<span class="op">=</span>arrow_stroke_width,</span>
<span id="cb6-138"><a href="#cb6-138" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb6-139"><a href="#cb6-139" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-140"><a href="#cb6-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-141"><a href="#cb6-141" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow from first plus to second plus</span></span>
<span id="cb6-142"><a href="#cb6-142" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(</span>
<span id="cb6-143"><a href="#cb6-143" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.arrow_via_points(</span>
<span id="cb6-144"><a href="#cb6-144" aria-hidden="true" tabindex="-1"></a>                [</span>
<span id="cb6-145"><a href="#cb6-145" aria-hidden="true" tabindex="-1"></a>                    first_plus.get_bottom()</span>
<span id="cb6-146"><a href="#cb6-146" aria-hidden="true" tabindex="-1"></a>                    <span class="op">+</span> (ln2.get_top() <span class="op">-</span> first_plus.get_bottom()) <span class="op">*</span> [<span class="fl">0.0</span>, <span class="fl">0.5</span>, <span class="fl">0.0</span>],</span>
<span id="cb6-147"><a href="#cb6-147" aria-hidden="true" tabindex="-1"></a>                    second_plus.get_top(),</span>
<span id="cb6-148"><a href="#cb6-148" aria-hidden="true" tabindex="-1"></a>                ],</span>
<span id="cb6-149"><a href="#cb6-149" aria-hidden="true" tabindex="-1"></a>                stroke_width<span class="op">=</span><span class="dv">2</span> <span class="op">*</span> arrow_stroke_width,</span>
<span id="cb6-150"><a href="#cb6-150" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb6-151"><a href="#cb6-151" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-152"><a href="#cb6-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-153"><a href="#cb6-153" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow from second plus to output</span></span>
<span id="cb6-154"><a href="#cb6-154" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(</span>
<span id="cb6-155"><a href="#cb6-155" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.arrow_via_points(</span>
<span id="cb6-156"><a href="#cb6-156" aria-hidden="true" tabindex="-1"></a>                [</span>
<span id="cb6-157"><a href="#cb6-157" aria-hidden="true" tabindex="-1"></a>                    second_plus.get_bottom(),</span>
<span id="cb6-158"><a href="#cb6-158" aria-hidden="true" tabindex="-1"></a>                    second_plus.get_bottom() <span class="op">+</span> DOWN <span class="op">*</span> <span class="fl">0.4</span>,</span>
<span id="cb6-159"><a href="#cb6-159" aria-hidden="true" tabindex="-1"></a>                ],</span>
<span id="cb6-160"><a href="#cb6-160" aria-hidden="true" tabindex="-1"></a>                stroke_width<span class="op">=</span><span class="dv">2</span> <span class="op">*</span> arrow_stroke_width,</span>
<span id="cb6-161"><a href="#cb6-161" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb6-162"><a href="#cb6-162" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-163"><a href="#cb6-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-164"><a href="#cb6-164" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> arrow_via_points(<span class="va">self</span>, points: Iterable[np.ndarray], stroke_width: <span class="bu">float</span> <span class="op">=</span> <span class="fl">2.0</span>):</span>
<span id="cb6-165"><a href="#cb6-165" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> (</span>
<span id="cb6-166"><a href="#cb6-166" aria-hidden="true" tabindex="-1"></a>            <span class="bu">len</span>(points) <span class="op">&gt;=</span> <span class="dv">2</span></span>
<span id="cb6-167"><a href="#cb6-167" aria-hidden="true" tabindex="-1"></a>        ), <span class="st">'Must provide at least two points to draw an arrow between'</span></span>
<span id="cb6-168"><a href="#cb6-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-169"><a href="#cb6-169" aria-hidden="true" tabindex="-1"></a>        lines <span class="op">=</span> VGroup()</span>
<span id="cb6-170"><a href="#cb6-170" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(points) <span class="op">&gt;</span> <span class="dv">2</span>:</span>
<span id="cb6-171"><a href="#cb6-171" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Draw lines up to the the second to last point</span></span>
<span id="cb6-172"><a href="#cb6-172" aria-hidden="true" tabindex="-1"></a>            lines.add(</span>
<span id="cb6-173"><a href="#cb6-173" aria-hidden="true" tabindex="-1"></a>                <span class="op">*</span>[</span>
<span id="cb6-174"><a href="#cb6-174" aria-hidden="true" tabindex="-1"></a>                    Line(start<span class="op">=</span>points[i], end<span class="op">=</span>points[i <span class="op">+</span> <span class="dv">1</span>], stroke_width<span class="op">=</span>stroke_width)</span>
<span id="cb6-175"><a href="#cb6-175" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(points) <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb6-176"><a href="#cb6-176" aria-hidden="true" tabindex="-1"></a>                ]</span>
<span id="cb6-177"><a href="#cb6-177" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb6-178"><a href="#cb6-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-179"><a href="#cb6-179" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Draw the final arrow</span></span>
<span id="cb6-180"><a href="#cb6-180" aria-hidden="true" tabindex="-1"></a>        arrow_length <span class="op">=</span> np.linalg.norm(points[<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> points[<span class="op">-</span><span class="dv">2</span>])</span>
<span id="cb6-181"><a href="#cb6-181" aria-hidden="true" tabindex="-1"></a>        arrow <span class="op">=</span> Arrow(</span>
<span id="cb6-182"><a href="#cb6-182" aria-hidden="true" tabindex="-1"></a>            max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>,</span>
<span id="cb6-183"><a href="#cb6-183" aria-hidden="true" tabindex="-1"></a>            max_stroke_width_to_length_ratio<span class="op">=</span>stroke_width <span class="op">/</span> arrow_length,</span>
<span id="cb6-184"><a href="#cb6-184" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-185"><a href="#cb6-185" aria-hidden="true" tabindex="-1"></a>        arrow.put_start_and_end_on(</span>
<span id="cb6-186"><a href="#cb6-186" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>points[<span class="op">-</span><span class="dv">2</span>],</span>
<span id="cb6-187"><a href="#cb6-187" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>points[<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb6-188"><a href="#cb6-188" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-189"><a href="#cb6-189" aria-hidden="true" tabindex="-1"></a>        arrow.set_stroke(</span>
<span id="cb6-190"><a href="#cb6-190" aria-hidden="true" tabindex="-1"></a>            width<span class="op">=</span>stroke_width <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb6-191"><a href="#cb6-191" aria-hidden="true" tabindex="-1"></a>        )  <span class="co"># I don't know why but to get the width right I need to divide by 2</span></span>
<span id="cb6-192"><a href="#cb6-192" aria-hidden="true" tabindex="-1"></a>        lines.add(arrow)</span>
<span id="cb6-193"><a href="#cb6-193" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> lines</span>
<span id="cb6-194"><a href="#cb6-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-195"><a href="#cb6-195" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> circled_plus(<span class="va">self</span>, radius: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.4</span>):</span>
<span id="cb6-196"><a href="#cb6-196" aria-hidden="true" tabindex="-1"></a>        circle <span class="op">=</span> Circle(radius<span class="op">=</span>radius, color<span class="op">=</span>WHITE)</span>
<span id="cb6-197"><a href="#cb6-197" aria-hidden="true" tabindex="-1"></a>        plus <span class="op">=</span> Text(<span class="st">'+'</span>, font<span class="op">=</span><span class="st">'Arial'</span>).move_to(circle.get_center())</span>
<span id="cb6-198"><a href="#cb6-198" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> VGroup(circle, plus)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-17" class="cell">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The <a href="https://spather.github.io/transformer-experiments/models/transformer.html#block"><code>Block</code></a> module implements this wiring in PyTorch:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""One transformer block"""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embed, n_head):</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        head_size <span class="op">=</span> n_embed <span class="op">//</span> n_head</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sa <span class="op">=</span> MultiHeadAttention(n_head, head_size)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ffwd <span class="op">=</span> FeedForward(n_embed)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln1  <span class="op">=</span> nn.LayerNorm(n_embed)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.LayerNorm(n_embed)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.sa(<span class="va">self</span>.ln1(x)) <span class="co"># The `x +` part is a skip connection</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffwd(<span class="va">self</span>.ln2(x)) <span class="co"># The `x +` part is a skip connection</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>While many words have been written and spoken about multi-head attention, comparatively little has been said about the feed-forward network because, it seems, comparatively little is known:</p>
<div id="cell-20" class="cell">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-21" class="cell">
<div class="cell-output cell-output-display">
<p style="text-align: center;font-style: italic;">Screenshot from <a href="https://stats.stackexchange.com/q/485910">https://stats.stackexchange.com/q/485910</a></p>
</div>
</div>
<p>I started this investigation wondering what comes after attention. Literally, the feed-forward network does. In the transformer I studied, across all 6 blocks, the feed-forward networks comprise over 65% of the total trainable parameters, so they must play some important role.</p>
<p>As I’ll show <a href="#transformation-via-vector-addition">later</a>, it turns out that the output of the feed-forward network is the primary factor that determines how a block transforms its input into its output.</p>
</section>
</section>
<section id="demo-my-proposal-in-action" class="level2">
<h2 class="anchored" data-anchor-id="demo-my-proposal-in-action">Demo: My Proposal In Action</h2>
<p>In this section, I’m going to show an example that illustrates what I’m proposing the transformer is doing. In the next section, I’ll go into detail about how this is implemented.</p>
<p>Imagine we did the following:</p>
<ul>
<li>Ran the prompt, <code>'And only l'</code>, through the model and extracted the output value of the feed-forward network in the first transformer block.</li>
<li>Went back to the training corpus, found all substrings of the same length as our prompt (10-characters), ran all of them through the model, and filtered out just the ones whose feed-forward network outputs in the first block have a cosine similarity of 0.95 or greater when compared to that of the prompt, <code>'And only l'</code>.</li>
</ul>
<p>We’d come up with this set of strings:</p>
<div id="cell-24" class="cell">
<details class="code-fold">
<summary>Helper function to print results in a table</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> text_table(</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    headers: Iterable[<span class="bu">str</span>], data_columns: Sequence[Sequence[<span class="bu">str</span>]], col_widths: Sequence[<span class="bu">int</span>]</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">len</span>(headers) <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> <span class="bu">len</span>(headers) <span class="op">==</span> <span class="bu">len</span>(</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        data_columns</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    ), <span class="st">"Must have either zero headers or the same number as data columns"</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">len</span>(data_columns) <span class="op">==</span> <span class="bu">len</span>(col_widths), <span class="st">"Must have same number of column widths as data columns"</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(headers) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="st">""</span>.join([<span class="ss">f"</span><span class="sc">{</span>header<span class="sc">:</span>{col_widths[i]}<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i, header <span class="kw">in</span> <span class="bu">enumerate</span>(headers)]) <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        header_underlines <span class="op">=</span> [<span class="st">"-"</span> <span class="op">*</span> <span class="bu">len</span>(header) <span class="cf">for</span> header <span class="kw">in</span> headers]</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        output <span class="op">+=</span> (</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>            <span class="st">""</span>.join(</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>                [</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>                    <span class="ss">f"</span><span class="sc">{</span>header_underline<span class="sc">:</span>{col_widths[i]}<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> i, header_underline <span class="kw">in</span> <span class="bu">enumerate</span>(header_underlines)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>                ]</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>            <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="st">""</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    max_len <span class="op">=</span> <span class="bu">max</span>([<span class="bu">len</span>(col) <span class="cf">for</span> col <span class="kw">in</span> data_columns])</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_len):</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>        items <span class="op">=</span> [</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>            data_column[i] <span class="cf">if</span> i <span class="op">&lt;</span> <span class="bu">len</span>(data_column) <span class="cf">else</span> <span class="st">" "</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> data_column <span class="kw">in</span> data_columns</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>        output <span class="op">+=</span> <span class="st">""</span>.join([<span class="ss">f"</span><span class="sc">{</span>item<span class="sc">:</span>{col_widths[i]}<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i, item <span class="kw">in</span> <span class="bu">enumerate</span>(items)]) <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-25" class="cell">
<details class="code-fold">
<summary>Code to generate similar strings (will be explained later)</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get all the unique substrings in the text</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>strings10 <span class="op">=</span> all_unique_substrings(text<span class="op">=</span>ts.text, substring_length<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up to look at prefiltering</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>prefiltered_threshold<span class="op">=</span><span class="fl">0.7</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>prefiltered_results_folder <span class="op">=</span> environment.data_root <span class="op">/</span> <span class="st">'cosine_sim_results/large_files/slen10'</span> <span class="op">/</span> <span class="ss">f'prefiltered_</span><span class="sc">{</span>prefiltered_threshold<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prefiltered_filename(block_idx: <span class="bu">int</span>, q_idx: <span class="bu">int</span>) <span class="op">-&gt;</span> Path:</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> prefiltered_results_folder <span class="op">/</span> <span class="ss">f'cosine_sim_ffwd_out_</span><span class="sc">{</span>q_idx<span class="sc">:05d}</span><span class="ss">_</span><span class="sc">{</span>block_idx<span class="sc">:02d}</span><span class="ss">.pt'</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_prefiltered_data(block_idx: <span class="bu">int</span>, q_idx: <span class="bu">int</span>):</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.load(prefiltered_filename(block_idx, q_idx))</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>block_idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>similarity_threshold<span class="op">=</span><span class="fl">0.95</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>q_idx <span class="op">=</span> <span class="dv">57</span> <span class="co"># Query index for `And only l`</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>similar_indices <span class="op">=</span> filter_on_prefiltered_results(</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    load_prefiltered<span class="op">=</span><span class="kw">lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    q_idx_start<span class="op">=</span>q_idx,</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    q_idx_end<span class="op">=</span>q_idx<span class="op">+</span><span class="dv">1</span>,</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    filter_fn<span class="op">=</span><span class="kw">lambda</span> values: values <span class="op">&gt;</span> similarity_threshold</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>similar_strings <span class="op">=</span> [</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    [strings10[i] <span class="cf">for</span> i <span class="kw">in</span> indices]</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> indices <span class="kw">in</span> similar_indices</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>data_columns<span class="op">=</span>[</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>    [<span class="bu">repr</span>(s) <span class="cf">for</span> s <span class="kw">in</span> similar_strings[<span class="dv">0</span>][i : i <span class="op">+</span> <span class="dv">20</span>]] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(similar_strings[<span class="dv">0</span>]), <span class="dv">20</span>)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(text_table(</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>    headers<span class="op">=</span>[],</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>    data_columns<span class="op">=</span>data_columns,</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>    col_widths<span class="op">=</span>[<span class="dv">16</span> <span class="cf">for</span> _ <span class="kw">in</span> data_columns]</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>'hat only l'    's sickly l'    ' as\nthey l'   'r kingly l'    're; they l'    
'eby they l'    'ar, they l'    'im, only l'    'ling any l'    'life may l'    
'nobility l'    'e\nBy any l'   ' as they l'    ', if any l'    ' hastily l'    
'tly they l'    ' ghastly l'    '\nMy only l'   'For many l'    'r in any l'    
' till my l'    'all they l'    'hen they l'    'at Henry l'    'oolishly l'    
'er:\nThey l'   'may they l'    'or stony l'    'ur Henry l'    'l gladly l'    
'yet they l'    'y;\nDelay l'   'e, on my l'    'or Henry l'    'I dearly l'    
' if they l'    ' she may l'    't\nfairly l'   'ould say l'    'd all my l'    
'her they l'    ' Stanley l'    ' and may l'    'uld they l'    'u all my l'    
'friendly l'    'h gently l'    'e deadly l'    'f all my l'    'n all my l'    
'Ere they l'    'steel my l'    ' tell my l'    'e kingly l'    'learn my l'    
'd he say l'    't basely l'    'Thursday l'    'iciously l'    " 'if any l"    
' as many l'    'hy glory l'    'not very l'    'a goodly l'    'e surely l'    
'quiously l'    ', fairly l'    'lord! my l'    'entle my l'    ', he may l'    
'our holy l'    ' worldly l'    ' my only l'    ' all, my l'                    
'ul, they l'    'o lately l'    's in any l'    ' no lady l'                    
'ter many l'    'Our holy l'    't vainly l'    'e\nA lady l'                   
' you may l'    'y greedy l'    'untimely l'    'directly l'                    
'er on my l'    'e wistly l'    'ng Henry l'    'And only l'                    
's kindly l'    'KE:\nThey l'   ' of many l'    'o, on my l'                    
</code></pre>
</div>
</div>
<p>There’s a clear pattern across these: they all end in <code>y l</code> and several of them end in <code>ly l</code>. Similarity in the space of feed-forward network outputs seems to correspond to human-interpretable patterns.</p>
<p>Next, imagine we went back to the training corpus, found each of these strings and built a distribution of all the characters that came after them. We’d find, for example:</p>
<ul>
<li><code>'hat only l'</code> is followed by <code>i</code> (“T<code>hat only l</code><strong>i</strong>ke a gulf it did remain”)</li>
<li><code>'l gladly l'</code> is followed by <code>e</code> (“I’l<code>l gladly l</code><strong>e</strong>arn.”)</li>
<li><code>'n all my l'</code> is followed by both <code>a</code> and <code>i</code> (“I<code>n all my l</code><strong>a</strong>nds and leases whatsoever” and “never saw you before i<code>n all my l</code><strong>i</strong>fe”)</li>
</ul>
<p>Doing this for the complete set of 94 strings, we’d end up with this distribution:</p>
<div id="cell-27" class="cell">
<details class="code-fold">
<summary>Helper function to plot probability distribution for tokens</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_prob_distribution_for_tokens(</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    prob_distribution: torch.Tensor,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    title: <span class="bu">str</span> <span class="op">=</span> <span class="st">""</span>,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    ax: Optional[Axes] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>),</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ax <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>        _, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>figsize)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    x_indices <span class="op">=</span> np.arange(tokenizer.vocab_size)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    x_labels <span class="op">=</span> [<span class="bu">repr</span>(c)[<span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>] <span class="cf">for</span> c <span class="kw">in</span> tokenizer.chars]</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    ax.bar(x_indices, prob_distribution)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    ax.set_xticks(x_indices, x_labels, rotation<span class="op">=</span><span class="st">"vertical"</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    ax.set_title(title)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim(<span class="fl">0.0</span>, <span class="fl">1.0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-28" class="cell">
<details class="code-fold">
<summary>Code to produce distribution from tokens that follow similar strings</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>next_token_map10 <span class="op">=</span> build_next_token_map(</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    text<span class="op">=</span>ts.text,</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    prefix_len<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span>tokenizer.vocab_size,</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    stoi<span class="op">=</span>tokenizer.stoi</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>total_freq_distribution <span class="op">=</span> torch.stack([</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    next_token_map10[string] <span class="cf">for</span> string <span class="kw">in</span> similar_strings[<span class="dv">0</span>]</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>]).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>prob_distribution <span class="op">=</span> total_freq_distribution <span class="op">/</span> total_freq_distribution.<span class="bu">sum</span>()</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>plot_prob_distribution_for_tokens(prob_distribution, title<span class="op">=</span><span class="st">'Normalized frequency distribution from block 0 similar strings'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The various tokens in our model’s vocabulary appear on the x-axis and the normalized frequency of occurrence on the y-axis. This plot shows that <code>i</code> was the most frequent, then <code>o</code>, then <code>a</code>, and finally, <code>e</code>.</p>
<p>Now let’s look at the final output of the transformer as a whole when given <code>And only l</code> as a prompt:</p>
<div id="cell-31" class="cell">
<details class="code-fold">
<summary>Code to produce model predictions</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">'And only l'</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> encoding_helpers.tokenize_string(prompt)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>logits, _ <span class="op">=</span> m(tokens)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> LogitsWrapper(logits.detach(), tokenizer)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>logits.plot_probs(title<span class="op">=</span><span class="st">'Probability distribution from model'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This is a probability distribution representing the model’s predictions for the next token. Notice that it’s strikingly similar to the normalized frequency distribution shown in the previous plot!</p>
<p>We can quantify how similar they are. <a href="https://en.wikipedia.org/wiki/Hellinger_distance">Hellinger distance</a> is a measure of overlap between probability distributions. Given distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>, the Hellinger distance between them is:</p>
<p><span class="math display">\[
H(P, Q) = \frac{1}{\sqrt{2}} \sqrt{\sum_{i=1}^n (\sqrt{p_i} - \sqrt{q_i})^2}
\]</span></p>
<p>Or, in code:</p>
<div id="cell-34" class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> hellinger_distance(</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    p: torch.Tensor,</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    q: torch.Tensor,</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ((p.sqrt() <span class="op">-</span> q.sqrt())<span class="op">**</span><span class="dv">2</span>).<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>).sqrt() <span class="op">/</span> math.sqrt(<span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Hellinger distance of 0 means the two distributions are identical and 1 means they have no overlap.</p>
<p>The Hellinger distance between the two distributions above - the distribution formed from the tokens that follow the strings with similar feed-forward network outputs and the distribution the model predicts - is 0.07: very nearly identical.</p>
<p>For the sake of keeping the demo brief, I chose an example where the first block’s similar strings alone are enough to produce a distribution that closely matches the final output of the transformer. Typically, we’d need to need to do the same exercise - finding the strings in the training corpus that produce similar feed-forward network outputs to the prompt and building a distribution from the tokens that succeed them - for all 6 transformer blocks, and then calculate a weighted sum of the resulting distributions in order to get a good match. We’ll do that in the next section and see that <strong>across a sample of 20,000 prompts, the average Hellinger distance between distributions computed this way and the corresponding transformer output was just 0.17</strong>.</p>
<p>This small average Hellinger distances suggests the results produced by this approach are a good approximation for the transformer’s outputs. In addition, as I’ll explain in the <a href="#interpretation-why-does-the-approximation-work">interpretation</a> section, I think the approach itself is a reasonable approximation of what the transformer is actually doing.</p>
</section>
<section id="implementation-approximating-the-transformer-output-with-feed-forward-network-outputs" class="level2">
<h2 class="anchored" data-anchor-id="implementation-approximating-the-transformer-output-with-feed-forward-network-outputs">Implementation: Approximating the Transformer Output with Feed-forward Network Outputs</h2>
<p>In this section, I’m going to walk through in some detail and with code, the exact procedure I used to approximate the transformer’s output using strings that produced similar feed-forward network outputs. If you’re not interested in the implementation, skip this section and proceed to the <a href="#evaluating-the-approximation">evaluation</a> section.</p>
<p>To recap, this is the procedure to compute the approximation:</p>
<ol type="1">
<li>Run a prompt through the model and save the feed-forward network outputs for each block.</li>
<li>For each block:
<ul>
<li>Find the strings in the training corpus that produce the most similar feed-forward network outputs to the prompt for that block.</li>
<li>For each string found, build a frequency distribution of the tokens that come after it in the training corpus.</li>
<li>Sum the frequency distributions for all strings found for the current block.</li>
</ul></li>
<li>Compute a weighted sum of the frequency distributions for each block computed in the previous step.</li>
<li>Normalize the weighted sum to get a probability distribution.</li>
</ol>
<section id="procedure-setup" class="level3">
<h3 class="anchored" data-anchor-id="procedure-setup">Procedure Setup</h3>
<p>The first step of the procedure - running a prompt through the model and saving the feed-forward network outputs for each block - is straightforward to accomplish with some basic PyTorch hooks. But the first part of step two - finding the strings in the training corpus that produce similar feed-forward network outputs - requires some additional machinery to do efficiently.</p>
<p>I did all the analysis with length 10 strings for compute and storage efficiency (but I also observed that the results hold for both shorter and longer strings). The 1,115,394-character long training corpus contains 858,923 unique, length 10 substrings. Each feed-forward network output is a 384-dimensional vector of <code>float32</code> values and the model produces 6 of them (one for each block). Comparing the 6 384-dimensional feed-forward outputs for any prompt to 6 * 858,923 = 5,153,538 feed-forward outputs from all the other strings takes a long time. To able to work with this data, I had to pre-compute things. I built the following pipeline:</p>
<ol type="1">
<li>I chose 20,000 length 10 strings from the training corpus at random to use as prompts in this experiment.</li>
<li>Overnight, I ran a process to compute the cosine similarity between the feed-forward network outputs the model produced for the 20,000 prompts and those it produced for the 858,923 unique length 10 substrings of the training corpus. I did this in batches and saved the results to disk.</li>
<li>Even after pre-computing the cosine similarity results, searching through all of them to find the closest matches took a long time. Experiments showed matches of interest never had a cosine similarity below 0.7, so I ran another step to pre-filter the results of step 2 to just those entries with cosine similarity &gt;= 0.7. This greatly reduced the number of entries to search through.</li>
</ol>
<p>The code for this pre-computation and pre-filtering is too much to include in this post, but the implementation is available in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/cosine-sims.ipynb">the <code>cosine-sims</code> experiment notebook</a>.</p>
</section>
<section id="procedure-walkthrough" class="level3">
<h3 class="anchored" data-anchor-id="procedure-walkthrough">Procedure Walkthrough</h3>
<p>In this section, we’ll build up the code step by step and run it on one prompt at a time and for just one block. Over the following sections, we’ll extend it to additional blocks, run it across a large number of prompts, and examine the results.</p>
<p>First, we need to grab 20,000 length 10 strings from the training corpus to use as prompts:</p>
<div id="cell-42" class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get all the unique substrings in the text</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>strings10 <span class="op">=</span> all_unique_substrings(text<span class="op">=</span>ts.text, substring_length<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>n_prompts <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> torch.randperm(<span class="bu">len</span>(strings10))[:n_prompts]</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>prompts <span class="op">=</span> [strings10[i.item()] <span class="cf">for</span> i <span class="kw">in</span> indices]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As described in the <a href="#procedure-setup">Procedure Setup</a> section, I previously ran all these strings through the model, grabbed the feed-forward network outputs for each block, and pre-computed the cosine similarities to all the unique length 10 substrings in the training corpus. And then I pre-filtered the results to just those with cosine similarity &gt;= 0.7.</p>
<p>The <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/cosine-sims.ipynb">the <code>cosine-sims</code> experiment notebook</a> that implements all this also exports a helper function, <code>filter_on_prefiltered_results()</code>, that we can use to find the most similar strings to a given prompt by searching over the pre-filtered results.</p>
<blockquote class="blockquote">
<p>If you’re curious about how this works, check out the notebook. It’s pretty straightforward and the unit test provides a simple example that illustrates the shape of the inputs and outputs.</p>
</blockquote>
<p>To use <code>filter_on_prefiltered_results()</code>, we just need to tell it how to find the prefiltered files:</p>
<div id="cell-44" class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>prefiltered_threshold<span class="op">=</span><span class="fl">0.7</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>prefiltered_results_folder <span class="op">=</span> environment.data_root <span class="op">/</span> <span class="st">'cosine_sim_results/large_files/slen10'</span> <span class="op">/</span> <span class="ss">f'prefiltered_</span><span class="sc">{</span>prefiltered_threshold<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prefiltered_filename(block_idx: <span class="bu">int</span>, q_idx: <span class="bu">int</span>) <span class="op">-&gt;</span> Path:</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> prefiltered_results_folder <span class="op">/</span> <span class="ss">f'cosine_sim_ffwd_out_</span><span class="sc">{</span>q_idx<span class="sc">:05d}</span><span class="ss">_</span><span class="sc">{</span>block_idx<span class="sc">:02d}</span><span class="ss">.pt'</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_prefiltered_data(block_idx: <span class="bu">int</span>, q_idx: <span class="bu">int</span>):</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.load(prefiltered_filename(block_idx, q_idx))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<blockquote class="blockquote">
<p>Note on the use of <code>q_idx</code> here and in the rest of the code: <code>q_idx</code> refers to “query index”. The job that pre-computes all the cosine similarities takes a set of “queries” or values to compare to. These queries are the feed-forward network outputs the model produces for the prompts. There is a 1:1 correspondence between queries and prompts and so I’ve used the terms interchangeably in the code.</p>
</blockquote>
<p>To start, we’ll use the same prompt - <code>'And only l'</code> - we used in the earlier demo. It happens to be the prompt at index 57:</p>
<div id="cell-47" class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>prompts[<span class="dv">57</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>'And only l'</code></pre>
</div>
</div>
<p>We’ll find the strings whose feed-forward network outputs in block 0 had a cosine similarity of 0.95 or greater when compared to the block 0 feed forward network output of the prompt.</p>
<div id="cell-49" class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>block_idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>similarity_threshold<span class="op">=</span><span class="fl">0.95</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>q_idx <span class="op">=</span> <span class="dv">57</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>similar_indices <span class="op">=</span> filter_on_prefiltered_results(</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    load_prefiltered<span class="op">=</span><span class="kw">lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    q_idx_start<span class="op">=</span>q_idx,</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    q_idx_end<span class="op">=</span>q_idx<span class="op">+</span><span class="dv">1</span>,</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    filter_fn<span class="op">=</span><span class="kw">lambda</span> values: values <span class="op">&gt;</span> similarity_threshold</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>similar_strings <span class="op">=</span> [</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    [strings10[i] <span class="cf">for</span> i <span class="kw">in</span> indices]</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> indices <span class="kw">in</span> similar_indices</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(similar_strings[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>94</code></pre>
</div>
</div>
<p>This produced the 94 similar strings we saw in the demo. We can print them again to be sure:</p>
<div id="cell-51" class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original string: </span><span class="sc">{</span><span class="bu">repr</span>(prompts[q_idx])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Similar strings: </span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>data_columns<span class="op">=</span>[</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    [<span class="bu">repr</span>(s) <span class="cf">for</span> s <span class="kw">in</span> similar_strings[<span class="dv">0</span>][i : i <span class="op">+</span> <span class="dv">20</span>]] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(similar_strings[<span class="dv">0</span>]), <span class="dv">20</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(text_table(</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    headers<span class="op">=</span>[],</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    data_columns<span class="op">=</span>data_columns,</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    col_widths<span class="op">=</span>[<span class="dv">18</span> <span class="cf">for</span> _ <span class="kw">in</span> data_columns]</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Original string: 'And only l'
Similar strings: 

'hat only l'      's sickly l'      ' as\nthey l'     'r kingly l'      're; they l'      
'eby they l'      'ar, they l'      'im, only l'      'ling any l'      'life may l'      
'nobility l'      'e\nBy any l'     ' as they l'      ', if any l'      ' hastily l'      
'tly they l'      ' ghastly l'      '\nMy only l'     'For many l'      'r in any l'      
' till my l'      'all they l'      'hen they l'      'at Henry l'      'oolishly l'      
'er:\nThey l'     'may they l'      'or stony l'      'ur Henry l'      'l gladly l'      
'yet they l'      'y;\nDelay l'     'e, on my l'      'or Henry l'      'I dearly l'      
' if they l'      ' she may l'      't\nfairly l'     'ould say l'      'd all my l'      
'her they l'      ' Stanley l'      ' and may l'      'uld they l'      'u all my l'      
'friendly l'      'h gently l'      'e deadly l'      'f all my l'      'n all my l'      
'Ere they l'      'steel my l'      ' tell my l'      'e kingly l'      'learn my l'      
'd he say l'      't basely l'      'Thursday l'      'iciously l'      " 'if any l"      
' as many l'      'hy glory l'      'not very l'      'a goodly l'      'e surely l'      
'quiously l'      ', fairly l'      'lord! my l'      'entle my l'      ', he may l'      
'our holy l'      ' worldly l'      ' my only l'      ' all, my l'                        
'ul, they l'      'o lately l'      's in any l'      ' no lady l'                        
'ter many l'      'Our holy l'      't vainly l'      'e\nA lady l'                       
' you may l'      'y greedy l'      'untimely l'      'directly l'                        
'er on my l'      'e wistly l'      'ng Henry l'      'And only l'                        
's kindly l'      'KE:\nThey l'     ' of many l'      'o, on my l'                        
</code></pre>
</div>
</div>
<p>Next, we’ll need to build a frequency distribution for the tokens that came after these strings in the text. To make this easy and efficient (we’ll eventually be doing many times), we can pre-compute the next token frequency distributions for all the unique length 10 substrings in the training corpus. The helper function <code>build_next_token_map()</code>, implemented in the <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/common/text-analysis.ipynb">text-analysis module</a>, does this.</p>
<div id="cell-53" class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>next_token_map10 <span class="op">=</span> build_next_token_map(</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    text<span class="op">=</span>ts.text,</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    prefix_len<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span>tokenizer.vocab_size,</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    stoi<span class="op">=</span>tokenizer.stoi</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The return value stored in <code>next_token_map10</code> is a dictionary that maps each unique length 10 substring in the training corpus to a frequency distribution of the tokens that come after it. Conceptually, it looks something like this:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'the common'</span>: {</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">' '</span>: <span class="dv">12</span>, <span class="st">"'"</span>: <span class="dv">1</span>, <span class="st">','</span>: <span class="dv">1</span>, <span class="st">'?'</span>: <span class="dv">1</span>, <span class="st">'a'</span>: <span class="dv">1</span>, <span class="st">'s'</span>: <span class="dv">5</span>, <span class="st">'w'</span>: <span class="dv">3</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">' the gods '</span>: {</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">'b'</span>: <span class="dv">1</span>, <span class="st">'c'</span>: <span class="dv">1</span>, <span class="st">'d'</span>: <span class="dv">2</span>, <span class="st">'f'</span>: <span class="dv">1</span>, <span class="st">'g'</span>: <span class="dv">1</span>, <span class="st">'h'</span>: <span class="dv">2</span>, <span class="st">'k'</span>: <span class="dv">2</span>, <span class="st">'s'</span>: <span class="dv">2</span>, <span class="st">'t'</span>: <span class="dv">1</span>, <span class="st">'w'</span>: <span class="dv">2</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">' authority'</span>: {</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>: <span class="dv">1</span>, <span class="st">' '</span>: <span class="dv">5</span>, <span class="st">','</span>: <span class="dv">5</span>, <span class="st">':'</span>: <span class="dv">2</span>, <span class="st">';'</span>: <span class="dv">1</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In reality, the values are actually tensors of shape <code>(vocab_size,)</code> where <code>vocab_size</code> is the number of unique tokens the vocabulary (65, in our case). The item at index <code>i</code> in the tensor is the count of occurrences of the <code>i</code>th token after the string in that entry’s key. So it looks more like:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>      <span class="st">'the common'</span>: torch.tensor([</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>            <span class="dv">0</span>, <span class="dv">12</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">1</span>,  <span class="dv">1</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">1</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>            <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>            <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">1</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>            <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">5</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">3</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>      ]),</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>      <span class="st">' the gods '</span>: torch.tensor([</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>            <span class="dv">0</span>, <span class="dv">12</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">1</span>,  <span class="dv">1</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">1</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>            <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>            <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">1</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>            <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">5</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">3</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>      ]),</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>      <span class="st">' authority'</span>: torch.tensor([</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>          <span class="dv">0</span>, <span class="dv">12</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">1</span>,  <span class="dv">1</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">1</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>          <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>          <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">1</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>          <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">5</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">3</span>,  <span class="dv">0</span>,  <span class="dv">0</span>,  <span class="dv">0</span></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>      ]),</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Next, we need to sum the frequency distributions for all the strings we found to have similar feed-forward network outputs to our prompt. Because <code>next_token_map10</code> stores the individual frequency distributions as tensors, this is easy to accomplish:</p>
<div id="cell-56" class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>total_freq_distribution <span class="op">=</span> torch.stack([</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    next_token_map10[string] <span class="cf">for</span> string <span class="kw">in</span> similar_strings[<span class="dv">0</span>]</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>]).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We stack up the distributions for each similar string into a single tensor and then sum across all of them. We can now turn this into a probability distribution by dividing each entry by the sum of all the entries:</p>
<div id="cell-58" class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>prob_distribution <span class="op">=</span> total_freq_distribution <span class="op">/</span> total_freq_distribution.<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, we can visualize this distribution:</p>
<div id="cell-60" class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>plot_prob_distribution_for_tokens(prob_distribution, title<span class="op">=</span><span class="st">'Probability distribution using only block 0 similar strings'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-25-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It’s the same distribution we saw in the demo.</p>
<p>Now let’s code the comparison to the model output:</p>
<div id="cell-62" class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> encoding_helpers.tokenize_string(prompts[q_idx])</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>logits, _ <span class="op">=</span> m(tokens)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> LogitsWrapper(logits.detach(), tokenizer)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>logits.plot_probs(title<span class="op">=</span><span class="st">'Probability distribution from model'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-26-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Again, the two distributions look very similar, and in this example, the approximation uses only values from the first block. To better compare them, we can look at the distributions in text form:</p>
<div id="cell-64" class="cell">
<details class="code-fold">
<summary>Helper function to print comparison of distributions as a table</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_distribution_comparison(</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    approx_top_tokens: Sequence[Tuple[<span class="bu">str</span>, <span class="bu">float</span>]],</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    model_top_tokens: Sequence[Tuple[<span class="bu">str</span>, <span class="bu">float</span>]],</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    max_len <span class="op">=</span> <span class="bu">min</span>(<span class="bu">len</span>(approx_top_tokens), <span class="bu">len</span>(model_top_tokens))</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>        text_table(</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>            headers<span class="op">=</span>[<span class="st">"Model Predictions"</span>, <span class="st">"Approximation Predictions"</span>],</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>            data_columns<span class="op">=</span>[</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>                [</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>                    <span class="ss">f"</span><span class="sc">{</span><span class="bu">repr</span>(token)[<span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>prob<span class="sc">:.3f}</span><span class="ss">"</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> i, (token, prob) <span class="kw">in</span> <span class="bu">enumerate</span>(model_top_tokens)</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> i <span class="op">&lt;</span> max_len</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>                ],</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>                [</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>                    <span class="ss">f"</span><span class="sc">{</span><span class="bu">repr</span>(token)[<span class="dv">1</span>:<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>prob<span class="sc">:.3f}</span><span class="ss">"</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> i, (token, prob) <span class="kw">in</span> <span class="bu">enumerate</span>(approx_top_tokens)</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> i <span class="op">&lt;</span> max_len</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>                ],</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>            ],</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>            col_widths<span class="op">=</span>[<span class="dv">20</span>, <span class="dv">20</span>],</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-65" class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>approx_top_tokens <span class="op">=</span> top_nonzero_tokens(prob_distribution, tokenizer.itos)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>model_top_tokens <span class="op">=</span> logits.topk_tokens(k<span class="op">=</span><span class="dv">10</span>)[<span class="dv">0</span>][<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>print_distribution_comparison(approx_top_tokens, model_top_tokens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            i: 0.389            
o: 0.204            o: 0.250            
a: 0.195            a: 0.222            
e: 0.160            e: 0.139            
</code></pre>
</div>
</div>
<p>Finally, we can also compare the Hellinger distance between these distributions:</p>
<div id="cell-67" class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>hellinger_distance(prob_distribution, logits.probs()[<span class="dv">0</span>][<span class="op">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(0.0711)</code></pre>
</div>
</div>
<p>By combining the next token frequency distributions of the similar strings from just the first layer of the model, we are able to pretty closely approximate the output of the transformer. Of course, I chose an example that works particularly well.</p>
<p>Here’s an example where the frequency distribution from just the first layer doesn’t work well:</p>
<div id="cell-69" class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>q_idx<span class="op">=</span><span class="dv">40</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>prompts[q_idx]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>'hing tremb'</code></pre>
</div>
</div>
<p>Using the same method, we can identify 57 strings from the training corpus that produce similar feed-forward network outputs to the prompt:</p>
<div id="cell-71" class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>block_idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>similarity_threshold<span class="op">=</span><span class="fl">0.95</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>similar_indices <span class="op">=</span> filter_on_prefiltered_results(</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    load_prefiltered<span class="op">=</span><span class="kw">lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    q_idx_start<span class="op">=</span>q_idx,</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>    q_idx_end<span class="op">=</span>q_idx<span class="op">+</span><span class="dv">1</span>,</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>    filter_fn<span class="op">=</span><span class="kw">lambda</span> values: values <span class="op">&gt;</span> similarity_threshold</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>similar_strings <span class="op">=</span> [</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>    [strings10[i] <span class="cf">for</span> i <span class="kw">in</span> indices]</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> indices <span class="kw">in</span> similar_indices</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(similar_strings[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>57</code></pre>
</div>
</div>
<p>We can look up, sum, and normalize the frequency distributions of tokens that follow these strings in the training corpus, and compare the result to the model outputs, as we did before:</p>
<div id="cell-73" class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>total_freq_distribution <span class="op">=</span> torch.stack([</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    next_token_map10[string] <span class="cf">for</span> string <span class="kw">in</span> similar_strings[<span class="dv">0</span>]</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>]).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>prob_distribution <span class="op">=</span> total_freq_distribution <span class="op">/</span> total_freq_distribution.<span class="bu">sum</span>()</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>approx_top_tokens <span class="op">=</span> top_nonzero_tokens(prob_distribution, tokenizer.itos)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> encoding_helpers.tokenize_string(prompts[q_idx])</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>logits, _ <span class="op">=</span> m(tokens)</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> LogitsWrapper(logits.detach(), tokenizer)</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>model_top_tokens <span class="op">=</span> logits.topk_tokens(k<span class="op">=</span><span class="dv">10</span>)[<span class="dv">0</span>][<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>print_distribution_comparison(approx_top_tokens, model_top_tokens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
l: 0.999            e: 0.543            
e: 0.000            l: 0.343            
r: 0.000            r: 0.114            
</code></pre>
</div>
</div>
<p>Unlike the previous example, these distributions are quite different. The top 3 tokens are the same in each, but they’re in the wrong order and their probabilities are far apart. These differences contribute to a large Hellinger distance:</p>
<div id="cell-75" class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> encoding_helpers.tokenize_string(prompts[q_idx])</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>logits, _ <span class="op">=</span> m(tokens)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> LogitsWrapper(logits.detach(), tokenizer)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>hellinger_distance(prob_distribution, logits.probs()[<span class="dv">0</span>][<span class="op">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(0.6305)</code></pre>
</div>
</div>
<p>For the prompt, <code>'hing tremb'</code>, just using the values from the first block results in a poor approximation of the transformer’s output. We’ll soon add the contributions from other blocks and when we do, we’ll get the Hellinger distance between the approximation and the real transformer output for this prompt down from 0.63 to just 0.02.</p>
</section>
<section id="similarity-thresholds" class="level3">
<h3 class="anchored" data-anchor-id="similarity-thresholds">Similarity Thresholds</h3>
<p>In the preceding examples, I used a similarity threshold of 0.95: I searched for strings whose feed-forward network outputs in block 0 produced values with a cosine similarity of 0.95 or greater when compared to the feed-forward network output of the prompt.</p>
<p>A different threshold would have yielded different results. For example, doing the same exercise for prompt id 57 (<code>'And only l'</code>) with a threshold of 0.90 finds 612 similar strings, vs the 94 we had before:</p>
<div id="cell-78" class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>block_idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>similarity_threshold<span class="op">=</span><span class="fl">0.90</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>q_idx <span class="op">=</span> <span class="dv">57</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>similar_indices <span class="op">=</span> filter_on_prefiltered_results(</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    load_prefiltered<span class="op">=</span><span class="kw">lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    q_idx_start<span class="op">=</span>q_idx,</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    q_idx_end<span class="op">=</span>q_idx<span class="op">+</span><span class="dv">1</span>,</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>    filter_fn<span class="op">=</span><span class="kw">lambda</span> values: values <span class="op">&gt;</span> similarity_threshold</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>similar_strings <span class="op">=</span> [</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    [strings10[i] <span class="cf">for</span> i <span class="kw">in</span> indices]</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> indices <span class="kw">in</span> similar_indices</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(similar_strings[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>612</code></pre>
</div>
</div>
<p>If we do the rest of the approximation procedure, we see different (and worse) results:</p>
<div id="cell-80" class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>total_freq_distribution <span class="op">=</span> torch.stack([</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    next_token_map10[string] <span class="cf">for</span> string <span class="kw">in</span> similar_strings[<span class="dv">0</span>]</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>]).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>prob_distribution <span class="op">=</span> total_freq_distribution <span class="op">/</span> total_freq_distribution.<span class="bu">sum</span>()</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>approx_top_tokens <span class="op">=</span> top_nonzero_tokens(prob_distribution, tokenizer.itos)</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> encoding_helpers.tokenize_string(prompts[q_idx])</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>logits, _ <span class="op">=</span> m(tokens)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> LogitsWrapper(logits.detach(), tokenizer)</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>model_top_tokens <span class="op">=</span> logits.topk_tokens(k<span class="op">=</span><span class="dv">10</span>)[<span class="dv">0</span>][<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>print_distribution_comparison(approx_top_tokens, model_top_tokens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            o: 0.584            
o: 0.204            i: 0.251            
a: 0.195            a: 0.095            
e: 0.160            e: 0.066            
u: 0.004            u: 0.002            
l: 0.000            y: 0.001            
</code></pre>
</div>
</div>
<p>The top 5 tokens are the same, but when ranked by probability, the approximation has a different ordering than the model. The Hellinger distance is also higher:</p>
<div id="cell-82" class="cell">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>hellinger_distance(prob_distribution, logits.probs()[<span class="dv">0</span>][<span class="op">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(0.2856)</code></pre>
</div>
</div>
<p>Loosening the similarity threshold introduced strings into the calculation that resulted in a worse approximation. Tightening beyond 0.95 also produces worse results than we got with 0.95, presumably because we’re excluding strings that were needed to produce a good approximation:</p>
<div id="cell-84" class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>block_idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>similarity_threshold<span class="op">=</span><span class="fl">0.97</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>q_idx <span class="op">=</span> <span class="dv">57</span></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>similar_indices <span class="op">=</span> filter_on_prefiltered_results(</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>    load_prefiltered<span class="op">=</span><span class="kw">lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    q_idx_start<span class="op">=</span>q_idx,</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>    q_idx_end<span class="op">=</span>q_idx<span class="op">+</span><span class="dv">1</span>,</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>    filter_fn<span class="op">=</span><span class="kw">lambda</span> values: values <span class="op">&gt;</span> similarity_threshold</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>similar_strings <span class="op">=</span> [</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>    [strings10[i] <span class="cf">for</span> i <span class="kw">in</span> indices]</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> indices <span class="kw">in</span> similar_indices</span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(similar_strings[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>33</code></pre>
</div>
</div>
<div id="cell-85" class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>total_freq_distribution <span class="op">=</span> torch.stack([</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    next_token_map10[string] <span class="cf">for</span> string <span class="kw">in</span> similar_strings[<span class="dv">0</span>]</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>]).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>prob_distribution <span class="op">=</span> total_freq_distribution <span class="op">/</span> total_freq_distribution.<span class="bu">sum</span>()</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>approx_top_tokens <span class="op">=</span> top_nonzero_tokens(prob_distribution, tokenizer.itos)</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> encoding_helpers.tokenize_string(prompts[q_idx])</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>logits, _ <span class="op">=</span> m(tokens)</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> LogitsWrapper(logits.detach(), tokenizer)</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>model_top_tokens <span class="op">=</span> logits.topk_tokens(k<span class="op">=</span><span class="dv">10</span>)[<span class="dv">0</span>][<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>print_distribution_comparison(approx_top_tokens, model_top_tokens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            o: 0.278            
o: 0.204            i: 0.250            
a: 0.195            a: 0.250            
e: 0.160            e: 0.222            
</code></pre>
</div>
</div>
<div id="cell-86" class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>hellinger_distance(prob_distribution, logits.probs()[<span class="dv">0</span>][<span class="op">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(0.1498)</code></pre>
</div>
</div>
<p>For the first block, 0.95 appears to be a sweet spot. I came up with this threshold through manual tuning: trying different values and binary searching towards one that produced the best results. The full history of this tuning exercise is in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/40_widening_similar_space.ipynb">the similar space analysis notebook</a>.</p>
<p>In the end, I found the following thresholds produce the best results for each block:</p>
<table class="table">
<thead>
<tr class="header">
<th>Block</th>
<th>Similarity Threshold</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.95</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.94</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.85</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.76</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.81</td>
</tr>
<tr class="even">
<td>5</td>
<td>0.89</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p>When I first started exploring this space, I assumed the approximation would get better the more similarity I could find. I tried a number of techniques, including experimenting with Euclidean distance vs cosine similarity, searching across strings of different lengths, etc. Every time I succeeded in finding strings with more similar feed-forward network outputs to use in the approximation, the results got worse. I realized that, at least for some blocks, including <em>less</em> similar values in the mix produced better approximations, probably because those blocks had learned to map prompts to broader classes of strings in the training corpus.</p>
</blockquote>
</section>
<section id="going-beyond-the-first-block" class="level3">
<h3 class="anchored" data-anchor-id="going-beyond-the-first-block">Going Beyond the First Block</h3>
<p>Thus far, we’ve only considered feed-forward network outputs from the first block. Now we’ll incorporate the contributions from the other blocks.</p>
<p>First, let’s find the strings that produce similar feed-forward network outputs in each block, using the similarity thresholds listed above. For now, we’ll do this for just one query (index 57, <code>'And only l'</code>):</p>
<div id="cell-90" class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>similarity_thresholds<span class="op">=</span>[<span class="fl">0.95</span>, <span class="fl">0.94</span>, <span class="fl">0.85</span>, <span class="fl">0.76</span>, <span class="fl">0.81</span>, <span class="fl">0.89</span>]</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>q_idx <span class="op">=</span> <span class="dv">57</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>similar_strings_per_block <span class="op">=</span> []</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> block_idx <span class="kw">in</span> <span class="bu">range</span>(n_layer):</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>    similar_indices <span class="op">=</span> filter_on_prefiltered_results(</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>        load_prefiltered<span class="op">=</span><span class="kw">lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>        q_idx_start<span class="op">=</span>q_idx,</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>        q_idx_end<span class="op">=</span>q_idx<span class="op">+</span><span class="dv">1</span>,</span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>        filter_fn<span class="op">=</span><span class="kw">lambda</span> values: values <span class="op">&gt;</span> similarity_thresholds[block_idx]</span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>    similar_strings <span class="op">=</span> [</span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a>        [strings10[i] <span class="cf">for</span> i <span class="kw">in</span> indices]</span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> indices <span class="kw">in</span> similar_indices</span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a>    similar_strings_per_block.append(similar_strings)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s summarize how many strings we found for each block based on these thresholds:</p>
<div id="cell-92" class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(text_table(</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    headers<span class="op">=</span>[<span class="st">"Block Index"</span>, <span class="st">"Similarity Threshold"</span>, <span class="st">"# of Similar Strings"</span>],</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    data_columns<span class="op">=</span>[</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>        [<span class="ss">f"</span><span class="sc">{</span>block_idx<span class="sc">:&gt;10}</span><span class="ss">"</span> <span class="cf">for</span> block_idx <span class="kw">in</span> <span class="bu">range</span>(n_layer)],</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>        [<span class="ss">f"</span><span class="sc">{</span>threshold<span class="sc">:&gt;19}</span><span class="ss">"</span> <span class="cf">for</span> threshold <span class="kw">in</span> similarity_thresholds],</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>        [<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(similar_strings[<span class="dv">0</span>])<span class="sc">:&gt;19}</span><span class="ss">"</span> <span class="cf">for</span> similar_strings <span class="kw">in</span> similar_strings_per_block],</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    col_widths<span class="op">=</span>[<span class="dv">14</span>, <span class="dv">23</span>, <span class="dv">23</span>]</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Block Index   Similarity Threshold   # of Similar Strings   
-----------   --------------------   --------------------   
         0                   0.95                     94    
         1                   0.94                     47    
         2                   0.85                     70    
         3                   0.76                    108    
         4                   0.81                    175    
         5                   0.89                   2237    
</code></pre>
</div>
</div>
<p>Now that we’ve identified the right strings for each block, we can do the next step of the approximation procedure: build the frequency distributions for the tokens that follow those strings, and sum them up. We’re going to be doing this several times over, so let’s define a function for it:</p>
<div id="cell-94" class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> frequency_distribution_from_similar_strings(</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>    similar_strings_per_block: Sequence[Sequence[Sequence[<span class="bu">str</span>]]],</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>    next_token_map: Dict[<span class="bu">str</span>, torch.Tensor],</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># freqs_per_block_per_query is a list of lists of tensors. The outer list has</span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># one item per block. The inner list has one item per query. Each</span></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># tensor is the next token frequency distribution for a particular</span></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># block and query.</span></span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>    freqs_per_block_per_query: List[List[torch.Tensor]] <span class="op">=</span> [[] <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_layer)]</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> block_idx <span class="kw">in</span> <span class="bu">range</span>(n_layer):</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> similar_strings <span class="kw">in</span> similar_strings_per_block[block_idx]:</span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a>            freqs_per_block_per_query[block_idx].append(</span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a>                torch.stack([next_token_map[string] <span class="cf">for</span> string <span class="kw">in</span> similar_strings]).<span class="bu">sum</span>(</span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a>                    dim<span class="op">=</span><span class="dv">0</span></span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb58-17"><a href="#cb58-17" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb58-18"><a href="#cb58-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-19"><a href="#cb58-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stack all frequency tensors into a single tensor of shape</span></span>
<span id="cb58-20"><a href="#cb58-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (n_layer, n_queries, vocab_size)</span></span>
<span id="cb58-21"><a href="#cb58-21" aria-hidden="true" tabindex="-1"></a>    freqs <span class="op">=</span> torch.stack(</span>
<span id="cb58-22"><a href="#cb58-22" aria-hidden="true" tabindex="-1"></a>        [</span>
<span id="cb58-23"><a href="#cb58-23" aria-hidden="true" tabindex="-1"></a>            torch.stack(freqs_per_block_per_query[block_idx])</span>
<span id="cb58-24"><a href="#cb58-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> block_idx <span class="kw">in</span> <span class="bu">range</span>(n_layer)</span>
<span id="cb58-25"><a href="#cb58-25" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb58-26"><a href="#cb58-26" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb58-27"><a href="#cb58-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-28"><a href="#cb58-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> freqs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This function, <code>frequency_distribution_from_similar_strings()</code>, does the equivalent of this code we looked at earlier:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>total_freq_distribution <span class="op">=</span> torch.stack([</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    next_token_map10[string] <span class="cf">for</span> string <span class="kw">in</span> similar_strings[<span class="dv">0</span>]</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>]).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>But with two key differences:</p>
<ul>
<li>It does this calculation for all the blocks, using the similar strings we found for each block above.</li>
<li>It allows for more than one query. In the code we’ve looked at so far, we only evaluated the approximation for a single prompt. In the next section, we’ll be running it for lots of prompts so I’ve written the code in a more general form to a allow for this. Specifically, the code allows for <code>similar_strings_per_block</code> to contain not just a single list of strings per block but multiple: one for each query.</li>
</ul>
<p>Let’s run this on the <code>similar_strings_per_block</code> we constructed earlier:</p>
<div id="cell-96" class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>freq_distribution <span class="op">=</span> frequency_distribution_from_similar_strings(</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    similar_strings_per_block,</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>    next_token_map10,</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>freq_distribution.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([6, 1, 65])</code></pre>
</div>
</div>
<p>It produces a tensor of shape <code>(6, 1, 65)</code>: 6 blocks, 1 query, 65 tokens in the vocabulary. If we’d been working with more queries, the middle dimension would be larger.</p>
<p>So now we have a frequency distribution for each block, based on the strings found for each block using the similarity thresholds. We now need to turn this into a probability distribution.</p>
<p>Earlier, when we just had a single frequency distribution for a single block, we just normalized it. But now we have multiple frequency distributions - one for each block - and need to combine them. In my experiments, I found that a weighted sum of these distributions produced the best results.</p>
<p>As with the similarity thresholds, I was able to find a set of good weights by trial and error. I also tried a deep-learning approach to find weights, but did not get better results than with the hand-tuned approach. The procedure for both hand-tuning and learning weights is implemented in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/40_widening_similar_space.ipynb">the similar space notebook</a>, the same one used for tuning thresholds.</p>
<p>For now, let’s use the optimal weights I found:</p>
<div id="cell-98" class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> torch.tensor([<span class="fl">0.01</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">1.5</span>, <span class="dv">6</span>, <span class="fl">0.01</span>]).unsqueeze(dim<span class="op">=</span><span class="dv">1</span>).unsqueeze(dim<span class="op">=</span><span class="dv">2</span>) <span class="co"># (n_layer, 1, 1)</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>total_freq_distribution <span class="op">=</span> (freq_distribution <span class="op">*</span> weights).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>prob_distribution <span class="op">=</span> total_freq_distribution <span class="op">/</span> total_freq_distribution.<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We multiply the frequency distributions by the weights, sum across all blocks, and then normalize into a probability distribution. We can now look at how the approximation’s distribution compares to the model’s.</p>
<blockquote class="blockquote">
<p>Note: in the code below, we have to index into the <code>prob_distribution</code> tensor with <code>[0]</code> because its first dimension is the number of queries. We’re only working with a single query, so we can just take the first element.</p>
</blockquote>
<div id="cell-100" class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>approx_top_tokens <span class="op">=</span> top_nonzero_tokens(prob_distribution[<span class="dv">0</span>], tokenizer.itos)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> encoding_helpers.tokenize_string(prompts[q_idx])</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>logits, _ <span class="op">=</span> m(tokens)</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> LogitsWrapper(logits.detach(), tokenizer)</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>model_top_tokens <span class="op">=</span> logits.topk_tokens(k<span class="op">=</span><span class="dv">10</span>)[<span class="dv">0</span>][<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>print_distribution_comparison(approx_top_tokens, model_top_tokens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            i: 0.363            
o: 0.204            o: 0.265            
a: 0.195            a: 0.213            
e: 0.160            e: 0.147            
u: 0.004            u: 0.011            
l: 0.000            y: 0.000            
</code></pre>
</div>
</div>
<div id="cell-101" class="cell">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>hellinger_distance(prob_distribution[<span class="dv">0</span>], logits.probs()[<span class="dv">0</span>][<span class="op">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(0.0731)</code></pre>
</div>
</div>
<p>In this particular case, adding the other layers didn’t change the approximation much (if anything, it’s very slightly worse based on Hellinger distance). But let’s look at the example that didn’t work well when we considered just the first layer: prompt id 40 (<code>'hing tremb'</code>).</p>
<div id="cell-103" class="cell">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>similarity_thresholds<span class="op">=</span>[<span class="fl">0.95</span>, <span class="fl">0.94</span>, <span class="fl">0.85</span>, <span class="fl">0.76</span>, <span class="fl">0.81</span>, <span class="fl">0.89</span>]</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>q_idx <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>similar_strings_per_block <span class="op">=</span> []</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> block_idx <span class="kw">in</span> <span class="bu">range</span>(n_layer):</span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>    similar_indices <span class="op">=</span> filter_on_prefiltered_results(</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>        load_prefiltered<span class="op">=</span><span class="kw">lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>        q_idx_start<span class="op">=</span>q_idx,</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>        q_idx_end<span class="op">=</span>q_idx<span class="op">+</span><span class="dv">1</span>,</span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>        filter_fn<span class="op">=</span><span class="kw">lambda</span> values: values <span class="op">&gt;</span> similarity_thresholds[block_idx]</span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>    similar_strings <span class="op">=</span> [</span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>        [strings10[i] <span class="cf">for</span> i <span class="kw">in</span> indices]</span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> indices <span class="kw">in</span> similar_indices</span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb67-17"><a href="#cb67-17" aria-hidden="true" tabindex="-1"></a>    similar_strings_per_block.append(similar_strings)</span>
<span id="cb67-18"><a href="#cb67-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-19"><a href="#cb67-19" aria-hidden="true" tabindex="-1"></a>freq_distribution <span class="op">=</span> frequency_distribution_from_similar_strings(</span>
<span id="cb67-20"><a href="#cb67-20" aria-hidden="true" tabindex="-1"></a>    similar_strings_per_block,</span>
<span id="cb67-21"><a href="#cb67-21" aria-hidden="true" tabindex="-1"></a>    next_token_map10,</span>
<span id="cb67-22"><a href="#cb67-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb67-23"><a href="#cb67-23" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> torch.tensor([<span class="fl">0.01</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">1.5</span>, <span class="dv">6</span>, <span class="fl">0.01</span>]).unsqueeze(dim<span class="op">=</span><span class="dv">1</span>).unsqueeze(dim<span class="op">=</span><span class="dv">2</span>) <span class="co"># (n_layer, 1, 1)</span></span>
<span id="cb67-24"><a href="#cb67-24" aria-hidden="true" tabindex="-1"></a>total_freq_distribution <span class="op">=</span> (freq_distribution <span class="op">*</span> weights).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb67-25"><a href="#cb67-25" aria-hidden="true" tabindex="-1"></a>prob_distribution <span class="op">=</span> total_freq_distribution <span class="op">/</span> total_freq_distribution.<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb67-26"><a href="#cb67-26" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> encoding_helpers.tokenize_string(prompts[q_idx])</span>
<span id="cb67-27"><a href="#cb67-27" aria-hidden="true" tabindex="-1"></a>logits, _ <span class="op">=</span> m(tokens)</span>
<span id="cb67-28"><a href="#cb67-28" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> LogitsWrapper(logits.detach(), tokenizer)</span>
<span id="cb67-29"><a href="#cb67-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-30"><a href="#cb67-30" aria-hidden="true" tabindex="-1"></a>approx_top_tokens <span class="op">=</span> top_nonzero_tokens(prob_distribution[<span class="dv">0</span>], tokenizer.itos)</span>
<span id="cb67-31"><a href="#cb67-31" aria-hidden="true" tabindex="-1"></a>model_top_tokens <span class="op">=</span> logits.topk_tokens(k<span class="op">=</span><span class="dv">10</span>)[<span class="dv">0</span>][<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb67-32"><a href="#cb67-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-33"><a href="#cb67-33" aria-hidden="true" tabindex="-1"></a>print_distribution_comparison(approx_top_tokens, model_top_tokens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
l: 0.999            l: 0.997            
e: 0.000            e: 0.002            
r: 0.000            r: 0.000            
</code></pre>
</div>
</div>
<div id="cell-104" class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>hellinger_distance(prob_distribution, logits.probs()[<span class="dv">0</span>][<span class="op">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([0.0233])</code></pre>
</div>
</div>
<p>Remember that for this example, when we used just the first layer’s similar strings, the approximation was quite different from the model’s prediction and had a Hellinger distance of &gt;0.63. Now it’s nearly identical and has a Hellinger distance of 0.02. So using the rest of the layers really helped this example.</p>
<p>In the next section, we’ll extend the code to evaluate the approximation over the whole set of 20,000 prompts. The section after that will look at how well the approximation does across all the prompts.</p>
</section>
<section id="extending-to-all-20000-prompts" class="level3">
<h3 class="anchored" data-anchor-id="extending-to-all-20000-prompts">Extending to All 20,000 Prompts</h3>
<p>We now have all the pieces we need to run the approximation procedure for all 20,000 prompts. First, let’s find the strings with similar feed-forward network outputs for all the prompts, for all blocks:</p>
<div id="cell-108" class="cell">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Takes about 7 minutes to run</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>similarity_thresholds<span class="op">=</span>[<span class="fl">0.95</span>, <span class="fl">0.94</span>, <span class="fl">0.85</span>, <span class="fl">0.76</span>, <span class="fl">0.81</span>, <span class="fl">0.89</span>]</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>similar_strings_per_block <span class="op">=</span> []</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> block_idx <span class="kw">in</span> <span class="bu">range</span>(n_layer):</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>    similar_indices <span class="op">=</span> filter_on_prefiltered_results(</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>        load_prefiltered<span class="op">=</span><span class="kw">lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>        q_idx_start<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>        q_idx_end<span class="op">=</span>n_prompts,</span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>        filter_fn<span class="op">=</span><span class="kw">lambda</span> values: values <span class="op">&gt;</span> similarity_thresholds[block_idx]</span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a>    similar_strings <span class="op">=</span> [</span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>        [strings10[i] <span class="cf">for</span> i <span class="kw">in</span> indices]</span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> indices <span class="kw">in</span> similar_indices</span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb71-18"><a href="#cb71-18" aria-hidden="true" tabindex="-1"></a>    similar_strings_per_block.append(similar_strings)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we compute the frequency distributions for each query based on the strings we found, perform the weighted sum, and normalize to produce a probability distribution.</p>
<div id="cell-110" class="cell">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>freq_distribution <span class="op">=</span> frequency_distribution_from_similar_strings(</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>    similar_strings_per_block,</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>    next_token_map10,</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> torch.tensor([<span class="fl">0.01</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">1.5</span>, <span class="dv">6</span>, <span class="fl">0.01</span>]).unsqueeze(dim<span class="op">=</span><span class="dv">1</span>).unsqueeze(dim<span class="op">=</span><span class="dv">2</span>) <span class="co"># (n_layer, 1, 1)</span></span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>total_freq_distribution <span class="op">=</span> (freq_distribution <span class="op">*</span> weights).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>prob_distribution <span class="op">=</span> total_freq_distribution <span class="op">/</span> total_freq_distribution.<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>prob_distribution.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([20000, 65])</code></pre>
</div>
</div>
<p>The output is a tensor of shape (20000, 65): one 65-entry distribution for each of 20,000 prompts.</p>
<p>In order to compare, we need to run all the prompts through the model and get the output probability distributions the model predicts:</p>
<div id="cell-112" class="cell">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> encoding_helpers.tokenize_strings(prompts)</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>logits, _ <span class="op">=</span> m(tokens)</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> LogitsWrapper(logits.detach(), tokenizer)</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>model_probs <span class="op">=</span> logits.probs()</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>model_probs <span class="op">=</span> model_probs[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># We're only interested in the last token</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we have outputs from the approximation and from the model for all prompts. In the next section, we’ll measure the Hellinger distance between them and evaluate the results.</p>
</section>
</section>
<section id="evaluating-the-approximation" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-the-approximation">Evaluating the Approximation</h2>
<p>In earlier sections, we compared output from the approximation to output from the model for individual prompts. Now that we have both outputs for all prompts, we can compare them and look at aggregate results.</p>
<p>First, we can compute the Hellinger distance between the approximation and the model’s prediction for each prompt:</p>
<div id="cell-116" class="cell">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> hellinger_distance(prob_distribution, model_probs)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>h.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([20000])</code></pre>
</div>
</div>
<p>This produced 20,000 Hellinger distance scores, one for each prompt. We can start by looking at some basic stats:</p>
<div id="cell-118" class="cell">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>h.mean(), h.std(), h.<span class="bu">min</span>(), h.<span class="bu">max</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor(0.1677), tensor(0.1215), tensor(0.0013), tensor(0.9994))</code></pre>
</div>
</div>
<p>The average Hellinger distance is just below 0.17, with a standard deviation of around 0.12, suggesting a distribution that skews low (a good thing). We’ve also got at least one really excellent sample (a min of 0.0013) and at least one really terrible one (max of 0.9994).</p>
<p>Let’s look at the distribution:</p>
<div id="cell-120" class="cell">
<details class="code-fold">
<summary>Code for distribution plot</summary>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>sns.kdeplot(h.numpy(), fill<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Distribution of Hellinger Distance across 20,000 Samples (Transformer vs Approximation)"</span>)</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-54-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Indeed, the distribution is skewed left, indicating most queries have Hellinger distance scores on the lower end.</p>
<p>The numbers and the distribution graph look promising, but is the approximation really a good one? It’s hard to say without something to compare against and it’s not obvious what a good comparison might be.</p>
<p>A thought experiment: let’s imagine that for some prompt, the model produced a distribution that looked like this:</p>
<div id="cell-123" class="cell">
<details class="code-fold">
<summary>Code to generate first imagined distribution</summary>
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>imagined_dist <span class="op">=</span> torch.zeros(tokenizer.vocab_size)</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>imagined_dist[tokenizer.stoi[<span class="st">'b'</span>]] <span class="op">=</span> <span class="fl">0.49</span></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>imagined_dist[tokenizer.stoi[<span class="st">'d'</span>]] <span class="op">=</span> <span class="fl">0.51</span></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>plot_prob_distribution_for_tokens(imagined_dist, title<span class="op">=</span><span class="st">'Imagined Distribution'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-55-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The tokens <code>b</code> and <code>d</code> have nearly the same predicted probability (0.49 vs 0.51). The model predicts an approximately equal chance of these tokens coming next. Now imagine our approximation, or another model, predicted this distribution:</p>
<div id="cell-125" class="cell">
<details class="code-fold">
<summary>Code to generate second imagined distribution</summary>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>imagined_dist2 <span class="op">=</span> torch.zeros(tokenizer.vocab_size)</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>imagined_dist2[tokenizer.stoi[<span class="st">'b'</span>]] <span class="op">=</span> <span class="fl">0.51</span></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>imagined_dist2[tokenizer.stoi[<span class="st">'d'</span>]] <span class="op">=</span> <span class="fl">0.49</span></span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>plot_prob_distribution_for_tokens(imagined_dist2, title<span class="op">=</span><span class="st">'Imagined Distribution 2'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-56-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Nearly the same, but the probabilities are reversed: <code>b</code> has probability 0.51 and <code>d</code> has 0.49. Would we care about this difference? Clearly both distributions are saying that <code>b</code> and <code>d</code> are about equally likely. If used for inference, either distribution would probably produce acceptable results. For most use cases I could imagine, the difference would just be noise.</p>
<p>The Hellinger distance between the two imagined distributions above is 0.0141. Not zero, but we’re saying it doesn’t matter for practical purposes. If 0.0141 is a Hellinger distance that doesn’t matter much, what about 0.02? Or 0.025? We can imagine there is some threshold Hellinger distance below which we wouldn’t care and above which we would consider distributions to be meaningfully different. What is that threshold value?</p>
<p>If we knew it, then we could look at how close the average Hellinger distance between our approximation’s predictions and model’s come to this threshold. That would be a measure of the goodness of the approximation.</p>
<p>I did an experiment to estimate what the threshold is. I trained the same transformer architecture three more times, starting with a different random seed each time and stopping at approximately the same training and validation loss as I did for the original model. This gave me three alternative transformers with roughly the same performance, but with different weights due to the different random initial starting points:</p>
<table class="table">
<colgroup>
<col style="width: 9%">
<col style="width: 18%">
<col style="width: 34%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Seed</th>
<th>Est. Training Loss</th>
<th>Est. Validation Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Original Model</td>
<td>1337</td>
<td>0.9334</td>
<td>1.5063</td>
</tr>
<tr class="even">
<td>Alternate 1</td>
<td>1442</td>
<td>0.9293</td>
<td>1.5038</td>
</tr>
<tr class="odd">
<td>Alternate 2</td>
<td>88</td>
<td>0.9294</td>
<td>1.4991</td>
</tr>
<tr class="even">
<td>Alternate 3</td>
<td>99999</td>
<td>0.9339</td>
<td>1.4941</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p>I used the same training/validation sets, hyperparameters, optimizer, etc. for the three alternate models as for the original model. The training code and output for the alternate models is in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/alternate-models.ipynb">the <code>alternate-models</code> experiment notebook</a>. Training code for the original model is at the end of <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer.ipynb">the main transformer notebook</a>.</p>
</blockquote>
<p>I then ran the same 20,000 prompts through the alternative models and calculated the Hellinger distance between their outputs and that of the original model. <a href="#ii-evaluation-of-main-model-vs-3-alternate-models">Appendix II</a> shows the code used to do this. The table below shows the aggregate results.</p>
<table class="table">
<thead>
<tr class="header">
<th>Comparison</th>
<th>Mean Hellinger Distance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Original vs Alternate 1</td>
<td>0.1064 ± 0.0823</td>
</tr>
<tr class="even">
<td>Original vs Alternate 2</td>
<td>0.1057 ± 0.0817</td>
</tr>
<tr class="odd">
<td>Original vs Alternate 3</td>
<td>0.1053 ± 0.0828</td>
</tr>
</tbody>
</table>
<p>The original model and the three alternate models are “equivalent” in the sense that they perform about equally well in terms of training and validation loss. I could have used any of them as the basis for this post. In other words, the differences between them likely aren’t meaningful - just noise.</p>
<p>Across all three alternate models, the average Hellinger distance was ~0.11 ± 0.08. We only have 3 data points, so it’s not a perfect measure, but ~0.11 is probably a reasonable lower bound for the threshold Hellinger distance we are looking for.</p>
<p>For comparison, the average Hellinger distance between the model and the approximation was ~0.17. A little higher than 0.11, but within a standard deviation.</p>
<p>Plotting the distributions of the various Hellinger distances shows this nicely:</p>
<div id="cell-131" class="cell">
<details class="code-fold">
<summary>Code to generate plot of Hellinger Distance Distributions</summary>
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>alt_models_dir <span class="op">=</span> environment.data_root <span class="op">/</span> <span class="st">'alternate-models/model-training/20240112-training/outputs/'</span></span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> alt_models_dir.exists(), <span class="st">"Alternate models directory does not exist. Run the training code in ../experiments/alternate-models.ipynb."</span></span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate the three alternative trained models</span></span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a>m_alt1, _ <span class="op">=</span> create_model_and_tokenizer(</span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>    saved_model_filename<span class="op">=</span>alt_models_dir <span class="op">/</span> <span class="st">'shakespeare-20240112-1.pt'</span>,</span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>    dataset<span class="op">=</span>ts,</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>device,</span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a>m_alt2, _ <span class="op">=</span> create_model_and_tokenizer(</span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a>    saved_model_filename<span class="op">=</span>alt_models_dir <span class="op">/</span> <span class="st">'shakespeare-20240112-2.pt'</span>,</span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a>    dataset<span class="op">=</span>ts,</span>
<span id="cb82-13"><a href="#cb82-13" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>device,</span>
<span id="cb82-14"><a href="#cb82-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb82-15"><a href="#cb82-15" aria-hidden="true" tabindex="-1"></a>m_alt3, _ <span class="op">=</span> create_model_and_tokenizer(</span>
<span id="cb82-16"><a href="#cb82-16" aria-hidden="true" tabindex="-1"></a>    saved_model_filename<span class="op">=</span>alt_models_dir <span class="op">/</span> <span class="st">'shakespeare-20240112-3.pt'</span>,</span>
<span id="cb82-17"><a href="#cb82-17" aria-hidden="true" tabindex="-1"></a>    dataset<span class="op">=</span>ts,</span>
<span id="cb82-18"><a href="#cb82-18" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>device,</span>
<span id="cb82-19"><a href="#cb82-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb82-20"><a href="#cb82-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-21"><a href="#cb82-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_model_probs(model: TransformerLanguageModel, tokens: torch.Tensor):</span>
<span id="cb82-22"><a href="#cb82-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Helper function to get a model's output probability distribution for a</span></span>
<span id="cb82-23"><a href="#cb82-23" aria-hidden="true" tabindex="-1"></a><span class="co">    batch of tokenized inputs."""</span></span>
<span id="cb82-24"><a href="#cb82-24" aria-hidden="true" tabindex="-1"></a>    logits, _ <span class="op">=</span> model(tokens)</span>
<span id="cb82-25"><a href="#cb82-25" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> LogitsWrapper(logits.detach(), tokenizer)</span>
<span id="cb82-26"><a href="#cb82-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logits.probs()[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># We'e only interested in the last token</span></span>
<span id="cb82-27"><a href="#cb82-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-28"><a href="#cb82-28" aria-hidden="true" tabindex="-1"></a>alt_model_probs1 <span class="op">=</span> get_model_probs(m_alt1, tokens)</span>
<span id="cb82-29"><a href="#cb82-29" aria-hidden="true" tabindex="-1"></a>alt_model_probs2 <span class="op">=</span> get_model_probs(m_alt2, tokens)</span>
<span id="cb82-30"><a href="#cb82-30" aria-hidden="true" tabindex="-1"></a>alt_model_probs3 <span class="op">=</span> get_model_probs(m_alt3, tokens)</span>
<span id="cb82-31"><a href="#cb82-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-32"><a href="#cb82-32" aria-hidden="true" tabindex="-1"></a>h_alt1 <span class="op">=</span> hellinger_distance(model_probs, alt_model_probs1)</span>
<span id="cb82-33"><a href="#cb82-33" aria-hidden="true" tabindex="-1"></a>h_alt2 <span class="op">=</span> hellinger_distance(model_probs, alt_model_probs2)</span>
<span id="cb82-34"><a href="#cb82-34" aria-hidden="true" tabindex="-1"></a>h_alt3 <span class="op">=</span> hellinger_distance(model_probs, alt_model_probs3)</span>
<span id="cb82-35"><a href="#cb82-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-36"><a href="#cb82-36" aria-hidden="true" tabindex="-1"></a>_, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb82-37"><a href="#cb82-37" aria-hidden="true" tabindex="-1"></a>sns.kdeplot(data<span class="op">=</span>h_alt1.numpy(), fill<span class="op">=</span><span class="va">False</span>, color<span class="op">=</span><span class="st">'blue'</span>, ax<span class="op">=</span>ax, label<span class="op">=</span><span class="st">'vs alternate model 1'</span>)</span>
<span id="cb82-38"><a href="#cb82-38" aria-hidden="true" tabindex="-1"></a>sns.kdeplot(data<span class="op">=</span>h_alt2.numpy(), fill<span class="op">=</span><span class="va">False</span>, color<span class="op">=</span><span class="st">'green'</span>, ax<span class="op">=</span>ax, label<span class="op">=</span><span class="st">'vs alternate model 2'</span>)</span>
<span id="cb82-39"><a href="#cb82-39" aria-hidden="true" tabindex="-1"></a>sns.kdeplot(data<span class="op">=</span>h_alt3.numpy(), fill<span class="op">=</span><span class="va">False</span>, color<span class="op">=</span><span class="st">'orange'</span>, ax<span class="op">=</span>ax, label<span class="op">=</span><span class="st">'vs alternate model 3'</span>)</span>
<span id="cb82-40"><a href="#cb82-40" aria-hidden="true" tabindex="-1"></a>sns.kdeplot(data<span class="op">=</span>h.numpy(), fill<span class="op">=</span><span class="va">False</span>, color<span class="op">=</span><span class="st">'red'</span>, ax<span class="op">=</span>ax, label<span class="op">=</span><span class="st">'vs approximation'</span>)</span>
<span id="cb82-41"><a href="#cb82-41" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.legend()</span>
<span id="cb82-42"><a href="#cb82-42" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plt.title(<span class="st">"Distribution of Hellinger Distance between Original Model Ouput and Alternate Model/Approximation Outputs across 20,000 Samples"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-57-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>There is clearly less deviation between the alternates and the original model than between the approximation and the original model, but it’s not wildly different. I think this result suggests the approximation is quite good. Most of the difference is within the “acceptable noise” threshold.</p>
</section>
<section id="interpretation-why-does-the-approximation-work" class="level2">
<h2 class="anchored" data-anchor-id="interpretation-why-does-the-approximation-work">Interpretation: Why Does the Approximation Work?</h2>
<p>The analysis in the previous section shows that the outputs of the approximation are quite similar to the transformer’s outputs. But that doesn’t necessarily mean that the approximation procedure is similar to what the transformer is actually doing. The approximation and the transformer might just represent two different ways of computing the same result.</p>
<p>My intuition is that this is not the case: <strong>I think the approximation is at least something like what the transformer is doing</strong>. In this section, I’ll break down <em>how</em> I think the transformer computes something similar to the approximation and then present some supporting evidence.</p>
<p>The key ideas are:</p>
<ul>
<li>The transformer, as its name suggests*, performs a series of transformations on its embedded input. The transformer blocks transform embeddings within embedding space and the final linear layer at the end transforms from embedding space to logit space.</li>
<li>Within each transformer block, the transformation from input to output embedding is done via vector addition: the block’s output embedding is its input embedding plus the output of the self-attention layer, plus the output of the feed-forward network. Of the two added components, the feed-forward network output value is dominant in determining the final output.</li>
<li>Within embedding space, subspaces exist that correspond to specific tokens. An embedding within the subspace for a particular token produces an output distribution in which all the probability is concentrated on that token (that token has probability near 1 and all other tokens have probability near 0). Embeddings that lie between the subspaces for multiple tokens result in outputs that distribute all the probability across those tokens.</li>
<li>The feed-forward network output at each block is an “adjustment vector” that orients the block output towards the subspaces for the tokens that the approximation procedure would predict: those that follow the strings in the training corpus that produce similar feed-forward network outputs at that block.</li>
</ul>
<p>In the subsections below, I’ll go into each of these ideas in more detail.</p>
<blockquote class="blockquote">
<p>*It’s unclear whether the name “transformer” alludes to transforming an input sequence to an output sequence (the use case in the original paper was machine translation) or the transformations within the layers of the model.</p>
</blockquote>
<section id="the-model-is-a-series-of-transformations" class="level3">
<h3 class="anchored" data-anchor-id="the-model-is-a-series-of-transformations">The Model is a Series of Transformations</h3>
<p>Once the input to the model has been embedded, we can view the model as a series of transformations:</p>
<div id="cell-134" class="cell">
<details class="code-fold">
<summary>Drawing code for diagram showing transformations within the model</summary>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformationsDiagram(Scene):</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> construct(<span class="va">self</span>):</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>        blocks <span class="op">=</span> VGroup(</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span>[Rectangle(height<span class="op">=</span><span class="fl">0.4</span>, width<span class="op">=</span><span class="dv">3</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_layer)],</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>        ).arrange(DOWN, buff<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a>        empty_rect <span class="op">=</span> Rectangle(height<span class="op">=</span><span class="fl">0.2</span>, width<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span>BLACK)</span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>        out_emb_text <span class="op">=</span> Text(<span class="st">"Output Embedding"</span>, font<span class="op">=</span><span class="st">"Arial"</span>).scale(<span class="fl">0.4</span>)</span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a>        lm_head_blocks <span class="op">=</span> [Rectangle(height<span class="op">=</span><span class="fl">0.4</span>, width<span class="op">=</span><span class="dv">3</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)]</span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-13"><a href="#cb83-13" aria-hidden="true" tabindex="-1"></a>        logits_text <span class="op">=</span> Text(<span class="st">"Logits"</span>, font<span class="op">=</span><span class="st">"Arial"</span>).scale(<span class="fl">0.4</span>)</span>
<span id="cb83-14"><a href="#cb83-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-15"><a href="#cb83-15" aria-hidden="true" tabindex="-1"></a>        softmax_block <span class="op">=</span> Rectangle(height<span class="op">=</span><span class="fl">0.4</span>, width<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb83-16"><a href="#cb83-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-17"><a href="#cb83-17" aria-hidden="true" tabindex="-1"></a>        arch <span class="op">=</span> VGroup(</span>
<span id="cb83-18"><a href="#cb83-18" aria-hidden="true" tabindex="-1"></a>            blocks,</span>
<span id="cb83-19"><a href="#cb83-19" aria-hidden="true" tabindex="-1"></a>            empty_rect,</span>
<span id="cb83-20"><a href="#cb83-20" aria-hidden="true" tabindex="-1"></a>            out_emb_text,</span>
<span id="cb83-21"><a href="#cb83-21" aria-hidden="true" tabindex="-1"></a>            empty_rect.copy(),</span>
<span id="cb83-22"><a href="#cb83-22" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span>lm_head_blocks,</span>
<span id="cb83-23"><a href="#cb83-23" aria-hidden="true" tabindex="-1"></a>            empty_rect.copy(),</span>
<span id="cb83-24"><a href="#cb83-24" aria-hidden="true" tabindex="-1"></a>            logits_text,</span>
<span id="cb83-25"><a href="#cb83-25" aria-hidden="true" tabindex="-1"></a>            empty_rect.copy(),</span>
<span id="cb83-26"><a href="#cb83-26" aria-hidden="true" tabindex="-1"></a>            softmax_block,</span>
<span id="cb83-27"><a href="#cb83-27" aria-hidden="true" tabindex="-1"></a>            empty_rect.copy(),</span>
<span id="cb83-28"><a href="#cb83-28" aria-hidden="true" tabindex="-1"></a>        ).arrange(DOWN, buff<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb83-29"><a href="#cb83-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(arch)</span>
<span id="cb83-30"><a href="#cb83-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-31"><a href="#cb83-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Text at the top</span></span>
<span id="cb83-32"><a href="#cb83-32" aria-hidden="true" tabindex="-1"></a>        in_emb_text <span class="op">=</span> (</span>
<span id="cb83-33"><a href="#cb83-33" aria-hidden="true" tabindex="-1"></a>            Text(<span class="st">"Input Embedding"</span>, font<span class="op">=</span><span class="st">"Arial"</span>)</span>
<span id="cb83-34"><a href="#cb83-34" aria-hidden="true" tabindex="-1"></a>            .scale(<span class="fl">0.4</span>)</span>
<span id="cb83-35"><a href="#cb83-35" aria-hidden="true" tabindex="-1"></a>            .next_to(blocks[<span class="dv">0</span>], UP, buff<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb83-36"><a href="#cb83-36" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb83-37"><a href="#cb83-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(in_emb_text)</span>
<span id="cb83-38"><a href="#cb83-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-39"><a href="#cb83-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Block labels</span></span>
<span id="cb83-40"><a href="#cb83-40" aria-hidden="true" tabindex="-1"></a>        block_labels <span class="op">=</span> [</span>
<span id="cb83-41"><a href="#cb83-41" aria-hidden="true" tabindex="-1"></a>            Text(<span class="ss">f"Block </span><span class="sc">{</span>b<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>, font<span class="op">=</span><span class="st">"Arial"</span>)</span>
<span id="cb83-42"><a href="#cb83-42" aria-hidden="true" tabindex="-1"></a>            .move_to(blocks[b].get_center())</span>
<span id="cb83-43"><a href="#cb83-43" aria-hidden="true" tabindex="-1"></a>            .scale(<span class="fl">0.4</span>)</span>
<span id="cb83-44"><a href="#cb83-44" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(n_layer)</span>
<span id="cb83-45"><a href="#cb83-45" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb83-46"><a href="#cb83-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(<span class="op">*</span>block_labels)</span>
<span id="cb83-47"><a href="#cb83-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-48"><a href="#cb83-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output block labels</span></span>
<span id="cb83-49"><a href="#cb83-49" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> [<span class="st">"Layer Norm"</span>, <span class="st">"Linear"</span>]</span>
<span id="cb83-50"><a href="#cb83-50" aria-hidden="true" tabindex="-1"></a>        lm_head_labels <span class="op">=</span> [</span>
<span id="cb83-51"><a href="#cb83-51" aria-hidden="true" tabindex="-1"></a>            Text(<span class="ss">f"</span><span class="sc">{</span>l<span class="sc">}</span><span class="ss">"</span>, font<span class="op">=</span><span class="st">"Arial"</span>)</span>
<span id="cb83-52"><a href="#cb83-52" aria-hidden="true" tabindex="-1"></a>            .move_to(lm_head_blocks[b].get_center())</span>
<span id="cb83-53"><a href="#cb83-53" aria-hidden="true" tabindex="-1"></a>            .scale(<span class="fl">0.4</span>)</span>
<span id="cb83-54"><a href="#cb83-54" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> b, l <span class="kw">in</span> <span class="bu">enumerate</span>(labels)</span>
<span id="cb83-55"><a href="#cb83-55" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb83-56"><a href="#cb83-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(<span class="op">*</span>lm_head_labels)</span>
<span id="cb83-57"><a href="#cb83-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-58"><a href="#cb83-58" aria-hidden="true" tabindex="-1"></a>        softmax_block_label <span class="op">=</span> (</span>
<span id="cb83-59"><a href="#cb83-59" aria-hidden="true" tabindex="-1"></a>            Text(<span class="st">"Softmax"</span>, font<span class="op">=</span><span class="st">"Arial"</span>).move_to(softmax_block.get_center()).scale(<span class="fl">0.4</span>)</span>
<span id="cb83-60"><a href="#cb83-60" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb83-61"><a href="#cb83-61" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(softmax_block_label)</span>
<span id="cb83-62"><a href="#cb83-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-63"><a href="#cb83-63" aria-hidden="true" tabindex="-1"></a>        arrow_buffer <span class="op">=</span> np.array([<span class="fl">0.0</span>, <span class="fl">0.05</span>, <span class="fl">0.0</span>])</span>
<span id="cb83-64"><a href="#cb83-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-65"><a href="#cb83-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow between input embeddings and blocks</span></span>
<span id="cb83-66"><a href="#cb83-66" aria-hidden="true" tabindex="-1"></a>        in_emb_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>)</span>
<span id="cb83-67"><a href="#cb83-67" aria-hidden="true" tabindex="-1"></a>        in_emb_arrow.put_start_and_end_on(</span>
<span id="cb83-68"><a href="#cb83-68" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>in_emb_text.get_bottom() <span class="op">-</span> arrow_buffer,</span>
<span id="cb83-69"><a href="#cb83-69" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>blocks[<span class="dv">0</span>].get_top(),</span>
<span id="cb83-70"><a href="#cb83-70" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb83-71"><a href="#cb83-71" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(in_emb_arrow)</span>
<span id="cb83-72"><a href="#cb83-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-73"><a href="#cb83-73" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow between last block and output embedding label</span></span>
<span id="cb83-74"><a href="#cb83-74" aria-hidden="true" tabindex="-1"></a>        blocks_to_out_emb_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>)</span>
<span id="cb83-75"><a href="#cb83-75" aria-hidden="true" tabindex="-1"></a>        blocks_to_out_emb_arrow.put_start_and_end_on(</span>
<span id="cb83-76"><a href="#cb83-76" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>blocks[<span class="op">-</span><span class="dv">1</span>].get_bottom(),</span>
<span id="cb83-77"><a href="#cb83-77" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>out_emb_text.get_top() <span class="op">+</span> arrow_buffer,</span>
<span id="cb83-78"><a href="#cb83-78" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb83-79"><a href="#cb83-79" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(blocks_to_out_emb_arrow)</span>
<span id="cb83-80"><a href="#cb83-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-81"><a href="#cb83-81" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow between output embedding label and LM head blocks</span></span>
<span id="cb83-82"><a href="#cb83-82" aria-hidden="true" tabindex="-1"></a>        out_embs_to_layer_norm_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>)</span>
<span id="cb83-83"><a href="#cb83-83" aria-hidden="true" tabindex="-1"></a>        out_embs_to_layer_norm_arrow.put_start_and_end_on(</span>
<span id="cb83-84"><a href="#cb83-84" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>out_emb_text.get_bottom() <span class="op">-</span> arrow_buffer,</span>
<span id="cb83-85"><a href="#cb83-85" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>lm_head_blocks[<span class="dv">0</span>].get_top(),</span>
<span id="cb83-86"><a href="#cb83-86" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb83-87"><a href="#cb83-87" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(out_embs_to_layer_norm_arrow)</span>
<span id="cb83-88"><a href="#cb83-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-89"><a href="#cb83-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow between LM head blocks and logits</span></span>
<span id="cb83-90"><a href="#cb83-90" aria-hidden="true" tabindex="-1"></a>        out_blocks_to_logits_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>)</span>
<span id="cb83-91"><a href="#cb83-91" aria-hidden="true" tabindex="-1"></a>        out_blocks_to_logits_arrow.put_start_and_end_on(</span>
<span id="cb83-92"><a href="#cb83-92" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>lm_head_blocks[<span class="op">-</span><span class="dv">1</span>].get_bottom(),</span>
<span id="cb83-93"><a href="#cb83-93" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>logits_text.get_top() <span class="op">+</span> arrow_buffer,</span>
<span id="cb83-94"><a href="#cb83-94" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb83-95"><a href="#cb83-95" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(out_blocks_to_logits_arrow)</span>
<span id="cb83-96"><a href="#cb83-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-97"><a href="#cb83-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow between logits and softmax</span></span>
<span id="cb83-98"><a href="#cb83-98" aria-hidden="true" tabindex="-1"></a>        logits_to_softmax_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>)</span>
<span id="cb83-99"><a href="#cb83-99" aria-hidden="true" tabindex="-1"></a>        logits_to_softmax_arrow.put_start_and_end_on(</span>
<span id="cb83-100"><a href="#cb83-100" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>logits_text.get_bottom() <span class="op">-</span> arrow_buffer,</span>
<span id="cb83-101"><a href="#cb83-101" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>softmax_block.get_top(),</span>
<span id="cb83-102"><a href="#cb83-102" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb83-103"><a href="#cb83-103" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(logits_to_softmax_arrow)</span>
<span id="cb83-104"><a href="#cb83-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-105"><a href="#cb83-105" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Text at the bottom</span></span>
<span id="cb83-106"><a href="#cb83-106" aria-hidden="true" tabindex="-1"></a>        out_prob_text <span class="op">=</span> (</span>
<span id="cb83-107"><a href="#cb83-107" aria-hidden="true" tabindex="-1"></a>            Text(<span class="st">"Output Probabilities"</span>, font<span class="op">=</span><span class="st">"Arial"</span>)</span>
<span id="cb83-108"><a href="#cb83-108" aria-hidden="true" tabindex="-1"></a>            .scale(<span class="fl">0.4</span>)</span>
<span id="cb83-109"><a href="#cb83-109" aria-hidden="true" tabindex="-1"></a>            .next_to(softmax_block[<span class="op">-</span><span class="dv">1</span>], DOWN, buff<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb83-110"><a href="#cb83-110" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb83-111"><a href="#cb83-111" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(out_prob_text)</span>
<span id="cb83-112"><a href="#cb83-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-113"><a href="#cb83-113" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow between softmax and output probs</span></span>
<span id="cb83-114"><a href="#cb83-114" aria-hidden="true" tabindex="-1"></a>        out_prob_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>)</span>
<span id="cb83-115"><a href="#cb83-115" aria-hidden="true" tabindex="-1"></a>        out_prob_arrow.put_start_and_end_on(</span>
<span id="cb83-116"><a href="#cb83-116" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>softmax_block.get_bottom(),</span>
<span id="cb83-117"><a href="#cb83-117" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>out_prob_text.get_top(),</span>
<span id="cb83-118"><a href="#cb83-118" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb83-119"><a href="#cb83-119" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(out_prob_arrow)</span>
<span id="cb83-120"><a href="#cb83-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-121"><a href="#cb83-121" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Brace next to blocks</span></span>
<span id="cb83-122"><a href="#cb83-122" aria-hidden="true" tabindex="-1"></a>        blocks_brace <span class="op">=</span> BraceLabel(</span>
<span id="cb83-123"><a href="#cb83-123" aria-hidden="true" tabindex="-1"></a>            blocks,</span>
<span id="cb83-124"><a href="#cb83-124" aria-hidden="true" tabindex="-1"></a>            brace_direction<span class="op">=</span>LEFT,</span>
<span id="cb83-125"><a href="#cb83-125" aria-hidden="true" tabindex="-1"></a>            text<span class="op">=</span><span class="st">"transformations within</span><span class="ch">\n</span><span class="st">embedding space"</span>,</span>
<span id="cb83-126"><a href="#cb83-126" aria-hidden="true" tabindex="-1"></a>            label_constructor<span class="op">=</span><span class="kw">lambda</span> text, <span class="op">**</span>kwargs: Text(</span>
<span id="cb83-127"><a href="#cb83-127" aria-hidden="true" tabindex="-1"></a>                text, font<span class="op">=</span><span class="st">"Arial"</span>, <span class="op">**</span>kwargs</span>
<span id="cb83-128"><a href="#cb83-128" aria-hidden="true" tabindex="-1"></a>            ).scale(<span class="fl">0.4</span>),</span>
<span id="cb83-129"><a href="#cb83-129" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb83-130"><a href="#cb83-130" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(blocks_brace)</span>
<span id="cb83-131"><a href="#cb83-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-132"><a href="#cb83-132" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Brace next to linear layer block</span></span>
<span id="cb83-133"><a href="#cb83-133" aria-hidden="true" tabindex="-1"></a>        linear_layer_brace <span class="op">=</span> BraceLabel(</span>
<span id="cb83-134"><a href="#cb83-134" aria-hidden="true" tabindex="-1"></a>            lm_head_blocks[<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb83-135"><a href="#cb83-135" aria-hidden="true" tabindex="-1"></a>            brace_direction<span class="op">=</span>LEFT,</span>
<span id="cb83-136"><a href="#cb83-136" aria-hidden="true" tabindex="-1"></a>            text<span class="op">=</span><span class="st">"transformation from </span><span class="ch">\n</span><span class="st">embedding space to</span><span class="ch">\n</span><span class="st">logit space"</span>,</span>
<span id="cb83-137"><a href="#cb83-137" aria-hidden="true" tabindex="-1"></a>            label_constructor<span class="op">=</span><span class="kw">lambda</span> text, <span class="op">**</span>kwargs: Text(</span>
<span id="cb83-138"><a href="#cb83-138" aria-hidden="true" tabindex="-1"></a>                text, font<span class="op">=</span><span class="st">"Arial"</span>, <span class="op">**</span>kwargs</span>
<span id="cb83-139"><a href="#cb83-139" aria-hidden="true" tabindex="-1"></a>            ).scale(<span class="fl">0.4</span>),</span>
<span id="cb83-140"><a href="#cb83-140" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb83-141"><a href="#cb83-141" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(linear_layer_brace)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-135" class="cell">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-59-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The sequence of 6 transformer blocks takes a tensor in embedding space (<span class="math inline">\(\mathbb{R}^{384}\)</span>, since <code>n_embed=384</code>) as input and outputs another tensor in embedding space. In this sense, represents a transformation <em>within</em> embedding space. In fact, each transformer block is itself a transformation within embedding space and the stack of all 6 blocks composes these individual transformations. It isn’t literally implemented this way in code, but its equivalent to:</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>output_embedding <span class="op">=</span> block6(block5(block4(block3(block2(block1(input_embedding))))))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>At the end of the sequence of blocks, the model sends the output embedding through a LayerNorm operation and then a linear layer that transforms from embedding space into logit space (<span class="math inline">\(\mathbb{R}^{65}\)</span>, since <code>vocab_size=65</code>). Finally, the softmax layer at the end turns the logits into probabilities for the next token.</p>
</section>
<section id="transformation-via-vector-addition" class="level3">
<h3 class="anchored" data-anchor-id="transformation-via-vector-addition">Transformation via Vector Addition</h3>
<p>We looked at the internal logic within a transformer block in the earlier <a href="#transformer-block-structure">Transformer Block Structure</a> section. To recap, the <code>forward()</code> method of the <a href="https://spather.github.io/transformer-experiments/models/transformer.html#block"><code>Block</code></a> module looks like this:</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""One transformer block"""</span></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.sa(<span class="va">self</span>.ln1(x)) <span class="co"># The `x +` part is a skip connection</span></span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffwd(<span class="va">self</span>.ln2(x)) <span class="co"># The `x +` part is a skip connection</span></span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a id="block-logic-with-intermediates"></a> This is equivalent to the following code, which, by using some intermediate local variables, clarifies what’s really going on:</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>        sa_out <span class="op">=</span> <span class="va">self</span>.sa(<span class="va">self</span>.ln1(x))</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>        ffwd_out <span class="op">=</span> <span class="va">self</span>.ffwd(<span class="va">self</span>.ln2(x <span class="op">+</span> sa_out))</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">+</span> sa_out <span class="op">+</span> ffwd_out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>The output of the block is equal to the input (<code>x</code>), plus the self-attention output (<code>sa_out</code>), plus the feed forward network output (<a href="https://spather.github.io/transformer-experiments/experiments/similar-strings.html#ffwd_out"><code>ffwd_out</code></a>).</strong> We can think of the block as taking the input embedding, and then making two adjustments to it.</p>
<p>These values being added together are vectors in <span class="math inline">\(\mathbb{R}^{384}\)</span>. If we imagine the embedding space reduced to just two dimensions, it might look something like this:</p>
<div id="cell-138" class="cell">
<details class="code-fold">
<summary>Code to generate plot of of 1 block’s vector additions in 2D</summary>
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># These values are generated in ../analyses/70_embedding_adjustments.ipynb. They</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="co"># are 2D vectors that have the same norms and cosine similarity relationships as</span></span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a><span class="co"># the block intermediate values for q_idx=57.</span></span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>vectors <span class="op">=</span> [</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>    torch.tensor([ <span class="fl">0.7202</span>,  <span class="fl">0.6359</span>]),</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>    torch.tensor([<span class="op">-</span><span class="fl">1.2350</span>,  <span class="fl">1.5638</span>]),</span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>    torch.tensor([<span class="op">-</span><span class="fl">3.2156</span>,  <span class="fl">4.6376</span>]),</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>    torch.tensor([<span class="op">-</span><span class="fl">2.1867</span>, <span class="op">-</span><span class="fl">0.3706</span>]),</span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a>    torch.tensor([<span class="op">-</span><span class="fl">3.2978</span>,  <span class="fl">0.2411</span>]),</span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>    torch.tensor([<span class="op">-</span><span class="fl">2.1838</span>, <span class="op">-</span><span class="fl">1.4804</span>]),</span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>    torch.tensor([<span class="op">-</span><span class="fl">3.9066</span>, <span class="op">-</span><span class="fl">1.8245</span>]),</span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>    torch.tensor([<span class="op">-</span><span class="fl">1.1337</span>, <span class="op">-</span><span class="fl">2.6656</span>]),</span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>    torch.tensor([<span class="op">-</span><span class="fl">1.9595</span>, <span class="op">-</span><span class="fl">3.2921</span>]),</span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>    torch.tensor([<span class="op">-</span><span class="fl">0.1060</span>, <span class="op">-</span><span class="fl">2.4951</span>]),</span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a>    torch.tensor([<span class="op">-</span><span class="fl">0.9699</span>, <span class="op">-</span><span class="fl">4.8422</span>]),</span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a>    torch.tensor([ <span class="fl">0.8515</span>, <span class="op">-</span><span class="fl">2.2819</span>]),</span>
<span id="cb87-17"><a href="#cb87-17" aria-hidden="true" tabindex="-1"></a>    torch.tensor([ <span class="fl">0.5064</span>, <span class="op">-</span><span class="fl">5.7759</span>])</span>
<span id="cb87-18"><a href="#cb87-18" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb87-19"><a href="#cb87-19" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'r'</span>, <span class="op">*</span>[<span class="st">'g'</span> <span class="cf">if</span> i <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">'b'</span> <span class="cf">for</span> i, _ <span class="kw">in</span> <span class="bu">enumerate</span>(vectors)]]</span>
<span id="cb87-20"><a href="#cb87-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-21"><a href="#cb87-21" aria-hidden="true" tabindex="-1"></a>_, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb87-22"><a href="#cb87-22" aria-hidden="true" tabindex="-1"></a>prev <span class="op">=</span> torch.tensor([<span class="fl">0.0</span>, <span class="fl">0.0</span>])</span>
<span id="cb87-23"><a href="#cb87-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-24"><a href="#cb87-24" aria-hidden="true" tabindex="-1"></a>min_val <span class="op">=</span> prev.<span class="bu">min</span>().item()</span>
<span id="cb87-25"><a href="#cb87-25" aria-hidden="true" tabindex="-1"></a>max_val <span class="op">=</span> prev.<span class="bu">max</span>().item()</span>
<span id="cb87-26"><a href="#cb87-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-27"><a href="#cb87-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> vector, color <span class="kw">in</span> <span class="bu">zip</span>(vectors[:<span class="dv">3</span>], colors[:<span class="dv">3</span>]):</span>
<span id="cb87-28"><a href="#cb87-28" aria-hidden="true" tabindex="-1"></a>    ax.quiver(prev[<span class="dv">0</span>], prev[<span class="dv">1</span>], vector[<span class="dv">0</span>], vector[<span class="dv">1</span>], angles<span class="op">=</span><span class="st">'xy'</span>, scale_units<span class="op">=</span><span class="st">'xy'</span>, scale<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span>color)</span>
<span id="cb87-29"><a href="#cb87-29" aria-hidden="true" tabindex="-1"></a>    prev <span class="op">=</span> prev <span class="op">+</span> vector</span>
<span id="cb87-30"><a href="#cb87-30" aria-hidden="true" tabindex="-1"></a>    min_val <span class="op">=</span> <span class="bu">min</span>(min_val, prev.<span class="bu">min</span>().item())</span>
<span id="cb87-31"><a href="#cb87-31" aria-hidden="true" tabindex="-1"></a>    max_val <span class="op">=</span> <span class="bu">max</span>(max_val, prev.<span class="bu">max</span>().item())</span>
<span id="cb87-32"><a href="#cb87-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-33"><a href="#cb87-33" aria-hidden="true" tabindex="-1"></a>ax.set_aspect(<span class="st">'equal'</span>)</span>
<span id="cb87-34"><a href="#cb87-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-35"><a href="#cb87-35" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(min_val<span class="op">-</span><span class="dv">1</span>, max_val<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb87-36"><a href="#cb87-36" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(min_val<span class="op">-</span><span class="dv">1</span>, max_val<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb87-37"><a href="#cb87-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-38"><a href="#cb87-38" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>)</span>
<span id="cb87-39"><a href="#cb87-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-40"><a href="#cb87-40" aria-hidden="true" tabindex="-1"></a>endpoint <span class="op">=</span> <span class="bu">sum</span>(vectors[:<span class="dv">3</span>])</span>
<span id="cb87-41"><a href="#cb87-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-42"><a href="#cb87-42" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.quiver(<span class="dv">0</span>, <span class="dv">0</span>, endpoint[<span class="dv">0</span>], endpoint[<span class="dv">1</span>], angles<span class="op">=</span><span class="st">'xy'</span>, scale_units<span class="op">=</span><span class="st">'xy'</span>, scale<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'gray'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-60-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The red vector represents the input embedding. The green vector represents the self-attention output (<code>sa_out</code> in code), and the blue vector represents the feed-forward network output (<a href="https://spather.github.io/transformer-experiments/experiments/similar-strings.html#ffwd_out"><code>ffwd_out</code></a> in code). The gray arrow represent the final sum, or the output of the first block: where you end up when you arrange the individual vectors tip to tail.</p>
<p>The plot above shows the additions that happen within just one block. Subsequent blocks add their self-attention outputs and feed-forward network outputs, starting from the output of this block. If we add the vectors from those other blocks to the diagram, it looks like this:</p>
<div id="cell-140" class="cell">
<details class="code-fold">
<summary>Code to generate plot of all blocks’ vector additions in 2D</summary>
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>_, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>prev <span class="op">=</span> torch.tensor([<span class="fl">0.0</span>, <span class="fl">0.0</span>])</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>min_val <span class="op">=</span> prev.<span class="bu">min</span>().item()</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>max_val <span class="op">=</span> prev.<span class="bu">max</span>().item()</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> vector, color <span class="kw">in</span> <span class="bu">zip</span>(vectors, colors):</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>    ax.quiver(prev[<span class="dv">0</span>], prev[<span class="dv">1</span>], vector[<span class="dv">0</span>], vector[<span class="dv">1</span>], angles<span class="op">=</span><span class="st">'xy'</span>, scale_units<span class="op">=</span><span class="st">'xy'</span>, scale<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span>color)</span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>    prev <span class="op">=</span> prev <span class="op">+</span> vector</span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>    min_val <span class="op">=</span> <span class="bu">min</span>(min_val, prev.<span class="bu">min</span>().item())</span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>    max_val <span class="op">=</span> <span class="bu">max</span>(max_val, prev.<span class="bu">max</span>().item())</span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a>ax.set_aspect(<span class="st">'equal'</span>)</span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-15"><a href="#cb88-15" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(min_val<span class="op">-</span><span class="dv">1</span>, max_val<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb88-16"><a href="#cb88-16" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(min_val<span class="op">-</span><span class="dv">1</span>, max_val<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb88-17"><a href="#cb88-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-18"><a href="#cb88-18" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>)</span>
<span id="cb88-19"><a href="#cb88-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-20"><a href="#cb88-20" aria-hidden="true" tabindex="-1"></a>endpoint <span class="op">=</span> <span class="bu">sum</span>(vectors)</span>
<span id="cb88-21"><a href="#cb88-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-22"><a href="#cb88-22" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.quiver(<span class="dv">0</span>, <span class="dv">0</span>, endpoint[<span class="dv">0</span>], endpoint[<span class="dv">1</span>], angles<span class="op">=</span><span class="st">'xy'</span>, scale_units<span class="op">=</span><span class="st">'xy'</span>, scale<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'gray'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-61-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Again, the red arrow represents the input vector, each green arrow represents one block’s self-attention output, each blue arrow represents one block’s feed-forward network output. Arranged tip to tail, their endpoint represents the final output from the stack of 6 blocks, depicted by the gray arrow.</p>
<p>Though it’s only in two dimensions, the diagram above is based on real data and is drawn “to scale”, in a way: the length of each 2D vector is the same as the <span class="math inline">\(\mathbb{R}^{384}\)</span> vector it represents for a real query (index 57). In addition, the cosine similarity between each 2D blue / green arrow and the sum of the arrows that precede it is the same as the cosine similarity between the corresponding self-attention/feed-forward network output and the block input in the real data.</p>
<blockquote class="blockquote">
<p>Code to generate the 2D representation from real data is in the <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb">embedding adjustments analysis notebook</a>.</p>
</blockquote>
<p>We can observe two interesting patterns:</p>
<ul>
<li>The feed-forward network outputs are generally longer than the self-attention outputs (the vectors have larger norms)</li>
<li>Within a given block, the feed-forward network output and the self-attention output point in roughly the same direction.</li>
</ul>
<p>Look at what happens when we eliminate the self-attention outputs from the vector sum, leaving just the feed-forward network outputs:</p>
<div id="cell-142" class="cell">
<details class="code-fold">
<summary>Code to generate plot of overlaying just the feed-forward vectors added</summary>
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>_, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>prev <span class="op">=</span> torch.tensor([<span class="fl">0.0</span>, <span class="fl">0.0</span>])</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>min_val <span class="op">=</span> prev.<span class="bu">min</span>().item()</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>max_val <span class="op">=</span> prev.<span class="bu">max</span>().item()</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> vector, color <span class="kw">in</span> <span class="bu">zip</span>(vectors, colors):</span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>    ax.quiver(prev[<span class="dv">0</span>], prev[<span class="dv">1</span>], vector[<span class="dv">0</span>], vector[<span class="dv">1</span>], angles<span class="op">=</span><span class="st">'xy'</span>, scale_units<span class="op">=</span><span class="st">'xy'</span>, scale<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span>color)</span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>    prev <span class="op">=</span> prev <span class="op">+</span> vector</span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>    min_val <span class="op">=</span> <span class="bu">min</span>(min_val, prev.<span class="bu">min</span>().item())</span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>    max_val <span class="op">=</span> <span class="bu">max</span>(max_val, prev.<span class="bu">max</span>().item())</span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a>ffwd_vectors <span class="op">=</span> [v <span class="cf">for</span> i, v <span class="kw">in</span> <span class="bu">enumerate</span>(vectors[<span class="dv">1</span>:]) <span class="cf">if</span> i <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">1</span>]</span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a>prev <span class="op">=</span> vectors[<span class="dv">0</span>]</span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> vector <span class="kw">in</span> ffwd_vectors:</span>
<span id="cb89-16"><a href="#cb89-16" aria-hidden="true" tabindex="-1"></a>    ax.quiver(prev[<span class="dv">0</span>], prev[<span class="dv">1</span>], vector[<span class="dv">0</span>], vector[<span class="dv">1</span>], angles<span class="op">=</span><span class="st">'xy'</span>, scale_units<span class="op">=</span><span class="st">'xy'</span>, scale<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb89-17"><a href="#cb89-17" aria-hidden="true" tabindex="-1"></a>    prev <span class="op">=</span> prev <span class="op">+</span> vector</span>
<span id="cb89-18"><a href="#cb89-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-19"><a href="#cb89-19" aria-hidden="true" tabindex="-1"></a>ax.set_aspect(<span class="st">'equal'</span>)</span>
<span id="cb89-20"><a href="#cb89-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-21"><a href="#cb89-21" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(min_val<span class="op">-</span><span class="dv">1</span>, max_val<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb89-22"><a href="#cb89-22" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(min_val<span class="op">-</span><span class="dv">1</span>, max_val<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb89-23"><a href="#cb89-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-24"><a href="#cb89-24" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>)</span>
<span id="cb89-25"><a href="#cb89-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-26"><a href="#cb89-26" aria-hidden="true" tabindex="-1"></a>endpoint <span class="op">=</span> <span class="bu">sum</span>(vectors)</span>
<span id="cb89-27"><a href="#cb89-27" aria-hidden="true" tabindex="-1"></a>endpoint_ffwd <span class="op">=</span> vectors[<span class="dv">0</span>] <span class="op">+</span> <span class="bu">sum</span>(ffwd_vectors)</span>
<span id="cb89-28"><a href="#cb89-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-29"><a href="#cb89-29" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.quiver(<span class="dv">0</span>, <span class="dv">0</span>, endpoint[<span class="dv">0</span>], endpoint[<span class="dv">1</span>], angles<span class="op">=</span><span class="st">'xy'</span>, scale_units<span class="op">=</span><span class="st">'xy'</span>, scale<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb89-30"><a href="#cb89-30" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> ax.quiver(<span class="dv">0</span>, <span class="dv">0</span>, endpoint_ffwd[<span class="dv">0</span>], endpoint_ffwd[<span class="dv">1</span>], angles<span class="op">=</span><span class="st">'xy'</span>, scale_units<span class="op">=</span><span class="st">'xy'</span>, scale<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'gray'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-62-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The inner blue curve in the above plot represents the sum of the input vector and only the feed-forward network outputs from each block. The tip to tail arrangement of these vectors ends at a point far from where the previous arrangement (including the self-attention outputs) ended. But notice that the feed-forward-only endpoint (shorter gray arrow) is quite closely aligned in <em>direction</em> with the original endpoint (longer gray arrow).</p>
<p>This plot shows values for only one query and we lose a lot of information dropping from 384 dimensions to 2. But the pattern does seem to hold in general and in the full, high-dimensional embedding space. The <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb">embedding adjustments analysis notebook</a> provides a deep-dive into this phenomenon across all 20,000 queries.</p>
<p>The takeaway is that <strong>simplifying the transformation performed by the blocks to just the contributions of the feed-forward networks results in an output vector that is shorter (has a smaller norm) than the original output but points in roughly the same direction</strong>. And the difference in norms would have no impact on the transformer’s final output, because of the LayerNorm operation after the stack of blocks. That LayerNorm step will adjust the norm of any input vector to similar value regardless of its initial magnitude; the final linear layer that follows it will always see inputs of approximately the same norm (see <a href="#iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">Appendix III</a> for a walk-through of this).</p>
<p>An important clarification: I’m not suggesting that we could remove the self attention computation from the transformer. The feed-forward networks take the self-attention output as part of their input (<code>ffwd_out = self.ffwd(self.ln2(x +</code><strong><code>sa_out</code></strong><code>))</code>); they would compute very different values were the self-attention outputs removed. What I am saying is that, after all block processing has been completed as normal including the self-attention computations, we get roughly the same result if we consider only the feed-forward network contributions, as our approximation does. This is probably because the feed-forward network outputs pass on some of the information they receive as input from the self-attention output.</p>
<p>For some additional evidence that an approximation based only on feed-forward network outputs can produce similar outputs to the transformer, see <a href="#iv-summary-of-experiment-on-relative-impact-of-self-attention-and-feed-forward-network-outputs">Appendix IV</a>.</p>
</section>
<section id="token-subspaces" class="level3">
<h3 class="anchored" data-anchor-id="token-subspaces">Token Subspaces</h3>
<p>In the examples we’ve seen so far, the model outputs have been distributions that include significant non-zero probabilities for several tokens. For example:</p>
<div id="cell-145" class="cell">
<details class="code-fold">
<summary>Code to generate plot of containing non-zero probabilities for several tokens</summary>
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> encoding_helpers.tokenize_string(<span class="st">'And only l'</span>)</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>logits, _ <span class="op">=</span> m(tokens)</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> LogitsWrapper(logits.detach(), tokenizer)</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>logits.plot_probs(title<span class="op">=</span><span class="st">'Next Token Probability Distribution for Prompt "And only l"'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-63-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Though we haven’t seen one yet, we might wonder whether <strong>specific inputs</strong> exist that compel the model to predict a <strong>single token</strong> with <strong>near certainty</strong>. In other words, do some inputs cause the model to output a probability distribution in which just one token has probability very near 1 and all other tokens very near zero? Such a distribution might look like this:</p>
<div id="cell-147" class="cell">
<details class="code-fold">
<summary>Code to generate distribution in which one token has all the probability mass</summary>
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>imagined_dist <span class="op">=</span> torch.zeros(tokenizer.vocab_size)</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>imagined_dist <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>imagined_dist[tokenizer.stoi[<span class="st">'a'</span>]] <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>imagined_dist <span class="op">=</span> F.softmax(imagined_dist, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>plot_prob_distribution_for_tokens(imagined_dist, title<span class="op">=</span><span class="st">'Distribution in Which One Token Has Probability Near 1'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-64-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In fact, we can ask this question about any stage of the model. “Input” doesn’t have to refer to the initial input to the model, but could be the input to any layer within the model. For example, consider only the layers that transform the final block’s output embedding to logit space (the final LayerNorm and linear layers):</p>
<div id="cell-149" class="cell">
<details class="code-fold">
<summary>Drawing code for diagram showing block 6 output to final layers</summary>
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> OnlyOutputLayersDiagram(Scene):</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> construct(<span class="va">self</span>):</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>        empty_rect <span class="op">=</span> Rectangle(height<span class="op">=</span><span class="fl">0.2</span>, width<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span>BLACK)</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>        out_emb_text <span class="op">=</span> Text(<span class="st">"Block 6 Output Embedding"</span>, font<span class="op">=</span><span class="st">"Arial"</span>).scale(<span class="fl">0.4</span>)</span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>        lm_head_blocks <span class="op">=</span> [Rectangle(height<span class="op">=</span><span class="fl">0.4</span>, width<span class="op">=</span><span class="dv">3</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)]</span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a>        logits_text <span class="op">=</span> Text(<span class="st">"Logits"</span>, font<span class="op">=</span><span class="st">"Arial"</span>).scale(<span class="fl">0.4</span>)</span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a>        softmax_block <span class="op">=</span> Rectangle(height<span class="op">=</span><span class="fl">0.4</span>, width<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb92-13"><a href="#cb92-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-14"><a href="#cb92-14" aria-hidden="true" tabindex="-1"></a>        arch <span class="op">=</span> VGroup(</span>
<span id="cb92-15"><a href="#cb92-15" aria-hidden="true" tabindex="-1"></a>            empty_rect,</span>
<span id="cb92-16"><a href="#cb92-16" aria-hidden="true" tabindex="-1"></a>            out_emb_text,</span>
<span id="cb92-17"><a href="#cb92-17" aria-hidden="true" tabindex="-1"></a>            empty_rect.copy(),</span>
<span id="cb92-18"><a href="#cb92-18" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span>lm_head_blocks,</span>
<span id="cb92-19"><a href="#cb92-19" aria-hidden="true" tabindex="-1"></a>            empty_rect.copy(),</span>
<span id="cb92-20"><a href="#cb92-20" aria-hidden="true" tabindex="-1"></a>            logits_text,</span>
<span id="cb92-21"><a href="#cb92-21" aria-hidden="true" tabindex="-1"></a>            empty_rect.copy(),</span>
<span id="cb92-22"><a href="#cb92-22" aria-hidden="true" tabindex="-1"></a>            softmax_block,</span>
<span id="cb92-23"><a href="#cb92-23" aria-hidden="true" tabindex="-1"></a>            empty_rect.copy(),</span>
<span id="cb92-24"><a href="#cb92-24" aria-hidden="true" tabindex="-1"></a>        ).arrange(DOWN, buff<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb92-25"><a href="#cb92-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(arch)</span>
<span id="cb92-26"><a href="#cb92-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-27"><a href="#cb92-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-28"><a href="#cb92-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output block labels</span></span>
<span id="cb92-29"><a href="#cb92-29" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> [<span class="st">"Layer Norm"</span>, <span class="st">"Linear"</span>]</span>
<span id="cb92-30"><a href="#cb92-30" aria-hidden="true" tabindex="-1"></a>        lm_head_labels <span class="op">=</span> [</span>
<span id="cb92-31"><a href="#cb92-31" aria-hidden="true" tabindex="-1"></a>            Text(<span class="ss">f"</span><span class="sc">{</span>l<span class="sc">}</span><span class="ss">"</span>, font<span class="op">=</span><span class="st">"Arial"</span>)</span>
<span id="cb92-32"><a href="#cb92-32" aria-hidden="true" tabindex="-1"></a>            .move_to(lm_head_blocks[b].get_center())</span>
<span id="cb92-33"><a href="#cb92-33" aria-hidden="true" tabindex="-1"></a>            .scale(<span class="fl">0.4</span>)</span>
<span id="cb92-34"><a href="#cb92-34" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> b, l <span class="kw">in</span> <span class="bu">enumerate</span>(labels)</span>
<span id="cb92-35"><a href="#cb92-35" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb92-36"><a href="#cb92-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(<span class="op">*</span>lm_head_labels)</span>
<span id="cb92-37"><a href="#cb92-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-38"><a href="#cb92-38" aria-hidden="true" tabindex="-1"></a>        softmax_block_label <span class="op">=</span> (</span>
<span id="cb92-39"><a href="#cb92-39" aria-hidden="true" tabindex="-1"></a>            Text(<span class="st">"Softmax"</span>, font<span class="op">=</span><span class="st">"Arial"</span>).move_to(softmax_block.get_center()).scale(<span class="fl">0.4</span>)</span>
<span id="cb92-40"><a href="#cb92-40" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb92-41"><a href="#cb92-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(softmax_block_label)</span>
<span id="cb92-42"><a href="#cb92-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-43"><a href="#cb92-43" aria-hidden="true" tabindex="-1"></a>        arrow_buffer <span class="op">=</span> np.array([<span class="fl">0.0</span>, <span class="fl">0.05</span>, <span class="fl">0.0</span>])</span>
<span id="cb92-44"><a href="#cb92-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-45"><a href="#cb92-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow between output embedding label and LM head blocks</span></span>
<span id="cb92-46"><a href="#cb92-46" aria-hidden="true" tabindex="-1"></a>        out_embs_to_layer_norm_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>)</span>
<span id="cb92-47"><a href="#cb92-47" aria-hidden="true" tabindex="-1"></a>        out_embs_to_layer_norm_arrow.put_start_and_end_on(</span>
<span id="cb92-48"><a href="#cb92-48" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>out_emb_text.get_bottom() <span class="op">-</span> arrow_buffer,</span>
<span id="cb92-49"><a href="#cb92-49" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>lm_head_blocks[<span class="dv">0</span>].get_top(),</span>
<span id="cb92-50"><a href="#cb92-50" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb92-51"><a href="#cb92-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(out_embs_to_layer_norm_arrow)</span>
<span id="cb92-52"><a href="#cb92-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-53"><a href="#cb92-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow between LM head blocks and logits</span></span>
<span id="cb92-54"><a href="#cb92-54" aria-hidden="true" tabindex="-1"></a>        out_blocks_to_logits_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>)</span>
<span id="cb92-55"><a href="#cb92-55" aria-hidden="true" tabindex="-1"></a>        out_blocks_to_logits_arrow.put_start_and_end_on(</span>
<span id="cb92-56"><a href="#cb92-56" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>lm_head_blocks[<span class="op">-</span><span class="dv">1</span>].get_bottom(),</span>
<span id="cb92-57"><a href="#cb92-57" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>logits_text.get_top() <span class="op">+</span> arrow_buffer,</span>
<span id="cb92-58"><a href="#cb92-58" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb92-59"><a href="#cb92-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(out_blocks_to_logits_arrow)</span>
<span id="cb92-60"><a href="#cb92-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-61"><a href="#cb92-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow between logits and softmax</span></span>
<span id="cb92-62"><a href="#cb92-62" aria-hidden="true" tabindex="-1"></a>        logits_to_softmax_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>)</span>
<span id="cb92-63"><a href="#cb92-63" aria-hidden="true" tabindex="-1"></a>        logits_to_softmax_arrow.put_start_and_end_on(</span>
<span id="cb92-64"><a href="#cb92-64" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>logits_text.get_bottom() <span class="op">-</span> arrow_buffer,</span>
<span id="cb92-65"><a href="#cb92-65" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>softmax_block.get_top(),</span>
<span id="cb92-66"><a href="#cb92-66" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb92-67"><a href="#cb92-67" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(logits_to_softmax_arrow)</span>
<span id="cb92-68"><a href="#cb92-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-69"><a href="#cb92-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Text at the bottom</span></span>
<span id="cb92-70"><a href="#cb92-70" aria-hidden="true" tabindex="-1"></a>        out_prob_text <span class="op">=</span> (</span>
<span id="cb92-71"><a href="#cb92-71" aria-hidden="true" tabindex="-1"></a>            Text(<span class="st">"Output Probabilities"</span>, font<span class="op">=</span><span class="st">"Arial"</span>)</span>
<span id="cb92-72"><a href="#cb92-72" aria-hidden="true" tabindex="-1"></a>            .scale(<span class="fl">0.4</span>)</span>
<span id="cb92-73"><a href="#cb92-73" aria-hidden="true" tabindex="-1"></a>            .next_to(softmax_block[<span class="op">-</span><span class="dv">1</span>], DOWN, buff<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb92-74"><a href="#cb92-74" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb92-75"><a href="#cb92-75" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(out_prob_text)</span>
<span id="cb92-76"><a href="#cb92-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-77"><a href="#cb92-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow between softmax and output probs</span></span>
<span id="cb92-78"><a href="#cb92-78" aria-hidden="true" tabindex="-1"></a>        out_prob_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>)</span>
<span id="cb92-79"><a href="#cb92-79" aria-hidden="true" tabindex="-1"></a>        out_prob_arrow.put_start_and_end_on(</span>
<span id="cb92-80"><a href="#cb92-80" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>softmax_block.get_bottom(),</span>
<span id="cb92-81"><a href="#cb92-81" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>out_prob_text.get_top(),</span>
<span id="cb92-82"><a href="#cb92-82" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb92-83"><a href="#cb92-83" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(out_prob_arrow)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-150" class="cell">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-66-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Is there some embedding block 6 might emit that would yield an output probability distribution in which some token, say the letter <code>a</code>, has probability very near 1?</p>
<section id="learning-token-subspaces" class="level4">
<h4 class="anchored" data-anchor-id="learning-token-subspaces">Learning Token Subspaces</h4>
<p>With the right math, it may be possible to find this embedding analytically. But it’s also possible to “learn” (in the sense of deep learning) such an embedding. Here’s the basic idea:</p>
<ul>
<li>Pick a point in the transformer where the input to subsequent layers is an embedding. This could be the input to any of the transformer blocks, or the point right after the final block (as shown in the diagram above).</li>
<li>Pick a token to learn an embedding for.</li>
<li>Create an embedding tensor and initialize it with random values. This tensor is the parameter the learning algorithm will optimize; the weights of the transformer are fixed.</li>
<li>Execute a forward pass by evaluating the transformer from the selected point, using the embedding as input. This will produce some set of logits.</li>
<li>Compute <a href="https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/#nll">negative log likelihood loss</a> relative to the token we’re learning an embedding for.</li>
<li>Do a backward pass, updating the embedding tensor according to the gradients.</li>
</ul>
<p>My implementation of this is in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/learn-embeddings.ipynb">the learned embeddings notebook</a>. I used it to learn embeddings for all tokens at various stages of the model and saved them. We can load one - a learned embedding that produces a distribution giving token <code>a</code> probability almost 1 - and check that it does what we expect when given to the part of the model shown in the diagram above:</p>
<div id="cell-152" class="cell">
<details class="code-fold">
<summary>Code to generate plot of probabilities from learned embedding for <code>a</code></summary>
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>learned_embeddings_dir <span class="op">=</span> environment.data_root <span class="op">/</span> <span class="st">'learned_embeddings'</span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>emb_a <span class="op">=</span> torch.load(learned_embeddings_dir <span class="op">/</span> <span class="st">'no_blocks'</span> <span class="op">/</span> <span class="st">'lower_a.pt'</span>, map_location<span class="op">=</span>device)[<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> LogitsWrapper(accessors.logits_from_embedding(unsqueeze_emb(emb_a)), tokenizer)</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>logits.plot_probs(title<span class="op">=</span><span class="st">'Next Token Probability Distribution for From Learned Embedding for Token "a"'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-67-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As expected, all the probability mass is concentrated on <code>a</code>. Inference using this distribution would generate <code>a</code> with near certainty.</p>
<p>The same procedure can learn embeddings for use at other parts of the model. If we wanted to find an embedding for <code>a</code> that could be input to block 6, we could run the same learning algorithm but use this part of the transformer in the forward pass:</p>
<div id="cell-154" class="cell">
<details class="code-fold">
<summary>Drawing code for diagram showing block 6 onwards</summary>
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Block6OnwardsDiagram(Scene):</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> construct(<span class="va">self</span>):</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>        block_idx_start <span class="op">=</span> n_layer <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>        blocks <span class="op">=</span> VGroup(</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span>[Rectangle(height<span class="op">=</span><span class="fl">0.4</span>, width<span class="op">=</span><span class="dv">3</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(block_idx_start, n_layer)],</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>        ).arrange(DOWN, buff<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>        empty_rect <span class="op">=</span> Rectangle(height<span class="op">=</span><span class="fl">0.2</span>, width<span class="op">=</span><span class="dv">3</span>, color<span class="op">=</span>BLACK)</span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a>        out_emb_text <span class="op">=</span> Text(<span class="st">"Output Embedding"</span>, font<span class="op">=</span><span class="st">"Arial"</span>).scale(<span class="fl">0.4</span>)</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a>        lm_head_blocks <span class="op">=</span> [Rectangle(height<span class="op">=</span><span class="fl">0.4</span>, width<span class="op">=</span><span class="dv">3</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)]</span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a>        logits_text <span class="op">=</span> Text(<span class="st">"Logits"</span>, font<span class="op">=</span><span class="st">"Arial"</span>).scale(<span class="fl">0.4</span>)</span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a>        softmax_block <span class="op">=</span> Rectangle(height<span class="op">=</span><span class="fl">0.4</span>, width<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a>        arch <span class="op">=</span> VGroup(</span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a>            blocks,</span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a>            empty_rect,</span>
<span id="cb94-21"><a href="#cb94-21" aria-hidden="true" tabindex="-1"></a>            out_emb_text,</span>
<span id="cb94-22"><a href="#cb94-22" aria-hidden="true" tabindex="-1"></a>            empty_rect.copy(),</span>
<span id="cb94-23"><a href="#cb94-23" aria-hidden="true" tabindex="-1"></a>            <span class="op">*</span>lm_head_blocks,</span>
<span id="cb94-24"><a href="#cb94-24" aria-hidden="true" tabindex="-1"></a>            empty_rect.copy(),</span>
<span id="cb94-25"><a href="#cb94-25" aria-hidden="true" tabindex="-1"></a>            logits_text,</span>
<span id="cb94-26"><a href="#cb94-26" aria-hidden="true" tabindex="-1"></a>            empty_rect.copy(),</span>
<span id="cb94-27"><a href="#cb94-27" aria-hidden="true" tabindex="-1"></a>            softmax_block,</span>
<span id="cb94-28"><a href="#cb94-28" aria-hidden="true" tabindex="-1"></a>            empty_rect.copy(),</span>
<span id="cb94-29"><a href="#cb94-29" aria-hidden="true" tabindex="-1"></a>        ).arrange(DOWN, buff<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb94-30"><a href="#cb94-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(arch)</span>
<span id="cb94-31"><a href="#cb94-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-32"><a href="#cb94-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Text at the top</span></span>
<span id="cb94-33"><a href="#cb94-33" aria-hidden="true" tabindex="-1"></a>        in_emb_text <span class="op">=</span> (</span>
<span id="cb94-34"><a href="#cb94-34" aria-hidden="true" tabindex="-1"></a>            Text(<span class="st">"Block 5 Output Embedding"</span>, font<span class="op">=</span><span class="st">"Arial"</span>)</span>
<span id="cb94-35"><a href="#cb94-35" aria-hidden="true" tabindex="-1"></a>            .scale(<span class="fl">0.4</span>)</span>
<span id="cb94-36"><a href="#cb94-36" aria-hidden="true" tabindex="-1"></a>            .next_to(blocks[<span class="dv">0</span>], UP, buff<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb94-37"><a href="#cb94-37" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb94-38"><a href="#cb94-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(in_emb_text)</span>
<span id="cb94-39"><a href="#cb94-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-40"><a href="#cb94-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Block labels</span></span>
<span id="cb94-41"><a href="#cb94-41" aria-hidden="true" tabindex="-1"></a>        block_labels <span class="op">=</span> [</span>
<span id="cb94-42"><a href="#cb94-42" aria-hidden="true" tabindex="-1"></a>            Text(<span class="ss">f"Block </span><span class="sc">{</span>b<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>, font<span class="op">=</span><span class="st">"Arial"</span>)</span>
<span id="cb94-43"><a href="#cb94-43" aria-hidden="true" tabindex="-1"></a>            .move_to(blocks[b<span class="op">-</span>block_idx_start].get_center())</span>
<span id="cb94-44"><a href="#cb94-44" aria-hidden="true" tabindex="-1"></a>            .scale(<span class="fl">0.4</span>)</span>
<span id="cb94-45"><a href="#cb94-45" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(block_idx_start, n_layer)</span>
<span id="cb94-46"><a href="#cb94-46" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb94-47"><a href="#cb94-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(<span class="op">*</span>block_labels)</span>
<span id="cb94-48"><a href="#cb94-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-49"><a href="#cb94-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output block labels</span></span>
<span id="cb94-50"><a href="#cb94-50" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> [<span class="st">"Layer Norm"</span>, <span class="st">"Linear"</span>]</span>
<span id="cb94-51"><a href="#cb94-51" aria-hidden="true" tabindex="-1"></a>        lm_head_labels <span class="op">=</span> [</span>
<span id="cb94-52"><a href="#cb94-52" aria-hidden="true" tabindex="-1"></a>            Text(<span class="ss">f"</span><span class="sc">{</span>l<span class="sc">}</span><span class="ss">"</span>, font<span class="op">=</span><span class="st">"Arial"</span>)</span>
<span id="cb94-53"><a href="#cb94-53" aria-hidden="true" tabindex="-1"></a>            .move_to(lm_head_blocks[b].get_center())</span>
<span id="cb94-54"><a href="#cb94-54" aria-hidden="true" tabindex="-1"></a>            .scale(<span class="fl">0.4</span>)</span>
<span id="cb94-55"><a href="#cb94-55" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> b, l <span class="kw">in</span> <span class="bu">enumerate</span>(labels)</span>
<span id="cb94-56"><a href="#cb94-56" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb94-57"><a href="#cb94-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(<span class="op">*</span>lm_head_labels)</span>
<span id="cb94-58"><a href="#cb94-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-59"><a href="#cb94-59" aria-hidden="true" tabindex="-1"></a>        softmax_block_label <span class="op">=</span> (</span>
<span id="cb94-60"><a href="#cb94-60" aria-hidden="true" tabindex="-1"></a>            Text(<span class="st">"Softmax"</span>, font<span class="op">=</span><span class="st">"Arial"</span>).move_to(softmax_block.get_center()).scale(<span class="fl">0.4</span>)</span>
<span id="cb94-61"><a href="#cb94-61" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb94-62"><a href="#cb94-62" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(softmax_block_label)</span>
<span id="cb94-63"><a href="#cb94-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-64"><a href="#cb94-64" aria-hidden="true" tabindex="-1"></a>        arrow_buffer <span class="op">=</span> np.array([<span class="fl">0.0</span>, <span class="fl">0.05</span>, <span class="fl">0.0</span>])</span>
<span id="cb94-65"><a href="#cb94-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-66"><a href="#cb94-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow between input embeddings and blocks</span></span>
<span id="cb94-67"><a href="#cb94-67" aria-hidden="true" tabindex="-1"></a>        in_emb_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>)</span>
<span id="cb94-68"><a href="#cb94-68" aria-hidden="true" tabindex="-1"></a>        in_emb_arrow.put_start_and_end_on(</span>
<span id="cb94-69"><a href="#cb94-69" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>in_emb_text.get_bottom() <span class="op">-</span> arrow_buffer,</span>
<span id="cb94-70"><a href="#cb94-70" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>blocks[<span class="dv">0</span>].get_top(),</span>
<span id="cb94-71"><a href="#cb94-71" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb94-72"><a href="#cb94-72" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(in_emb_arrow)</span>
<span id="cb94-73"><a href="#cb94-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-74"><a href="#cb94-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow between last block and output embedding label</span></span>
<span id="cb94-75"><a href="#cb94-75" aria-hidden="true" tabindex="-1"></a>        blocks_to_out_emb_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>)</span>
<span id="cb94-76"><a href="#cb94-76" aria-hidden="true" tabindex="-1"></a>        blocks_to_out_emb_arrow.put_start_and_end_on(</span>
<span id="cb94-77"><a href="#cb94-77" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>blocks[<span class="op">-</span><span class="dv">1</span>].get_bottom(),</span>
<span id="cb94-78"><a href="#cb94-78" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>out_emb_text.get_top() <span class="op">+</span> arrow_buffer,</span>
<span id="cb94-79"><a href="#cb94-79" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb94-80"><a href="#cb94-80" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(blocks_to_out_emb_arrow)</span>
<span id="cb94-81"><a href="#cb94-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-82"><a href="#cb94-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow between output embedding label and LM head blocks</span></span>
<span id="cb94-83"><a href="#cb94-83" aria-hidden="true" tabindex="-1"></a>        out_embs_to_layer_norm_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>)</span>
<span id="cb94-84"><a href="#cb94-84" aria-hidden="true" tabindex="-1"></a>        out_embs_to_layer_norm_arrow.put_start_and_end_on(</span>
<span id="cb94-85"><a href="#cb94-85" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>out_emb_text.get_bottom() <span class="op">-</span> arrow_buffer,</span>
<span id="cb94-86"><a href="#cb94-86" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>lm_head_blocks[<span class="dv">0</span>].get_top(),</span>
<span id="cb94-87"><a href="#cb94-87" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb94-88"><a href="#cb94-88" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(out_embs_to_layer_norm_arrow)</span>
<span id="cb94-89"><a href="#cb94-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-90"><a href="#cb94-90" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow between LM head blocks and logits</span></span>
<span id="cb94-91"><a href="#cb94-91" aria-hidden="true" tabindex="-1"></a>        out_blocks_to_logits_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>)</span>
<span id="cb94-92"><a href="#cb94-92" aria-hidden="true" tabindex="-1"></a>        out_blocks_to_logits_arrow.put_start_and_end_on(</span>
<span id="cb94-93"><a href="#cb94-93" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>lm_head_blocks[<span class="op">-</span><span class="dv">1</span>].get_bottom(),</span>
<span id="cb94-94"><a href="#cb94-94" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>logits_text.get_top() <span class="op">+</span> arrow_buffer,</span>
<span id="cb94-95"><a href="#cb94-95" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb94-96"><a href="#cb94-96" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(out_blocks_to_logits_arrow)</span>
<span id="cb94-97"><a href="#cb94-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-98"><a href="#cb94-98" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow between logits and softmax</span></span>
<span id="cb94-99"><a href="#cb94-99" aria-hidden="true" tabindex="-1"></a>        logits_to_softmax_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>)</span>
<span id="cb94-100"><a href="#cb94-100" aria-hidden="true" tabindex="-1"></a>        logits_to_softmax_arrow.put_start_and_end_on(</span>
<span id="cb94-101"><a href="#cb94-101" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>logits_text.get_bottom() <span class="op">-</span> arrow_buffer,</span>
<span id="cb94-102"><a href="#cb94-102" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>softmax_block.get_top(),</span>
<span id="cb94-103"><a href="#cb94-103" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb94-104"><a href="#cb94-104" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(logits_to_softmax_arrow)</span>
<span id="cb94-105"><a href="#cb94-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-106"><a href="#cb94-106" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Text at the bottom</span></span>
<span id="cb94-107"><a href="#cb94-107" aria-hidden="true" tabindex="-1"></a>        out_prob_text <span class="op">=</span> (</span>
<span id="cb94-108"><a href="#cb94-108" aria-hidden="true" tabindex="-1"></a>            Text(<span class="st">"Output Probabilities"</span>, font<span class="op">=</span><span class="st">"Arial"</span>)</span>
<span id="cb94-109"><a href="#cb94-109" aria-hidden="true" tabindex="-1"></a>            .scale(<span class="fl">0.4</span>)</span>
<span id="cb94-110"><a href="#cb94-110" aria-hidden="true" tabindex="-1"></a>            .next_to(softmax_block[<span class="op">-</span><span class="dv">1</span>], DOWN, buff<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb94-111"><a href="#cb94-111" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb94-112"><a href="#cb94-112" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(out_prob_text)</span>
<span id="cb94-113"><a href="#cb94-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-114"><a href="#cb94-114" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Arrow between softmax and output probs</span></span>
<span id="cb94-115"><a href="#cb94-115" aria-hidden="true" tabindex="-1"></a>        out_prob_arrow <span class="op">=</span> Arrow(max_tip_length_to_length_ratio<span class="op">=</span><span class="fl">0.08</span>)</span>
<span id="cb94-116"><a href="#cb94-116" aria-hidden="true" tabindex="-1"></a>        out_prob_arrow.put_start_and_end_on(</span>
<span id="cb94-117"><a href="#cb94-117" aria-hidden="true" tabindex="-1"></a>            start<span class="op">=</span>softmax_block.get_bottom(),</span>
<span id="cb94-118"><a href="#cb94-118" aria-hidden="true" tabindex="-1"></a>            end<span class="op">=</span>out_prob_text.get_top(),</span>
<span id="cb94-119"><a href="#cb94-119" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb94-120"><a href="#cb94-120" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add(out_prob_arrow)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-155" class="cell">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-69-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It’s more computationally expensive to learn embeddings at earlier stages of the model because the optimizer has to contend with a larger computation graph involving operations from all included blocks. Thankfully, as I’ll explain <a href="#use-only-final-subspaces">shortly</a>, we need only the embeddings learned for the part of the transformer after all the blocks (embeddings that go straight into the final LayerNorm layer) to show how the transformer operates like the approximation.</p>
</section>
<section id="from-embeddings-to-subspaces" class="level4">
<h4 class="anchored" data-anchor-id="from-embeddings-to-subspaces">From Embeddings to Subspaces</h4>
<p>For any token, the procedure described in the previous section can learn an embedding that makes the model predict that token with probability near 1. It turns out <strong>there isn’t just one such embedding for each token.</strong> We can learn many different embeddings that all produce probability distributions that assign a given token nearly all the probability mass. It was easy to learn thousands of unique embeddings for every token in the vocabulary.</p>
<p>I think <strong>the model has learned a complex, non-linear embedding subspace corresponding to each token</strong>. Any embedding within that subspace results in an output distribution that assigns the token near certain probability. Each embedding I was able to learn is probably a point in the embedding subspace for the corresponding token.</p>
<p>If we imagine the full embedding space (<span class="math inline">\(\mathbb{R}^{384}\)</span>) reduced to <span class="math inline">\(\mathbb{R}^3\)</span> (and the complex subspaces reduced to 2D planes), it might look something like this:</p>
<div id="cell-159" class="cell">
<details class="code-fold">
<summary>Drawing code for diagram showing token subspaces</summary>
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define non-parallel, non-overlapping planes by using different sets of spanning vectors</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Subspace 1: Plane spanned by vectors [1, 1, 0] and [0, 1, 1]</span></span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>v1_1 <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>v1_2 <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>t1_1 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">10</span>)</span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>t1_2 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">10</span>)</span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>points1 <span class="op">=</span> np.array([s <span class="op">*</span> v1_1 <span class="op">+</span> t <span class="op">*</span> v1_2 <span class="cf">for</span> s <span class="kw">in</span> t1_1 <span class="cf">for</span> t <span class="kw">in</span> t1_2])</span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Subspace 2: Plane spanned by vectors [1, -1, 0] and [0, 0, 1]</span></span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a>v2_1 <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a>v2_2 <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb95-13"><a href="#cb95-13" aria-hidden="true" tabindex="-1"></a>t2_1 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">10</span>)</span>
<span id="cb95-14"><a href="#cb95-14" aria-hidden="true" tabindex="-1"></a>t2_2 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">10</span>)</span>
<span id="cb95-15"><a href="#cb95-15" aria-hidden="true" tabindex="-1"></a>points2 <span class="op">=</span> np.array([s <span class="op">*</span> v2_1 <span class="op">+</span> t <span class="op">*</span> v2_2 <span class="cf">for</span> s <span class="kw">in</span> t2_1 <span class="cf">for</span> t <span class="kw">in</span> t2_2])</span>
<span id="cb95-16"><a href="#cb95-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-17"><a href="#cb95-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Subspace 3: Plane spanned by vectors [1, 0, 1] and [0, 1, 0]</span></span>
<span id="cb95-18"><a href="#cb95-18" aria-hidden="true" tabindex="-1"></a>v3_1 <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb95-19"><a href="#cb95-19" aria-hidden="true" tabindex="-1"></a>v3_2 <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb95-20"><a href="#cb95-20" aria-hidden="true" tabindex="-1"></a>t3_1 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">10</span>)</span>
<span id="cb95-21"><a href="#cb95-21" aria-hidden="true" tabindex="-1"></a>t3_2 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">10</span>)</span>
<span id="cb95-22"><a href="#cb95-22" aria-hidden="true" tabindex="-1"></a>points3 <span class="op">=</span> np.array([s <span class="op">*</span> v3_1 <span class="op">+</span> t <span class="op">*</span> v3_2 <span class="cf">for</span> s <span class="kw">in</span> t3_1 <span class="cf">for</span> t <span class="kw">in</span> t3_2])</span>
<span id="cb95-23"><a href="#cb95-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-24"><a href="#cb95-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a 3D plot</span></span>
<span id="cb95-25"><a href="#cb95-25" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb95-26"><a href="#cb95-26" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">111</span>, projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb95-27"><a href="#cb95-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-28"><a href="#cb95-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the subspaces</span></span>
<span id="cb95-29"><a href="#cb95-29" aria-hidden="true" tabindex="-1"></a>ax.scatter(points1[:, <span class="dv">0</span>], points1[:, <span class="dv">1</span>], points1[:, <span class="dv">2</span>], color<span class="op">=</span><span class="st">'r'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Subspace for Token 1'</span>)</span>
<span id="cb95-30"><a href="#cb95-30" aria-hidden="true" tabindex="-1"></a>ax.scatter(points2[:, <span class="dv">0</span>], points2[:, <span class="dv">1</span>], points2[:, <span class="dv">2</span>], color<span class="op">=</span><span class="st">'g'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Subspace for Token 2'</span>)</span>
<span id="cb95-31"><a href="#cb95-31" aria-hidden="true" tabindex="-1"></a>ax.scatter(points3[:, <span class="dv">0</span>], points3[:, <span class="dv">1</span>], points3[:, <span class="dv">2</span>], color<span class="op">=</span><span class="st">'b'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Subspace for Token 3'</span>)</span>
<span id="cb95-32"><a href="#cb95-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-33"><a href="#cb95-33" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels([])</span>
<span id="cb95-34"><a href="#cb95-34" aria-hidden="true" tabindex="-1"></a>ax.set_yticklabels([])</span>
<span id="cb95-35"><a href="#cb95-35" aria-hidden="true" tabindex="-1"></a>ax.set_zticklabels([])</span>
<span id="cb95-36"><a href="#cb95-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-37"><a href="#cb95-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Set labels and title</span></span>
<span id="cb95-38"><a href="#cb95-38" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Imagined Token Subspaces in $\mathbb</span><span class="sc">{R}</span><span class="st">^3$'</span>)</span>
<span id="cb95-39"><a href="#cb95-39" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb95-40"><a href="#cb95-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-41"><a href="#cb95-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb95-42"><a href="#cb95-42" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-70-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>I don’t know how to determine the exact subspaces for each token mathematically. But I do know how to get a workable approximation of them <strong>if we’re willing to pretend that they are linear</strong>. They are almost certainly not linear, even at the end of the model, because of the non-linear LayerNorm operation. But they are likely <em>closer</em> to linear near the end of the model because the LayerNorm is the only non-linearity. Earlier in the model, each feed-forward network introduces an additional non-linearity via its ReLU operation.</p>
<blockquote class="blockquote">
<p><a href="https://www.lesswrong.com/posts/jfG6vdJZCwTQmG7kb/re-examining-layernorm">This post on LessWrong</a> illustrates of the non-linearity of LayerNorm clearly.</p>
</blockquote>
<p>Pretending the subspaces are linear actually works quite well for the part of the model after the transformer blocks. And that is the only part of the model we need to consider for this analysis (as I’ll explain <a href="#use-only-final-subspaces">soon</a>).</p>
</section>
<section id="linear-approximations-for-subspaces" class="level4">
<h4 class="anchored" data-anchor-id="linear-approximations-for-subspaces">Linear Approximations for Subspaces</h4>
<p>The idea is quite simple: for a given token, we can learn a whole lot of different embeddings, treating each one as a data point. Then we can determine the best fitting line, plane, or other low-dimensional linear subspace that fits the data.</p>
<p>Again, if we imagine our embedding space reduced to just 3 dimensions, it might look something like the following diagram. The blue dots each represent a learned embedding and the red arrow is the line that minimizes projected distance from each point.</p>
<div id="cell-161" class="cell">
<details class="code-fold">
<summary>Drawing code for diagram line fit to points in 3D space</summary>
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>num_points <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>noise_factor <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(<span class="dv">1442</span>)</span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a>noise <span class="op">=</span> noise_factor <span class="op">*</span> rng.standard_normal(size<span class="op">=</span>(num_points, <span class="dv">3</span>))</span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a>points <span class="op">=</span> noise <span class="op">+</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform SVD</span></span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span class="op">=</span> np.linalg.svd(points)</span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Top right singular vector (the last row of Vt)</span></span>
<span id="cb96-14"><a href="#cb96-14" aria-hidden="true" tabindex="-1"></a>top_singular_vector <span class="op">=</span> <span class="op">-</span>Vt[<span class="dv">0</span>] <span class="op">*</span> np.linalg.norm(points, axis<span class="op">=</span><span class="dv">1</span>).<span class="bu">max</span>()</span>
<span id="cb96-15"><a href="#cb96-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-16"><a href="#cb96-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-17"><a href="#cb96-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a 3D plot</span></span>
<span id="cb96-18"><a href="#cb96-18" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb96-19"><a href="#cb96-19" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">111</span>, projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb96-20"><a href="#cb96-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-21"><a href="#cb96-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the points</span></span>
<span id="cb96-22"><a href="#cb96-22" aria-hidden="true" tabindex="-1"></a>ax.scatter(points[:, <span class="dv">0</span>], points[:, <span class="dv">1</span>], points[:, <span class="dv">2</span>], color<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb96-23"><a href="#cb96-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-24"><a href="#cb96-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the arrow corresponding to the top right singular vector</span></span>
<span id="cb96-25"><a href="#cb96-25" aria-hidden="true" tabindex="-1"></a>arrow_start <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb96-26"><a href="#cb96-26" aria-hidden="true" tabindex="-1"></a>arrow_end <span class="op">=</span> top_singular_vector</span>
<span id="cb96-27"><a href="#cb96-27" aria-hidden="true" tabindex="-1"></a>ax.quiver(arrow_start[<span class="dv">0</span>], arrow_start[<span class="dv">1</span>], arrow_start[<span class="dv">2</span>],</span>
<span id="cb96-28"><a href="#cb96-28" aria-hidden="true" tabindex="-1"></a>          top_singular_vector[<span class="dv">0</span>], top_singular_vector[<span class="dv">1</span>], top_singular_vector[<span class="dv">2</span>],</span>
<span id="cb96-29"><a href="#cb96-29" aria-hidden="true" tabindex="-1"></a>          color<span class="op">=</span><span class="st">'r'</span>, arrow_length_ratio<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb96-30"><a href="#cb96-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-31"><a href="#cb96-31" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb96-32"><a href="#cb96-32" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>, <span class="fl">2.7</span>)</span>
<span id="cb96-33"><a href="#cb96-33" aria-hidden="true" tabindex="-1"></a>ax.set_zlim(<span class="dv">0</span>, <span class="dv">4</span>)</span>
<span id="cb96-34"><a href="#cb96-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-35"><a href="#cb96-35" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels([])</span>
<span id="cb96-36"><a href="#cb96-36" aria-hidden="true" tabindex="-1"></a>ax.set_yticklabels([])</span>
<span id="cb96-37"><a href="#cb96-37" aria-hidden="true" tabindex="-1"></a>ax.set_zticklabels([])</span>
<span id="cb96-38"><a href="#cb96-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-39"><a href="#cb96-39" aria-hidden="true" tabindex="-1"></a>ax.view_init(<span class="dv">25</span>, <span class="dv">15</span>)</span>
<span id="cb96-40"><a href="#cb96-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-41"><a href="#cb96-41" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'A Line Fit to Points in $\mathbb</span><span class="sc">{R}</span><span class="st">^3$'</span>)</span>
<span id="cb96-42"><a href="#cb96-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-43"><a href="#cb96-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb96-44"><a href="#cb96-44" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-71-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can use Singular Value Decomposition (SVD) to find the best fitting linear subspace for the learned embeddings.</p>
<blockquote class="blockquote">
<p>To learn more about singular value decomposition in this context, I recommend reading Jeremy Kun’s excellent two-part post. <a href="https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/">Part 1</a> <a href="https://jeremykun.com/2016/05/16/singular-value-decomposition-part-2-theorem-proof-algorithm/">Part 2</a>.</p>
</blockquote>
<p><a href="#v-performing-svd-to-get-a-linear-approximation-of-a-token-subspace">Appendix V</a> walks through the code that uses SVD to find a linear approximation for the subspace corresponding to one token. I did this for all tokens, using the embeddings I learned for the final stages of the transformer. In every case, I was able to find a single vector (1-D subspace) that approximates the token subspace quite well.</p>
<blockquote class="blockquote">
<p>For completeness, I also tried this at earlier stages of the transformer and found, as expected, that the linear approximations, even at higher dimensions, didn’t fit the data as well. The relevant experiments are in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb">the approximation details notebook</a></p>
</blockquote>
</section>
<section id="mixing-subspace-approximations" class="level4">
<h4 class="anchored" data-anchor-id="mixing-subspace-approximations">Mixing Subspace Approximations</h4>
<p>By learning a large number of embeddings for each token and then using SVD on them, we can find one vector for each token that approximates its subspace. Given one of these vectors, any embedding that falls on its span will produce an output distribution that concentrates all the probability mass on the corresponding token. But many of the real transformer outputs we’ve seen distribute the probability mass across several tokens. How do we get from subspaces for individual tokens to embeddings that produce these more diverse distributions?</p>
<p>We can create embeddings that produce probability distributions where several tokens have substantial probability via linear combinations of the subspace approximation vectors for those tokens. This is the distribution we get when we create an embedding by simply adding the approximation vectors for the subspaces for <code>a</code> and <code>b</code>:</p>
<div id="cell-164" class="cell">
<details class="code-fold">
<summary>Code to generate plot of probabilities from <code>a</code> + <code>b</code></summary>
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>accessors <span class="op">=</span> TransformerAccessors(m, device)</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>learned_embeddings_dir <span class="op">=</span> environment.data_root <span class="op">/</span> <span class="st">'learned_embeddings'</span></span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>multi_emb_a <span class="op">=</span> torch.load(learned_embeddings_dir <span class="op">/</span> <span class="st">'no_blocks'</span> <span class="op">/</span> <span class="st">'lower_a.pt'</span>, map_location<span class="op">=</span>device)</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>_, Sa, Va <span class="op">=</span> torch.linalg.svd(multi_emb_a[:, <span class="op">-</span><span class="dv">1</span>, :])</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>v0a <span class="op">=</span> adjust_singular_vector_sign(Va[<span class="dv">0</span>], multi_emb_a[:, <span class="op">-</span><span class="dv">1</span>, :])</span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a>multi_emb_b <span class="op">=</span> torch.load(learned_embeddings_dir <span class="op">/</span> <span class="st">'no_blocks'</span> <span class="op">/</span> <span class="st">'lower_b.pt'</span>, map_location<span class="op">=</span>device)</span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a>_, Sb, Vb <span class="op">=</span> torch.linalg.svd(multi_emb_b[:, <span class="op">-</span><span class="dv">1</span>, :])</span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a>v0b <span class="op">=</span> adjust_singular_vector_sign(Vb[<span class="dv">0</span>], multi_emb_b[:, <span class="op">-</span><span class="dv">1</span>, :])</span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-12"><a href="#cb97-12" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> LogitsWrapper(accessors.logits_from_embedding(unsqueeze_emb(v0a <span class="op">+</span> v0b)), tokenizer)</span>
<span id="cb97-13"><a href="#cb97-13" aria-hidden="true" tabindex="-1"></a>logits.plot_probs(title<span class="op">=</span><span class="st">'Probability Distribution from "a" + "b"'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-72-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The sum of subspace approximations vectors for two tokens is an embedding somewhere in between the two subspaces, which results in a final distribution that is the combination of the two tokens.</p>
<p>Sadly, adding the approximation vectors for <code>a</code> and <code>b</code>, without weighting either one, results in not quite a 50-50 distribution across the two tokens (as shown above). I think there are three reasons for this:</p>
<ol type="1">
<li>The approximation vectors are just approximations and not perfect representations of their subspaces.</li>
<li>The subspace approximation vectors are not perfectly orthogonal. To the extent that <code>a</code>’s vector has a small component that points in the direction of <code>b</code>, the sum results in an overweighting of <code>b</code>.</li>
<li>The final linear layer of the model produces logits of different magnitudes for different tokens. For example, given the approximation for <code>a</code>, the logit for <code>a</code> is ~18.2. The logit for <code>b</code> from its approximation is ~19.5.</li>
</ol>
<p>Together, these errors accumulate and the softmax function at the very end exaggerates even small differences. For more analysis on the reasoning behind the differences and how they might be compensated for, see <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/60_combining_token_subspaces.ipynb">the combining token subspaces notebook</a>.</p>
<p>These imperfections aside, I think we can conclude that <strong>it’s possible to derive an embedding that produces a distribution for multiple tokens via a linear combination of the approximation vectors for those tokens’ subspaces</strong>.</p>
</section>
</section>
<section id="putting-it-all-together" class="level3">
<h3 class="anchored" data-anchor-id="putting-it-all-together">Putting it All Together</h3>
<p>To summarize where we are, the preceding sections have shown:</p>
<ul>
<li>The transformer blocks perform a series of transformations in embedding space.</li>
<li>Those transformations can be thought of as moving from one point in embedding space to another by adding the feed-forward network output vector to the input embedding.</li>
<li>Embedding space contains subspaces corresponding to predicting particular tokens and embeddings between subspaces for multiple tokens result in predictions including all those tokens.</li>
</ul>
<p>This section adds the final piece, which is the correspondence between what the transformer is doing and what the approximation is doing:</p>
<ul>
<li>Within a block, adding the feed-forward network output vector to the input produces an output embedding that better aligns with the embedding subspaces of specific tokens. And <strong>those tokens are the same ones predicted in the approximation</strong>: they’re the tokens that follow the strings in the training corpus that yield similar feed-forward network outputs to the current prompt.</li>
</ul>
<p>Let’s look at an example that shows this. The following is the output distribution predicted by the approximation for the prompt, <code>med me Aut</code> (query index 33), using only the feed-forward network outputs from the final block:</p>
<div id="cell-167" class="cell">
<details class="code-fold">
<summary>Helper function to perform the approximation for a single query in a single block.</summary>
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> perform_approximation(block_idx: <span class="bu">int</span>, q_idx: <span class="bu">int</span>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Find similar strings</span></span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>    similar_indices <span class="op">=</span> filter_on_prefiltered_results(</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>        load_prefiltered<span class="op">=</span><span class="kw">lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),</span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a>        q_idx_start<span class="op">=</span>q_idx,</span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a>        q_idx_end<span class="op">=</span>q_idx<span class="op">+</span><span class="dv">1</span>,</span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a>        filter_fn<span class="op">=</span><span class="kw">lambda</span> values: values <span class="op">&gt;</span> similarity_thresholds[block_idx]</span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a>    similar_strings <span class="op">=</span> [</span>
<span id="cb98-10"><a href="#cb98-10" aria-hidden="true" tabindex="-1"></a>        [strings10[i] <span class="cf">for</span> i <span class="kw">in</span> indices]</span>
<span id="cb98-11"><a href="#cb98-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> indices <span class="kw">in</span> similar_indices</span>
<span id="cb98-12"><a href="#cb98-12" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb98-13"><a href="#cb98-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-14"><a href="#cb98-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute Probability Distribution from Similar Strings</span></span>
<span id="cb98-15"><a href="#cb98-15" aria-hidden="true" tabindex="-1"></a>    total_freq_distribution <span class="op">=</span> torch.stack(</span>
<span id="cb98-16"><a href="#cb98-16" aria-hidden="true" tabindex="-1"></a>        [next_token_map10[string] <span class="cf">for</span> string <span class="kw">in</span> similar_strings[<span class="dv">0</span>]]</span>
<span id="cb98-17"><a href="#cb98-17" aria-hidden="true" tabindex="-1"></a>    ).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb98-18"><a href="#cb98-18" aria-hidden="true" tabindex="-1"></a>    prob_distribution <span class="op">=</span> total_freq_distribution <span class="op">/</span> total_freq_distribution.<span class="bu">sum</span>()</span>
<span id="cb98-19"><a href="#cb98-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-20"><a href="#cb98-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> prob_distribution, <span class="bu">len</span>(similar_strings[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-168" class="cell">
<details class="code-fold">
<summary>Code to plot output distribution for q_idx=33 in block 5</summary>
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>block_idx <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>q_idx <span class="op">=</span> <span class="dv">33</span></span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>prob_distribution, _ <span class="op">=</span> perform_approximation(block_idx, q_idx)</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>plot_prob_distribution_for_tokens(prob_distribution, title<span class="op">=</span><span class="ss">f'Probability Distribution from Approximation (block_idx=</span><span class="sc">{</span>block_idx<span class="sc">}</span><span class="ss">, q_idx=</span><span class="sc">{</span>q_idx<span class="sc">}</span><span class="ss">, prompt=</span><span class="sc">{</span><span class="bu">repr</span>(prompts[q_idx])<span class="sc">}</span><span class="ss">)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-74-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Based on the strings in the training corpus with similar feed-forward network outputs at the final block, the approximation predicts <code>o</code> is the most likely next token and <code>h</code> is next.</p>
<p>Next, we need to look at the feed-forward network output for the prompt in this block and determine which token subspaces it’s most oriented towards. I’m going to show a little code here, because I think it’s the best way to explain what’s going on. Readers who aren’t interested in the implementation can focus only on the output.</p>
<p>First we need to actually grab the feed-forward outputs (we haven’t needed them so far because we’ve been working with precomputed/prefiltered similarity data). We’ll use some <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer-helpers.ipynb">helper functions</a> that provide easy access to the transformer’s intermediate representations:</p>
<div id="cell-171" class="cell">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize the strings</span></span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> encoding_helpers.tokenize_strings(prompts)</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Embed the tokens</span></span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> accessors.embed_tokens(tokens)</span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate TransformerAccessors</span></span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a>accessors <span class="op">=</span> TransformerAccessors(m, device)</span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-10"><a href="#cb100-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Run them through the model with hooks attached that let us look at</span></span>
<span id="cb100-11"><a href="#cb100-11" aria-hidden="true" tabindex="-1"></a><span class="co"># intermediate values</span></span>
<span id="cb100-12"><a href="#cb100-12" aria-hidden="true" tabindex="-1"></a>_, io_accessors <span class="op">=</span> accessors.run_model(embeddings)</span>
<span id="cb100-13"><a href="#cb100-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-14"><a href="#cb100-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Grab the outputs of the ffwd networks at each layer</span></span>
<span id="cb100-15"><a href="#cb100-15" aria-hidden="true" tabindex="-1"></a>ffwd_outs <span class="op">=</span> torch.stack([</span>
<span id="cb100-16"><a href="#cb100-16" aria-hidden="true" tabindex="-1"></a>    io_accessors[block_idx].output(<span class="st">'ffwd'</span>)[:, <span class="op">-</span><span class="dv">1</span>, :].clone()</span>
<span id="cb100-17"><a href="#cb100-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> block_idx <span class="kw">in</span> <span class="bu">range</span>(n_layer)</span>
<span id="cb100-18"><a href="#cb100-18" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb100-19"><a href="#cb100-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-20"><a href="#cb100-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Free up some memory</span></span>
<span id="cb100-21"><a href="#cb100-21" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> io_accessors</span>
<span id="cb100-22"><a href="#cb100-22" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> gc.collect()</span>
<span id="cb100-23"><a href="#cb100-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-24"><a href="#cb100-24" aria-hidden="true" tabindex="-1"></a>ffwd_outs.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([6, 20000, 384])</code></pre>
</div>
</div>
<p>To determine which token subspaces the feed-forward network output aligns with, we’ll project it onto the subspace approximation for each token, then determine which projections are most similar to the original vector. To do this, we’ll need to get the projection matrix for the rank 1 approximation to each token subspace:</p>
<blockquote class="blockquote">
<p>The code below uses the <code>projection_matrix_for_rank_k_approximation()</code> helper function, defined in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/common/svd-helpers.ipynb">the SVD helpers notebook</a>.</p>
</blockquote>
<blockquote class="blockquote">
<p>In the case of a rank 1 approximation, the projection isn’t really necessary. We could just take the cosine similarity with the approximation vector, but I wanted to keep this code general because I tried out higher-dimensional approximations in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb">other places</a>.</p>
</blockquote>
<div id="cell-173" class="cell">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>filename_for_token <span class="op">=</span> FilenameForToken(tokenizer)</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>subspace_dims <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a>projection_matrices <span class="op">=</span> torch.stack([</span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a>    projection_matrix_for_rank_k_approximation(</span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a>        original_matrix<span class="op">=</span>torch.load(</span>
<span id="cb102-6"><a href="#cb102-6" aria-hidden="true" tabindex="-1"></a>            learned_embeddings_dir <span class="op">/</span>  <span class="st">'no_blocks'</span> <span class="op">/</span> <span class="ss">f"</span><span class="sc">{</span>filename_for_token(token)<span class="sc">}</span><span class="ss">.pt"</span>,</span>
<span id="cb102-7"><a href="#cb102-7" aria-hidden="true" tabindex="-1"></a>            map_location<span class="op">=</span>device,</span>
<span id="cb102-8"><a href="#cb102-8" aria-hidden="true" tabindex="-1"></a>        )[:, <span class="dv">0</span>, :],</span>
<span id="cb102-9"><a href="#cb102-9" aria-hidden="true" tabindex="-1"></a>        k<span class="op">=</span>subspace_dims,</span>
<span id="cb102-10"><a href="#cb102-10" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb102-11"><a href="#cb102-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> token <span class="kw">in</span> tokenizer.chars</span>
<span id="cb102-12"><a href="#cb102-12" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we’ll perform the projections and find the top 5 most similar ones to the original feed-forward output vector:</p>
<div id="cell-175" class="cell">
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>projections <span class="op">=</span> projection_matrices <span class="op">@</span> ffwd_outs[block_idx, q_idx, :]</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>values, indices <span class="op">=</span> torch.topk(</span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a>    F.cosine_similarity(projections, ffwd_outs[block_idx][q_idx], dim<span class="op">=-</span><span class="dv">1</span>),</span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a>    dim<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb103-6"><a href="#cb103-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb103-7"><a href="#cb103-7" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> [tokenizer.chars[i.item()] <span class="cf">for</span> i <span class="kw">in</span> indices]</span>
<span id="cb103-8"><a href="#cb103-8" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(<span class="bu">zip</span>(tokens, values.tolist()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[('o', 0.5074884295463562),
 ('h', 0.40787822008132935),
 ('i', 0.26926180720329285),
 ('u', 0.22823508083820343),
 ('y', 0.20325089991092682)]</code></pre>
</div>
</div>
<p>It turns out that <code>o</code> and <code>h</code> are the most similar, indicating that the feed-forward network output is most oriented towards the subspaces for these tokens. And these are the same tokens that the approximation predicted from the strings with similar feed-forward network outputs (see the distribution above).</p>
<p>Another example, this time looking at query index 36 (<code>if and thy</code>), but staying in the final block:</p>
<div id="cell-178" class="cell">
<details class="code-fold">
<summary>Code to plot output distribution for q_idx=36 in block 5</summary>
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>block_idx <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>q_idx <span class="op">=</span> <span class="dv">36</span></span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>prob_distribution, _ <span class="op">=</span> perform_approximation(block_idx, q_idx)</span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a>plot_prob_distribution_for_tokens(prob_distribution, title<span class="op">=</span><span class="ss">f'Probability Distribution from Approximation (block_idx=</span><span class="sc">{</span>block_idx<span class="sc">}</span><span class="ss">, q_idx=</span><span class="sc">{</span>q_idx<span class="sc">}</span><span class="ss">, prompt=</span><span class="sc">{</span><span class="bu">repr</span>(prompts[q_idx])<span class="sc">}</span><span class="ss">)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-78-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-179" class="cell">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>projections <span class="op">=</span> projection_matrices <span class="op">@</span> ffwd_outs[block_idx, q_idx, :]</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>values, indices <span class="op">=</span> torch.topk(</span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a>    F.cosine_similarity(projections, ffwd_outs[block_idx][q_idx], dim<span class="op">=-</span><span class="dv">1</span>),</span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a>    dim<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> [tokenizer.chars[i.item()] <span class="cf">for</span> i <span class="kw">in</span> indices]</span>
<span id="cb106-8"><a href="#cb106-8" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(<span class="bu">zip</span>(tokens, values.tolist()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[(' ', 0.5869003534317017),
 ('s', 0.47689366340637207),
 ('\n', 0.38412901759147644),
 ('$', 0.23048195242881775),
 ('a', 0.21783535182476044)]</code></pre>
</div>
</div>
<p>Here <code></code> (space), <code>s</code>, and <code>\n</code> (newline) were the tokens predicted from what follows the strings with similar feed-forward outputs, and indeed these are the token subspaces most aligned with the prompt’s feed-forward output.</p>
<section id="aggregate-performance" class="level4">
<h4 class="anchored" data-anchor-id="aggregate-performance">Aggregate Performance</h4>
<p>In the previous section, I purposely picked examples that exhibit strong correlation between the approximation’s predictions and the most aligned subspaces, to illustrate the point most clearly. Of course, there are other examples for which the correlation is less strong. Rather than looking at specific cases, let’s try to get a sense of how well the correlation holds up across all 20,000 prompts.</p>
<p>This immediately leads to a question: what is the right measure of aggregate performance? Unfortunately, even if the hypothesis - that the prompt’s feed-forward output aligns with the subspaces for tokens predicted from the strings with similar feed forward outputs - is true, a few practical issues make it difficult to demonstrate objectively:</p>
<ul>
<li>We don’t have exact definitions of the token subspaces, just imperfect, linear approximations.</li>
<li>Magnitudes don’t line up: the tokens with the most probability mass in the approximation’s predictions don’t always correspond to the subspaces with the greatest cosine similarity (because of the imperfect approximations, because the adjustment required may be bigger or smaller for some tokens vs others based on the input embedding’s current alignment, because, as explained in the <a href="#mixing-subspace-approximations">Mixing Subspace Approximations</a> section, the model is more “sensitive” to some tokens than others).</li>
</ul>
<p>Given these impediments, we can’t just do something simple like normalizing the cosine similarities and computing Hellinger distance with the predicted probability distribution.</p>
<p>Instead, we need to devise a criterion on which to judge whether the data from a particular prompt supports the hypothesis or not. Then we can evaluate aggregate performance by how many of the 20,000 prompts satisfy the criterion. I experimented with several different approaches and in the end came up with this candidate criterion:</p>
<p>High-level description: <em>Do the subspaces for the tokens containing 90% of the probability mass in the approximation’s predictions appear in the top half of all token subspaces when ranked by cosine similarity with the prompt’s feed-forward output vector?</em></p>
<p>Exact definition:</p>
<ul>
<li>Define <code>top_n</code> as the number of tokens required to cover at least 90% of the probability mass in the approximation’s predictions for this prompt.</li>
<li>Define <code>n_subspaces</code> as <code>tokenizer.vocab_size // 2</code> (32, based on our 65-token vocabulary).</li>
<li>Determine: Are the subspaces for the first <code>top_n</code> tokens predicted by the approximation in the first <code>n_subspaces</code> subspaces ranked by cosine similarity with the prompt’s feed-forward output vector?</li>
</ul>
<p>Admittedly, this is an arbitrary definition and reasonable people could debate any of the specifics. But I do think it gives as an indication of whether the data from a particular example prompt supports the hypothesis, while allowing for some of the measurement challenges noted above.</p>
<p>I evaluated this criteria at three places: the outputs of blocks 6, 5, and 4, using projection matrices derived from learned embeddings at each of these places.</p>
<blockquote class="blockquote">
<p>I didn’t evaluate at earlier blocks because the GPU time required to learn embeddings at those blocks became prohibitive. The further back in the model, the bigger the computation graph that the learning algorithm needs to optimize over.</p>
</blockquote>
<p>The table below shows the results:</p>
<blockquote class="blockquote">
<p>The code that produced these results appears at the end of <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb">the approximation details notebook</a></p>
</blockquote>
<table class="table">
<thead>
<tr class="header">
<th>Block</th>
<th># of Prompts Satisfying Criterion</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>6</td>
<td>16357 (81.78%)</td>
</tr>
<tr class="even">
<td>5</td>
<td>10142 (50.71%)</td>
</tr>
<tr class="odd">
<td>4</td>
<td>7760 (38.80%)</td>
</tr>
</tbody>
</table>
<p>These numbers aren’t exactly a ringing endorsement. As expected, they get worse the further back we go, probably due to the increased non-linearity.</p>
<p><a id="use-only-final-subspaces"></a> What if we always used the subspace approximations from the very end of the transformer (which are likely to be the most linear), even when comparing against feed-forward network outputs from earlier blocks? The results get better:</p>
<table class="table">
<thead>
<tr class="header">
<th>Block</th>
<th># of Prompts Satisfying Criterion</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>6</td>
<td>16357 (81.78%)</td>
</tr>
<tr class="even">
<td>5</td>
<td>13652 (68.26%)</td>
</tr>
<tr class="odd">
<td>4</td>
<td>11630 (58.15%)</td>
</tr>
<tr class="even">
<td>3</td>
<td>11469 (57.34%)</td>
</tr>
<tr class="odd">
<td>2</td>
<td>10404 (52.02%)</td>
</tr>
<tr class="even">
<td>1</td>
<td>9942 (49.71%)</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p>Like many good findings, this one resulted from a bug. I accidentally ran the analysis using the projection matrices for the final part of the transformer with the feed-forward network outputs from earlier blocks and was surprised when the numbers turned out to be so good.</p>
</blockquote>
<p>It’s valid to use the subspace approximations (and corresponding projection matrices) from the end of the transformer at earlier stages. All blocks operate in the same embedding space and each one seems to make a small refinement on the output of its predecessors, rather than wild changes in direction. So if any block’s feed-forward network output adjusts an embedding towards the subspaces for a set of tokens as defined at the end of the transformer, it is likely also adjusting it towards whatever the subspaces would be for those same tokens at the block where it operates.</p>
<blockquote class="blockquote">
<p>The <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens">logit lens post</a>, in the section “why? / is this surprising?” provides an explanation that supports this idea. In summary, the residual connections encourage the transformer to learn weights that operate within the same basis across blocks and the use of weight decay in training results in a computation that’s spread out over as many layers as possible, with each layer making only a small, incremental change.</p>
</blockquote>
<p>To put these numbers in perspective, I investigated how likely it would be for the criterion I’ve defined here to be satisfied by chance. In other words, if we assume the hypothesis is false and that the cosine similarities between the feed-forward network output and the token subspace approximation vectors are random, rather than expressing meaningful relationships, how likely would it be for the criterion to still be satisfied?</p>
<p>I ran a simulation of this, taking care to ensure that the distribution of randomly generated cosine similarities matches the real data, among other details. The implementation is at the end of <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb">the approximation details notebook</a>. The final results are:</p>
<table class="table">
<thead>
<tr class="header">
<th>Block</th>
<th>Likely % of Prompts Satisfying Criteria By Chance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>6</td>
<td>20.76% ± 0.25%</td>
</tr>
<tr class="even">
<td>5</td>
<td>20.55% ± 0.26%</td>
</tr>
<tr class="odd">
<td>4</td>
<td>18.37% ± 0.24%</td>
</tr>
<tr class="even">
<td>3</td>
<td>18.20% ± 0.24%</td>
</tr>
<tr class="odd">
<td>2</td>
<td>17.04% ± 0.23%</td>
</tr>
<tr class="even">
<td>1</td>
<td>16.31% ± 0.23%</td>
</tr>
</tbody>
</table>
<p>So the best performance numbers we have are clearly much better than chance. But in fairness, they’re still not a slam dunk.</p>
<p>Even when we use approximations for the most linear subspaces we have, I think there is still a lot of noise in the measurement, for all the reasons outlined earlier in this section. Personally, I take the numbers to be overall supportive of the hypothesis, at least directionally, though I wish the evidence was more conclusive.</p>
</section>
</section>
<section id="final-summary-of-correspondence-between-transformer-and-approximation" class="level3">
<h3 class="anchored" data-anchor-id="final-summary-of-correspondence-between-transformer-and-approximation">Final Summary of Correspondence Between Transformer and Approximation</h3>
<p>The analyses in this post point towards two ideas. First, that the approximation and the transformer produce similar outputs. Second, that there is a correspondence between the approximation procedure and what the transformer is doing. I think the evidence for the first idea is quite strong. The evidence for the second is less clear cut, but still suggests it’s probably at least partially right.</p>
<p>To close, I want to provide a high-level summary of what I think that correspondence is, even if I can’t yet demonstrate it more definitively:</p>
<table class="table">
<thead>
<tr class="header">
<th>Concept</th>
<th>Transformer</th>
<th>Approximation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Prompts map to classes of strings in the training corpus.</td>
<td>The transformer learns an embedding scheme, along with weights in its self-attention and feed-forward networks, that cause strings in the training corpus with similar characteristics to produce similar output values. Prompts that share those characteristics also produce similar output values.</td>
<td>The approximation performs the same mapping as the transformer by examining the feed-forward network outputs from all substrings in the training corpus and identifying the ones similar to the outputs from a given prompt.</td>
</tr>
<tr class="even">
<td>Predictions for the tokens likely to follow a prompt derive from the frequency distribution of tokens that follow strings in the training corpus that produce feed-forward network output values similar to those of the prompt.</td>
<td>A feed-forward network output is a compressed, latent representation, in the embedding space, of the frequency distribution of the tokens that follow strings in the training corpus that produce similar outputs. The weights in the final linear layer map the latent representation into logit space such that it become the correct probability distribution after applying the softmax operation.</td>
<td>The approximation reconstructs the same frequency distribution manually, by looking up the strings identified as having similar outputs in the training corpus and counting the tokens that follow them. Normalizing the frequency distribution turns it into a probability distribution.</td>
</tr>
<tr class="odd">
<td>Final output is a weighted sum of predictions from each block.</td>
<td>As shown <a href="#transformation-via-vector-addition">earlier</a>, the transformer output is roughly the vector sum of all feed-forward network outputs and the input embedding. The learned weights in the layers within a block determine the magnitude and direction of the output and thus how much it influences the overall direction of the final sum.</td>
<td>The approximation performs a weighted sum of the distributions determined for each block. The weights control the degree of influence of any given block and are manually selected to produce results as close to the transformer’s as possible.</td>
</tr>
</tbody>
</table>
</section>
<section id="what-about-attention" class="level3">
<h3 class="anchored" data-anchor-id="what-about-attention">What About Attention?</h3>
<p>I began this post by observing that most explanations of how transformers work focus on attention but don’t say how attention results turn into the final predictions. I may be guilty of the opposite: I’ve written at length about how the transformers produce their output probabilities and said very little about attention.</p>
<p>To wrap up the analysis, I’d like to rectify this with a few words about attention. In the mechanism I’ve laid out, whether executed in the form of the approximation or the transformer, a key operation is mapping the prompt to a class of strings from the training corpus at each block. Predictions for the next token follow directly from the distribution of tokens that follow those strings in the training corpus. <strong>Making good predictions depends on mapping the prompt to the right class of training corpus strings. And that is the job of self-attention.</strong></p>
<p>The self-attention layers learn to identify patterns across the tokens that make up a prompt. Those patterns might be simple, such as a common sequence appearing at the beginning or end of the prompt (for example, as we saw <a href="#demo-my-proposal-in-action">earlier</a>, strings that end in ‘y l’). They can also be more general: instead of matching specific tokens, they might match <em>kinds</em> of tokens, such as vowels or capitals, in specific places. The learned weights in the attention heads determine which patterns they respond to, and thus which strings in the training corpus produce similar values. The output of the self-attention heads, when passed through the feed-forward network, yield representations in embedding space that encode information about the distribution of tokens in the training corpus that follow those strings.</p>
<p>Because the transformer has multiple blocks and each block has multiple attention heads (6 blocks and 6 heads per block in the one we looked at), it’s possible to evaluate each prompt against a large number of different potential patterns. The richness and diversity of the patterns that the attention heads can identify gives the transformer its predictive power.</p>
</section>
</section>
<section id="closing-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="closing-thoughts">Closing Thoughts</h2>
<p>I started this project because I wanted to understand the transformer architecture. It’s given me a satisfying explanation of what at least one transformer is doing, but has been even more fruitful as an exercise in learning how to learn. This was my first foray into an open-ended ML research project on my own. It taught me how to interrogate the internals of models, how to set up experiments to answer questions, and, perhaps most importantly, how to keep moving the project forward when I felt stuck.</p>
<p>Language models have always seemed magical to me, from the first time I used ChatGPT. I wondered if finding a reductive explanation for what happens internally would rob them of their magic. In fact, I think the opposite has happened. I’ve come to appreciate the beauty in an elegantly simple mechanism that produces such rich complexity in its outputs.</p>
<p>I don’t know whether the results I found here have any generality beyond the small transformer I trained or if any of it will be of use to anyone else. Regardless, it’s been a joy to do this work and I’m grateful to have had the things I needed along the way: time, resources, and endless support from my family and mentors.</p>
</section>
<section id="appendices" class="level2">
<h2 class="anchored" data-anchor-id="appendices">Appendices</h2>
<section id="i-model-details" class="level3">
<h3 class="anchored" data-anchor-id="i-model-details">I: Model Details</h3>
<p>Some notable specs:</p>
<ul>
<li>Vocabulary size: 65 (the unique characters in the TinyShakespeare dataset)</li>
<li>Embedding size (<code>n_embed</code>): 384</li>
<li>Number of transformer blocks (<code>n_layer</code>): 6</li>
<li>Number of attention heads (<code>n_head</code>): 6</li>
<li>Context window size (<code>block_size</code>): 256</li>
</ul>
<p>The feed-forward networks comprise over 65% of the total trainable parameters:</p>
<div id="cell-187" class="cell">
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>all_trainable_params <span class="op">=</span> [p <span class="cf">for</span> p <span class="kw">in</span> m.parameters() <span class="cf">if</span> p.requires_grad]</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>n_all_trainable_params <span class="op">=</span> <span class="bu">sum</span>([np.prod(p.size()) <span class="cf">for</span> p <span class="kw">in</span> all_trainable_params])</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a>ffwd_trainable_params <span class="op">=</span> [</span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a>    p</span>
<span id="cb108-6"><a href="#cb108-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> block_idx <span class="kw">in</span> <span class="bu">range</span>(n_layer)</span>
<span id="cb108-7"><a href="#cb108-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> m.blocks[block_idx].ffwd.parameters()</span>
<span id="cb108-8"><a href="#cb108-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> p.requires_grad</span>
<span id="cb108-9"><a href="#cb108-9" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb108-10"><a href="#cb108-10" aria-hidden="true" tabindex="-1"></a>n_ffwd_trainable_params <span class="op">=</span> <span class="bu">sum</span>([np.prod(p.size()) <span class="cf">for</span> p <span class="kw">in</span> ffwd_trainable_params])</span>
<span id="cb108-11"><a href="#cb108-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-12"><a href="#cb108-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb108-13"><a href="#cb108-13" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"</span><span class="sc">{</span>n_ffwd_trainable_params<span class="sc">:,}</span><span class="ss"> ffwd params out of </span><span class="sc">{</span>n_all_trainable_params<span class="sc">:,}</span><span class="ss"> total params (</span><span class="sc">{</span>n_ffwd_trainable_params <span class="op">/</span> n_all_trainable_params<span class="sc">:.2%}</span><span class="ss">)"</span></span>
<span id="cb108-14"><a href="#cb108-14" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>7,089,408 ffwd params out of 10,788,929 total params (65.71%)</code></pre>
</div>
</div>
</section>
<section id="ii-evaluation-of-main-model-vs-3-alternate-models" class="level3">
<h3 class="anchored" data-anchor-id="ii-evaluation-of-main-model-vs-3-alternate-models">II: Evaluation of Main Model vs 3 Alternate Models</h3>
<p>As described in the <a href="#evaluating-the-approximation">Evaluation section</a>, I trained the same transformer architecture used for the main model in this post three additional times, starting from a different random seed each time. This appendix shows the code used to measure the average Hellinger distance between the output of the main models and each of the three alternates, across the 20,000 sample prompts. The results provide a plausible lower bound for the threshold Hellinger distance that indicates a meaningful change in an output probability distribution.</p>
<p>First, we instantiate the three alternate models from their saved weights:</p>
<div id="cell-190" class="cell">
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>alt_models_dir <span class="op">=</span> environment.data_root <span class="op">/</span> <span class="st">'alternate-models/model-training/20240112-training/outputs/'</span></span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> alt_models_dir.exists(), <span class="st">"Alternate models directory does not exist. Run the training code in ../experiments/alternate-models.ipynb."</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-191" class="cell">
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate the three alternative trained models</span></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>m_alt1, _ <span class="op">=</span> create_model_and_tokenizer(</span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a>    saved_model_filename<span class="op">=</span>alt_models_dir <span class="op">/</span> <span class="st">'shakespeare-20240112-1.pt'</span>,</span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a>    dataset<span class="op">=</span>ts,</span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>device,</span>
<span id="cb111-6"><a href="#cb111-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb111-7"><a href="#cb111-7" aria-hidden="true" tabindex="-1"></a>m_alt2, _ <span class="op">=</span> create_model_and_tokenizer(</span>
<span id="cb111-8"><a href="#cb111-8" aria-hidden="true" tabindex="-1"></a>    saved_model_filename<span class="op">=</span>alt_models_dir <span class="op">/</span> <span class="st">'shakespeare-20240112-2.pt'</span>,</span>
<span id="cb111-9"><a href="#cb111-9" aria-hidden="true" tabindex="-1"></a>    dataset<span class="op">=</span>ts,</span>
<span id="cb111-10"><a href="#cb111-10" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>device,</span>
<span id="cb111-11"><a href="#cb111-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb111-12"><a href="#cb111-12" aria-hidden="true" tabindex="-1"></a>m_alt3, _ <span class="op">=</span> create_model_and_tokenizer(</span>
<span id="cb111-13"><a href="#cb111-13" aria-hidden="true" tabindex="-1"></a>    saved_model_filename<span class="op">=</span>alt_models_dir <span class="op">/</span> <span class="st">'shakespeare-20240112-3.pt'</span>,</span>
<span id="cb111-14"><a href="#cb111-14" aria-hidden="true" tabindex="-1"></a>    dataset<span class="op">=</span>ts,</span>
<span id="cb111-15"><a href="#cb111-15" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>device,</span>
<span id="cb111-16"><a href="#cb111-16" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we feed the input tokens into the original model and the three alternate models and get their output probability distributions:</p>
<div id="cell-193" class="cell">
<details class="code-fold">
<summary>Helper function to get the model’s output probability distribution for a batch of tokenized inputs.</summary>
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_model_probs(model: TransformerLanguageModel, tokens: torch.Tensor):</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Helper function to get a model's output probability distribution for a</span></span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a><span class="co">    batch of tokenized inputs."""</span></span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true" tabindex="-1"></a>    logits, _ <span class="op">=</span> model(tokens)</span>
<span id="cb112-5"><a href="#cb112-5" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> LogitsWrapper(logits.detach(), tokenizer)</span>
<span id="cb112-6"><a href="#cb112-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logits.probs()[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># We'e only interested in the last token</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-194" class="cell">
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> encoding_helpers.tokenize_strings(prompts)</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a>model_probs <span class="op">=</span> get_model_probs(m, tokens)</span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a>alt_model_probs1 <span class="op">=</span> get_model_probs(m_alt1, tokens)</span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true" tabindex="-1"></a>alt_model_probs2 <span class="op">=</span> get_model_probs(m_alt2, tokens)</span>
<span id="cb113-6"><a href="#cb113-6" aria-hidden="true" tabindex="-1"></a>alt_model_probs3 <span class="op">=</span> get_model_probs(m_alt3, tokens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can compute the Hellinger distance between the outputs from the three alternative models and the outputs from the original model. Remember that each of the model probabilities tensors (<code>model_probs</code> and <code>alt_model_probs*</code>) is a 20,000x65 tensor i.e.&nbsp;20,000 probability distributions of 65 elements each.</p>
<p>We’re computing the Hellinger distance between those probability distributions. So for each alternative model, we end up with 20,000 Hellinger distance values. These values tell us, for each prompt, how the probability distribution for the next token predicted by one of the alternate models differed from the probability distribution predicted by the original model.</p>
<div id="cell-196" class="cell">
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>h_alt1 <span class="op">=</span> hellinger_distance(model_probs, alt_model_probs1)</span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>h_alt2 <span class="op">=</span> hellinger_distance(model_probs, alt_model_probs2)</span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a>h_alt3 <span class="op">=</span> hellinger_distance(model_probs, alt_model_probs3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With the Hellinger distances computed, we can look at aggregate stats:</p>
<div id="cell-198" class="cell">
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>h_alts <span class="op">=</span> torch.stack([h_alt1, h_alt2, h_alt3], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>h_alts.mean(dim<span class="op">=</span><span class="dv">0</span>), h_alts.std(dim<span class="op">=</span><span class="dv">0</span>), h_alts.<span class="bu">min</span>(dim<span class="op">=</span><span class="dv">0</span>).values, h_alts.<span class="bu">max</span>(dim<span class="op">=</span><span class="dv">0</span>).values</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor([0.1064, 0.1057, 0.1053]),
 tensor([0.0823, 0.0817, 0.0828]),
 tensor([0.0005, 0.0008, 0.0008]),
 tensor([0.8351, 0.7881, 0.8743]))</code></pre>
</div>
</div>
<p>For all three alternate models, the average Hellinger distance was ~0.11 ± 0.08. All had very small minimums (&lt;= 0.0008) and maximums around ~0.80.</p>
</section>
<section id="iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm" class="level3">
<h3 class="anchored" data-anchor-id="iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">III: The Output Embedding’s Norm Doesn’t Matter Because of the Final LayerNorm</h3>
<p>This appendix demonstrates the assertion from the <a href="#transformation-via-vector-addition">Transformation via Vector Addition</a> that the norm of the output embeddings from the final transformer block does not matter, because of the LayerNorm before the final linear layer.</p>
<p>To begin, let’s grab the final block outputs for the first 1000 prompts:</p>
<div id="cell-203" class="cell">
<div class="sourceCode cell-code" id="cb117"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the block outputs for the first 1000 prompts</span></span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a>tokens_sample <span class="op">=</span> encoding_helpers.tokenize_strings(prompts[:<span class="dv">1000</span>])</span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true" tabindex="-1"></a>_, io_accessors_sample <span class="op">=</span> accessors.run_model(accessors.embed_tokens(tokens_sample))</span>
<span id="cb117-5"><a href="#cb117-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-6"><a href="#cb117-6" aria-hidden="true" tabindex="-1"></a>final_block_outputs <span class="op">=</span> io_accessors_sample[<span class="op">-</span><span class="dv">1</span>].output(<span class="st">'.'</span>)[:, <span class="op">-</span><span class="dv">1</span>, :].clone()</span>
<span id="cb117-7"><a href="#cb117-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-8"><a href="#cb117-8" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> io_accessors_sample</span>
<span id="cb117-9"><a href="#cb117-9" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> gc.collect()</span>
<span id="cb117-10"><a href="#cb117-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-11"><a href="#cb117-11" aria-hidden="true" tabindex="-1"></a>final_block_outputs.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([1000, 384])</code></pre>
</div>
</div>
<p>Next, let’s create a copy of those outputs scaled by a factor of 10:</p>
<div id="cell-205" class="cell">
<div class="sourceCode cell-code" id="cb119"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>scaled_final_block_outputs <span class="op">=</span> final_block_outputs <span class="op">*</span> <span class="dv">10</span></span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a>scaled_final_block_outputs.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([1000, 384])</code></pre>
</div>
</div>
<p>Comparing average norms, we see that those of the scaled outputs indeed are 10 times bigger:</p>
<div id="cell-207" class="cell">
<div class="sourceCode cell-code" id="cb121"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>final_block_outputs.norm(dim<span class="op">=-</span><span class="dv">1</span>).mean(), scaled_final_block_outputs.norm(dim<span class="op">=-</span><span class="dv">1</span>).mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor(22.8909), tensor(228.9091))</code></pre>
</div>
</div>
<p>Now, let’s put both the original and scaled outputs through the final LayerNorm of the model and calculate the average norm of the results:</p>
<div id="cell-209" class="cell">
<div class="sourceCode cell-code" id="cb123"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a>layer_normed_original <span class="op">=</span> m.ln_f(final_block_outputs).detach()</span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a>layer_normed_scaled <span class="op">=</span> m.ln_f(scaled_final_block_outputs).detach()</span>
<span id="cb123-3"><a href="#cb123-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-4"><a href="#cb123-4" aria-hidden="true" tabindex="-1"></a>layer_normed_original.norm(dim<span class="op">=-</span><span class="dv">1</span>).mean(), layer_normed_scaled.norm(dim<span class="op">=-</span><span class="dv">1</span>).mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor(23.1262), tensor(23.1263))</code></pre>
</div>
</div>
<p>They’re virtually identical.</p>
<p>In the preceding example, the output norms were so close to identical because the two inputs differed only in scale: they had the same direction, or cosine similarity of 1. Vectors that have different norms and different directions will emerge from the LayerNorm with norms that are still quite similar but a little further apart.</p>
<p>To see an example, we can add a little noise to one of the vectors and then scale it:</p>
<div id="cell-212" class="cell">
<div class="sourceCode cell-code" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>original_vector <span class="op">=</span> final_block_outputs[<span class="dv">0</span>]</span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Add some random noise to the original vector</span></span>
<span id="cb125-4"><a href="#cb125-4" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>) <span class="co"># keep the noise consistent</span></span>
<span id="cb125-5"><a href="#cb125-5" aria-hidden="true" tabindex="-1"></a>comparison_vector <span class="op">=</span> original_vector <span class="op">+</span> torch.randn_like(original_vector) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb125-6"><a href="#cb125-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-7"><a href="#cb125-7" aria-hidden="true" tabindex="-1"></a><span class="co"># And scale it</span></span>
<span id="cb125-8"><a href="#cb125-8" aria-hidden="true" tabindex="-1"></a>comparison_vector <span class="op">=</span> comparison_vector <span class="op">/</span> comparison_vector.norm()</span>
<span id="cb125-9"><a href="#cb125-9" aria-hidden="true" tabindex="-1"></a>comparison_vector <span class="op">*=</span> <span class="dv">10</span> <span class="op">*</span> original_vector.norm()</span>
<span id="cb125-10"><a href="#cb125-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-11"><a href="#cb125-11" aria-hidden="true" tabindex="-1"></a>original_vector.norm(), comparison_vector.norm(), F.cosine_similarity(original_vector, comparison_vector, dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor(23.7909), tensor(237.9092), tensor(0.9967))</code></pre>
</div>
</div>
<p>The <code>comparison_vector</code>’s norm is exactly 10x that of <code>original_vector</code>, but they’re not perfectly aligned in direction, though still quite close.</p>
<div id="cell-214" class="cell">
<div class="sourceCode cell-code" id="cb127"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a>m.ln_f(original_vector).detach().norm(), m.ln_f(comparison_vector).detach().norm()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor(23.1496), tensor(23.1671))</code></pre>
</div>
</div>
<p>Their norms after layer norm are close but further apart than in the previous example.</p>
<p>If we add a lot more noise, we’ll end up with two vectors with quite different directions:</p>
<div id="cell-216" class="cell">
<div class="sourceCode cell-code" id="cb129"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a>original_vector <span class="op">=</span> final_block_outputs[<span class="dv">0</span>]</span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Add some random noise to the original vector</span></span>
<span id="cb129-4"><a href="#cb129-4" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">4211</span>) <span class="co"># keep the noise consistent</span></span>
<span id="cb129-5"><a href="#cb129-5" aria-hidden="true" tabindex="-1"></a>comparison_vector <span class="op">=</span> original_vector <span class="op">+</span> torch.randn_like(original_vector) <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb129-6"><a href="#cb129-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-7"><a href="#cb129-7" aria-hidden="true" tabindex="-1"></a><span class="co"># And scale it</span></span>
<span id="cb129-8"><a href="#cb129-8" aria-hidden="true" tabindex="-1"></a>comparison_vector <span class="op">=</span> comparison_vector <span class="op">/</span> comparison_vector.norm()</span>
<span id="cb129-9"><a href="#cb129-9" aria-hidden="true" tabindex="-1"></a>comparison_vector <span class="op">*=</span> <span class="dv">10</span> <span class="op">*</span> original_vector.norm()</span>
<span id="cb129-10"><a href="#cb129-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-11"><a href="#cb129-11" aria-hidden="true" tabindex="-1"></a>original_vector.norm(), comparison_vector.norm(), F.cosine_similarity(original_vector, comparison_vector, dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor(23.7909), tensor(237.9093), tensor(0.5178))</code></pre>
</div>
</div>
<p>But their norms after layer norm are only a little more divergent:</p>
<div id="cell-218" class="cell">
<div class="sourceCode cell-code" id="cb131"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>m.ln_f(original_vector).detach().norm(), m.ln_f(comparison_vector).detach().norm()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(tensor(23.1496), tensor(23.0546))</code></pre>
</div>
</div>
<p>So in summary:</p>
<ul>
<li>The LayerNorm will remove substantial differences in input norms.</li>
<li>Norms of the outputs from the LayerNorm will vary a little depending on how closely aligned the input vectors were.</li>
</ul>
</section>
<section id="iv-summary-of-experiment-on-relative-impact-of-self-attention-and-feed-forward-network-outputs" class="level3">
<h3 class="anchored" data-anchor-id="iv-summary-of-experiment-on-relative-impact-of-self-attention-and-feed-forward-network-outputs">IV: Summary of Experiment on Relative Impact of Self-Attention and Feed Forward Network Outputs</h3>
<p>The <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb">embedding adjustments analysis notebook</a> contains the implementation of an experiment to understand the relative impact of the self-attention outputs and the feed-forward network outputs on the final output of the transformer. This appendix summarizes the experiment and the results.</p>
<p>Experiment procedure:</p>
<ul>
<li>I ran all 20,000 prompts through the model and captured the final output probability distributions as well as the intermediate self-attention outputs, feed-forward network outputs, and final block outputs for each block.</li>
<li>For each block, I then ran two tests. First, instead of sending the block output as normally implemented (<code>x + sa_out + ffwd_out</code>, as shown <a href="#block-logic-with-intermediates">earlier</a>) to the next stage of the model, I sent a version that omits the self-attention output i.e.&nbsp;just <code>x + ffwd_out</code>, and saved the final output probability distribution that resulted. Then, I did the same thing but removed the feed-forward network output instead, sending on just <code>x + sa_out</code>.</li>
<li>I then calculated the Hellinger distance between the probability distribution produced with the regular block output and that produced by each of the two modifications.</li>
</ul>
<p>The table below shows the results, averaged across all 20,000 prompts:</p>
<blockquote class="blockquote">
<p>For the implementation of this analysis, see <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb">embedding adjustments analysis notebook</a></p>
</blockquote>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Block</th>
<th>H(output(<code>x+sa_out+ffwd_out</code>), output(<code>x+ffwd_out</code>))</th>
<th>H(output(<code>x+sa_out+ffwd_out</code>), output(<code>x+sa_out</code>))</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0.11 ± 0.07</td>
<td>0.70 ± 0.17</td>
</tr>
<tr class="even">
<td>2</td>
<td>0.07 ± 0.04</td>
<td>0.19 ± 0.11</td>
</tr>
<tr class="odd">
<td>3</td>
<td>0.09 ± 0.07</td>
<td>0.15 ± 0.10</td>
</tr>
<tr class="even">
<td>4</td>
<td>0.06 ± 0.05</td>
<td>0.13 ± 0.10</td>
</tr>
<tr class="odd">
<td>5</td>
<td>0.04 ± 0.03</td>
<td>0.14 ± 0.10</td>
</tr>
<tr class="even">
<td>6</td>
<td>0.03 ± 0.03</td>
<td>0.17 ± 0.10</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p>Remember that Hellinger distance ranges between 0 and 1, with 0 meaning identical and 1 meaning no overlap. A larger Hellinger distance in this table means a larger divergence between the experiment output and the normal transformer output.</p>
</blockquote>
<p>The effect is most pronounced in the first block: omitting the feed-forward network output results in an almost completely different probability distribution (H = 0.70) but omitting the self-attention output results in a very similar distribution (H = 0.11). Across the rest of the layers, the difference is less dramatic, but the feed-forward network output always has the larger impact.</p>
<blockquote class="blockquote">
<p>Though I think these results support the notion that an approximation based only on feed-forward network outputs can produce similar results to the transformer, it would be interesting to see if the approximation would improve if we include the self-attention outputs, particularly for some of the intermediate layers. But I’m leaving that as an area for future investigation.</p>
</blockquote>
</section>
<section id="v-performing-svd-to-get-a-linear-approximation-of-a-token-subspace" class="level3">
<h3 class="anchored" data-anchor-id="v-performing-svd-to-get-a-linear-approximation-of-a-token-subspace">V: Performing SVD to Get a Linear Approximation of a Token Subspace</h3>
<p>This appendix walks through an example of how we can find a linear approximation for a token subspace using SVD. First, let’s load all the embeddings learned for the token <code>a</code> at the output of the last block of the transformer (input to the final layer norm and linear layer):</p>
<div id="cell-222" class="cell">
<div class="sourceCode cell-code" id="cb133"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a>learned_embeddings_dir <span class="op">=</span> environment.data_root <span class="op">/</span> <span class="st">'learned_embeddings'</span></span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a>multi_emb_a <span class="op">=</span> torch.load(learned_embeddings_dir <span class="op">/</span> <span class="st">'no_blocks'</span> <span class="op">/</span> <span class="st">'lower_a.pt'</span>, map_location<span class="op">=</span>device)</span>
<span id="cb133-3"><a href="#cb133-3" aria-hidden="true" tabindex="-1"></a>multi_emb_a.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([100, 1, 384])</code></pre>
</div>
</div>
<p>We’ve got 100 different 384-dimensional embedding vectors. Each one, when given as input to the final blocks in the transformer, produces an output distribution that assigns nearly all the probability mass to the token, <code>a</code>. Each one can be thought of as a point in the subspace for token <code>a</code>.</p>
<p>We can stack these embeddings form a 100x384 matrix:</p>
<p><span class="math display">\[
\begin{bmatrix}
    e_{1,1} &amp; e_{1,2} &amp; \dots  &amp; e_{1,384} \\
    e_{2,1} &amp; e_{2,2} &amp; \dots  &amp; e_{2,384} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    e_{100,1} &amp; e_{100,2} &amp; \dots &amp; e_{100,384}
\end{bmatrix}
\]</span></p>
<p>Next, we can run SVD on this matrix:</p>
<div id="cell-225" class="cell">
<div class="sourceCode cell-code" id="cb135"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a>_, S, V <span class="op">=</span> torch.linalg.svd(multi_emb_a[:, <span class="op">-</span><span class="dv">1</span>, :])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For this analysis, we’re only interested in the singular values (<code>S</code>) and the right singular vectors (<code>V</code>). We can plot the singular values:</p>
<div id="cell-227" class="cell">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plt.plot(S.numpy(), <span class="st">'-o'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-97-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The first singular value is much larger (over 6x) than the next, which suggests the first right singular vector alone might be a good approximation for the subspace that predicts token <code>a</code>. We can test what gets predicted when we use this first right singular vector as an embedding:</p>
<div id="cell-229" class="cell">
<div class="sourceCode cell-code" id="cb137"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a>v0a <span class="op">=</span> adjust_singular_vector_sign(V[<span class="dv">0</span>], multi_emb_a[:, <span class="op">-</span><span class="dv">1</span>, :])</span>
<span id="cb137-2"><a href="#cb137-2" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> LogitsWrapper(accessors.logits_from_embedding(unsqueeze_emb(v0a)), tokenizer)</span>
<span id="cb137-3"><a href="#cb137-3" aria-hidden="true" tabindex="-1"></a>logits.plot_probs(title<span class="op">=</span><span class="st">'Next Token Probability Distribution from First Right Singular Vector of embeddings for Token "a"'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="beyond-self-attention_files/figure-html/cell-98-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In this distribution, <code>a</code> has probability near 1 and every other token has probability near 0. So the first right singular vector is effectively another embedding that produces an output predicting <code>a</code> with near certainty.</p>
<p>But it’s different from the other 100 learned embeddings in an important way: it’s the vector that is best aligned with <em>all</em> of them. More formally, it’s the vector that minimizes the squared distance to all 100 other embedding vectors. In this way, the first right singular vector is like a good summary of the embedding vectors we started from.</p>
<p>The first right singular vector is a unit vector (as are all the singular vectors):</p>
<div id="cell-231" class="cell">
<div class="sourceCode cell-code" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a>v0a.norm()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(1.0000)</code></pre>
</div>
</div>
<p>Any vector along its span will produce an output distribution predicting <code>a</code>, similar to the one above (see <a href="#iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">Appendix III</a> for an explanation of why the transformer output is invariant to the scale of the final embedding). So the span of this vector is a good, linear approximation to the subspace for token <code>a</code>.</p>
<p>The same results we saw here for token <code>a</code> hold for the other tokens too. For each, if we stack all the learned embeddings and perform SVD, we find that the first right singular vector forms a good linear approximation of the token subspace.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/spather/transformer-experiments/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>