{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting the Projection and Feed-Forward Layers in a Self-Attention Block\n",
    "\n",
    "> A summary of my experiments to understand the projection layer and feed-forward layer of a self-attention block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Optional, Iterable, Protocol, Sequence, Tuple, TypeVar, Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "from fastcore.test import *\n",
    "from matplotlib.axes import Axes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a manual seed so output is deterministic (used same value as @karpathy)\n",
    "_ = torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "from transformer_experiments.common.substring_generator import all_unique_substrings\n",
    "from transformer_experiments.common.text_analysis import (\n",
    "    build_next_token_map,\n",
    "    SubstringFrequencyAnalysis,\n",
    "    top_nonzero_tokens\n",
    ")\n",
    "from transformer_experiments.common.utils import (\n",
    "    aggregate_by_string_key,\n",
    "    DataWrapper,\n",
    "    topk_across_batches,\n",
    ")\n",
    "from transformer_experiments.dataset_split import split_text_dataset\n",
    "from transformer_experiments.datasets.tinyshakespeare import (\n",
    "    TinyShakespeareDataSet,\n",
    ")\n",
    "from transformer_experiments.models.transformer import (\n",
    "    n_layer,\n",
    "    TransformerLanguageModel\n",
    ")\n",
    "from transformer_experiments.models.transformer_helpers import (\n",
    "    unsqueeze_emb,\n",
    "    EncodingHelpers,\n",
    "    LogitsWrapper,\n",
    "    TransformerAccessors\n",
    ")\n",
    "from transformer_experiments.trained_models.tinyshakespeare_transformer import (\n",
    "    create_model_and_tokenizer\n",
    ")\n",
    "from transformer_experiments.experiments.block_internals import (\n",
    "    BlockInternalsAccessors,\n",
    "    BlockInternalsExperiment,\n",
    "    BatchedBlockInternalsExperiment,\n",
    "    BlockInternalsAnalysis,\n",
    ")\n",
    "from transformer_experiments.experiments.similar_strings import (\n",
    "    SimilarStringsData,\n",
    "    SimilarStringsExperiment,\n",
    "    SimilarStringsResult\n",
    ")\n",
    "from transformer_experiments.experiments.logit_lens import LogitLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ts = TinyShakespeareDataSet(cache_file='../artifacts/input.txt')\n",
    "m, tokenizer = create_model_and_tokenizer(\n",
    "    saved_model_filename='../artifacts/shakespeare.pt',\n",
    "    dataset=ts,\n",
    "    device=device,\n",
    ")\n",
    "_, val_data = split_text_dataset(ts.text, tokenizer, train_pct=0.9)\n",
    "encoding_helpers = EncodingHelpers(tokenizer, device)\n",
    "accessors = TransformerAccessors(m, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"device is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings10 = all_unique_substrings(ts.text, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a next token map for each prefix length that we've run experiments for.\n",
    "next_token_map3 = build_next_token_map(ts.text, prefix_len=3, vocab_size=tokenizer.vocab_size, stoi=tokenizer.stoi)\n",
    "next_token_map4 = build_next_token_map(ts.text, prefix_len=4, vocab_size=tokenizer.vocab_size, stoi=tokenizer.stoi)\n",
    "next_token_map5 = build_next_token_map(ts.text, prefix_len=5, vocab_size=tokenizer.vocab_size, stoi=tokenizer.stoi)\n",
    "next_token_map6 = build_next_token_map(ts.text, prefix_len=6, vocab_size=tokenizer.vocab_size, stoi=tokenizer.stoi)\n",
    "next_token_map7 = build_next_token_map(ts.text, prefix_len=7, vocab_size=tokenizer.vocab_size, stoi=tokenizer.stoi)\n",
    "next_token_map8 = build_next_token_map(ts.text, prefix_len=8, vocab_size=tokenizer.vocab_size, stoi=tokenizer.stoi)\n",
    "next_token_map9 = build_next_token_map(ts.text, prefix_len=9, vocab_size=tokenizer.vocab_size, stoi=tokenizer.stoi)\n",
    "next_token_map10 = build_next_token_map(ts.text, prefix_len=10, vocab_size=tokenizer.vocab_size, stoi=tokenizer.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_token_lens = [3, 4, 5, 6, 7, 8, 9, 10]\n",
    "all_token_maps = [\n",
    "    next_token_map3,\n",
    "    next_token_map4,\n",
    "    next_token_map5,\n",
    "    next_token_map6,\n",
    "    next_token_map7,\n",
    "    next_token_map8,\n",
    "    next_token_map9,\n",
    "    next_token_map10\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the token maps into one\n",
    "next_token_map_all = {\n",
    "    **next_token_map3,\n",
    "    **next_token_map4,\n",
    "    **next_token_map5,\n",
    "    **next_token_map6,\n",
    "    **next_token_map7,\n",
    "    **next_token_map8,\n",
    "    **next_token_map9,\n",
    "    **next_token_map10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for entries that have no next token. This should only be the case\n",
    "# for cases where the last substring in the text is unique.\n",
    "for k, v in next_token_map_all.items():\n",
    "    if v.sum() == 0:\n",
    "        print(f\"{repr(k)} has no next tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check all the lengths are right.\n",
    "for l, token_map in zip(all_token_lens, all_token_maps):\n",
    "    for k in token_map.keys():\n",
    "        if len(k) != l:\n",
    "            print(f\"{repr(k)} has length {len(k)} but should have length {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if list(Path('../artifacts/block_internals_results/large_files/slen10/').glob('*')) == []:\n",
    "    print(\"Run `make block_internals_slen10_dataset` in the project root to generate the required dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp10 = BatchedBlockInternalsExperiment(\n",
    "    eh=encoding_helpers,\n",
    "    accessors=accessors,\n",
    "    strings=strings10,\n",
    "    output_dir=Path('../artifacts/block_internals_results/large_files/slen10/'),\n",
    "    batch_size=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a similar strings experiment on a bunch of sample strings we'll use for analysis\n",
    "output_dir = Path('../artifacts/block_internals_results/similar_strings_sample')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "ssexp = SimilarStringsExperiment(output_dir, encoding_helpers)\n",
    "sample_strings = ['First Citi', 'Citizen:\\nB', 'Shyamalan ', 'more in jo']\n",
    "n_similars = 10\n",
    "batch_size=len(sample_strings)\n",
    "if not (output_dir / 'string_to_batch_map.json').exists():\n",
    "    ssexp.generate_string_to_batch_map(sample_strings, batch_size)\n",
    "\n",
    "try:\n",
    "    _ = next(iter(output_dir.glob('embs_sim_strings-*.json')))\n",
    "except StopIteration:\n",
    "    ssexp.generate_embeddings_files(sample_strings, accessors, exp10, batch_size=batch_size, n_similars=n_similars)\n",
    "\n",
    "try:\n",
    "    _ = next(iter(output_dir.glob('proj_out_sim_strings-*.json')))\n",
    "except StopIteration:\n",
    "    ssexp.generate_proj_out_files(sample_strings, t_i=-1, accessors=accessors, exp=exp10, batch_size=batch_size, n_similars=n_similars)\n",
    "\n",
    "try:\n",
    "    _ = next(iter(output_dir.glob('ffwd_out_sim_strings-*.json')))\n",
    "except StopIteration:\n",
    "    ssexp.generate_ffwd_out_files(sample_strings, t_i=-1, accessors=accessors, exp=exp10, batch_size=batch_size, n_similars=n_similars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar('T', bound='SimilarStringsFrequencyAndDistanceData')\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SimilarStringsFrequencyAndDistanceData:\n",
    "    \"\"\"Encapsulates the frequency and distance data associated\n",
    "    with a set of `SimilarStringsResult`s.\"\"\"\n",
    "\n",
    "    strings: Sequence[str] # N strings\n",
    "    string_to_idx: Dict[str, int]  # Maps N strings to indices 0..N-1\n",
    "    emb_freqs: torch.Tensor  # (N, n_similars, vocab_size)\n",
    "    emb_distances: torch.Tensor  # (N, n_similars)\n",
    "    proj_freqs: torch.Tensor  # (N, n_layer, n_similars, vocab_size)\n",
    "    proj_distances: torch.Tensor  # (N, n_layer, n_similars)\n",
    "    ffwd_freqs: torch.Tensor  # (N, n_layer, n_similars, vocab_size)\n",
    "    ffwd_distances: torch.Tensor  # (N, n_layer, n_similars)\n",
    "\n",
    "    @classmethod\n",
    "    def from_results(\n",
    "        cls: Type[T],\n",
    "        ss_results: Dict[str, SimilarStringsResult],\n",
    "        next_token_map: Dict[str, torch.Tensor],\n",
    "        aggregate_over_t_is: Sequence[int] = [-1],\n",
    "        largest: bool = False,\n",
    "    ) -> T:\n",
    "        strings: List[str] = []\n",
    "        string_to_idx: Dict[str, int] = {}\n",
    "\n",
    "        all_emb_freqs = []\n",
    "        all_emb_distances = []\n",
    "        all_proj_freqs = []\n",
    "        all_proj_distances = []\n",
    "        all_ffwd_freqs = []\n",
    "        all_ffwd_distances = []\n",
    "\n",
    "        for i, (s, result) in enumerate(ss_results.items()):\n",
    "            strings.append(s)\n",
    "            string_to_idx[s] = i\n",
    "            aggr_proj_out, aggr_ffwd_out = result.aggregate_over_t_is(\n",
    "                aggregate_over_t_is,\n",
    "                largest=largest,\n",
    "            )\n",
    "\n",
    "            emb_freqs = torch.stack(\n",
    "                [next_token_map[s] for s in result.embs.sim_strings]\n",
    "            )\n",
    "            all_emb_freqs.append(emb_freqs)\n",
    "\n",
    "            emb_distances = result.embs.distances\n",
    "            all_emb_distances.append(emb_distances)\n",
    "\n",
    "            proj_freqs = torch.stack(\n",
    "                [\n",
    "                    torch.stack(\n",
    "                        [\n",
    "                            next_token_map[s]\n",
    "                            for s in aggr_proj_out[block_idx].sim_strings\n",
    "                        ]\n",
    "                    )\n",
    "                    for block_idx in range(n_layer)\n",
    "                ]\n",
    "            )\n",
    "            all_proj_freqs.append(proj_freqs)\n",
    "\n",
    "            proj_distances = torch.stack(\n",
    "                [aggr_proj_out[block_idx].distances for block_idx in range(n_layer)]\n",
    "            )\n",
    "            all_proj_distances.append(proj_distances)\n",
    "\n",
    "            ffwd_freqs = torch.stack(\n",
    "                [\n",
    "                    torch.stack(\n",
    "                        [\n",
    "                            next_token_map[s]\n",
    "                            for s in aggr_ffwd_out[block_idx].sim_strings\n",
    "                        ]\n",
    "                    )\n",
    "                    for block_idx in range(n_layer)\n",
    "                ]\n",
    "            )\n",
    "            all_ffwd_freqs.append(ffwd_freqs)\n",
    "\n",
    "            ffwd_distances = torch.stack(\n",
    "                [aggr_ffwd_out[block_idx].distances for block_idx in range(n_layer)]\n",
    "            )\n",
    "            all_ffwd_distances.append(ffwd_distances)\n",
    "\n",
    "        return cls(\n",
    "            strings,\n",
    "            string_to_idx,\n",
    "            torch.stack(all_emb_freqs),  # (len(ss_results), n_similars, vocab_size)\n",
    "            torch.stack(all_emb_distances),  # (len(ss_results), n_similars)\n",
    "            torch.stack(\n",
    "                all_proj_freqs\n",
    "            ),  # (len(ss_results), n_layer, n_similars, vocab_size)\n",
    "            torch.stack(all_proj_distances),  # (len(ss_results), n_layer, n_similars)\n",
    "            torch.stack(\n",
    "                all_ffwd_freqs\n",
    "            ),  # (len(ss_results), n_layer, n_similars, vocab_size)\n",
    "            torch.stack(all_ffwd_distances),  # (len(ss_results), n_layer, n_similars)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for SimilarStringsFrequencyAndDistanceData\n",
    "ss_data = SimilarStringsFrequencyAndDistanceData.from_results(\n",
    "    ss_results=ssexp.load_results_for_strings(sample_strings),\n",
    "    next_token_map=next_token_map_all,\n",
    "    aggregate_over_t_is=[-1],\n",
    ")\n",
    "test_eq(ss_data.strings, sample_strings)\n",
    "test_eq(len(ss_data.string_to_idx), len(sample_strings))\n",
    "test_eq(ss_data.emb_freqs.shape, (len(sample_strings), n_similars, tokenizer.vocab_size))\n",
    "test_eq(ss_data.emb_distances.shape, (len(sample_strings), n_similars))\n",
    "test_eq(ss_data.proj_freqs.shape, (len(sample_strings), n_layer, n_similars, tokenizer.vocab_size))\n",
    "test_eq(ss_data.proj_distances.shape, (len(sample_strings), n_layer, n_similars))\n",
    "test_eq(ss_data.ffwd_freqs.shape, (len(sample_strings), n_layer, n_similars, tokenizer.vocab_size))\n",
    "test_eq(ss_data.ffwd_distances.shape, (len(sample_strings), n_layer, n_similars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeNextTokenFreqs(Protocol):\n",
    "    def __call__(\n",
    "        self, prompt_idxs: torch.Tensor, ss_data: SimilarStringsFrequencyAndDistanceData\n",
    "    ) -> torch.Tensor:\n",
    "        ...\n",
    "\n",
    "\n",
    "class ModelSimulation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        ss_data: SimilarStringsFrequencyAndDistanceData,\n",
    "        compute_next_token_freqs: ComputeNextTokenFreqs,\n",
    "        encoding_helpers: EncodingHelpers,\n",
    "    ):\n",
    "        self.ss_data = ss_data\n",
    "        self.get_next_token_freqs = compute_next_token_freqs\n",
    "        self.encoding_helpers = encoding_helpers\n",
    "\n",
    "    def __call__(self, prompts: Sequence[str]):\n",
    "        prompt_idxs = torch.tensor(\n",
    "            [self.ss_data.string_to_idx[prompt] for prompt in prompts], dtype=torch.long\n",
    "        )\n",
    "\n",
    "        freqs = self.get_next_token_freqs(prompt_idxs, self.ss_data)\n",
    "\n",
    "        return [\n",
    "            top_nonzero_tokens(f, self.encoding_helpers.tokenizer.itos)[:10]\n",
    "            for f in freqs\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_outputs(prompts: Sequence[str], encoding_helpers: EncodingHelpers):\n",
    "    # Compute the model's predictions:\n",
    "    tokens = encoding_helpers.tokenize_strings(prompts)\n",
    "    logits, _ = m(tokens)\n",
    "\n",
    "    logits = LogitsWrapper(logits, encoding_helpers.tokenizer)\n",
    "    return [topk_tokens[-1] for topk_tokens in logits.topk_tokens(k=10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_token_freqs_progressive_ffwd_weight(\n",
    "    prompt_idxs: torch.Tensor, ss_data: SimilarStringsFrequencyAndDistanceData\n",
    "):\n",
    "    emb_weight = torch.tensor(1.0, dtype=torch.float32)\n",
    "    proj_weights = torch.tensor(\n",
    "        [1.0 for _ in range(n_layer)], dtype=torch.float32\n",
    "    ).unsqueeze(dim=1)\n",
    "    ffwd_weights = torch.tensor(\n",
    "        [1 + block_idx for block_idx in range(n_layer)], dtype=torch.float32\n",
    "    ).unsqueeze(dim=1)\n",
    "    freqs = (\n",
    "        (emb_weight * ss_data.emb_freqs[prompt_idxs, :]).sum(dim=1)\n",
    "        + (proj_weights * ss_data.proj_freqs[prompt_idxs, :, :, :].sum(dim=2)).sum(\n",
    "            dim=1\n",
    "        )\n",
    "        + (ffwd_weights * ss_data.ffwd_freqs[prompt_idxs, :, :, :].sum(dim=2)).sum(\n",
    "            dim=1\n",
    "        )\n",
    "    )\n",
    "    return freqs / freqs.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate = ModelSimulation(\n",
    "    ss_data=ss_data,\n",
    "    compute_next_token_freqs=next_token_freqs_progressive_ffwd_weight,\n",
    "    encoding_helpers=encoding_helpers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['more in jo',\n",
       " 'ore in joy',\n",
       " 're in joy ',\n",
       " 'e in joy a',\n",
       " ' in joy at',\n",
       " 'in joy at ',\n",
       " 'n joy at f',\n",
       " ' joy at fi',\n",
       " 'joy at fir',\n",
       " 'oy at firs']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings10[14423:14433]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_idx = {\n",
    "    s: i for i, s in enumerate(sample_strings)\n",
    "}\n",
    "sample_model_outputs = get_model_outputs(sample_strings, encoding_helpers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('z', 0.9380468726158142),\n",
       "  ('i', 0.03080154024064541),\n",
       "  ('e', 0.01610080525279045),\n",
       "  ('c', 0.004550227429717779),\n",
       "  ('h', 0.003850192530080676),\n",
       "  ('p', 0.002100104931741953),\n",
       "  (':', 0.0014000700321048498),\n",
       "  ('o', 0.0014000700321048498),\n",
       "  ('u', 0.0007000350160524249),\n",
       "  (' ', 0.0007000350160524249)],\n",
       " [('z', 0.9996668100357056),\n",
       "  ('u', 0.00010660554107744247),\n",
       "  ('I', 7.993520557647571e-05),\n",
       "  ('U', 2.734881672949996e-05),\n",
       "  ('K', 2.4257360564661212e-05),\n",
       "  ('P', 1.5074498151079752e-05),\n",
       "  ('L', 1.0885321898967959e-05),\n",
       "  ('n', 8.451069334114436e-06),\n",
       "  ('O', 8.223939403251279e-06),\n",
       "  ('f', 7.135453870432684e-06)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(['First Citi'])[0], sample_model_outputs[string_to_idx['First Citi']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('e', 0.3421829044818878),\n",
       "  ('u', 0.17404130101203918),\n",
       "  ('o', 0.13716813921928406),\n",
       "  ('h', 0.09587020426988602),\n",
       "  ('y', 0.08554572612047195),\n",
       "  ('a', 0.07669616490602493),\n",
       "  ('n', 0.033923305571079254),\n",
       "  ('i', 0.028023598715662956),\n",
       "  ('r', 0.011799409985542297),\n",
       "  ('t', 0.007374631240963936)],\n",
       " [('e', 0.47825106978416443),\n",
       "  ('u', 0.2509588301181793),\n",
       "  ('y', 0.1266946792602539),\n",
       "  ('r', 0.05788085237145424),\n",
       "  ('i', 0.03135434538125992),\n",
       "  ('o', 0.02440422773361206),\n",
       "  ('a', 0.017102370038628578),\n",
       "  ('l', 0.012891546823084354),\n",
       "  ('s', 8.761954813962802e-05),\n",
       "  ('R', 7.359156006714329e-05)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(['Citizen:\\nB'])[0], sample_model_outputs[string_to_idx['Citizen:\\nB']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('t', 0.15815085172653198),\n",
       "  ('b', 0.1228710487484932),\n",
       "  ('o', 0.11313868314027786),\n",
       "  ('a', 0.10340632498264313),\n",
       "  ('i', 0.09975668787956238),\n",
       "  ('d', 0.058394160121679306),\n",
       "  ('s', 0.05352798104286194),\n",
       "  ('w', 0.04866180196404457),\n",
       "  ('m', 0.03041362576186657),\n",
       "  ('f', 0.027980534359812737)],\n",
       " [('t', 0.16370470821857452),\n",
       "  ('s', 0.10785210877656937),\n",
       "  ('a', 0.09744462370872498),\n",
       "  ('b', 0.09677103161811829),\n",
       "  ('c', 0.08353256434202194),\n",
       "  ('m', 0.056587863713502884),\n",
       "  ('d', 0.048968441784381866),\n",
       "  ('p', 0.04876013845205307),\n",
       "  ('h', 0.04751131683588028),\n",
       "  ('w', 0.038983285427093506)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(['Shyamalan '])[0], sample_model_outputs[string_to_idx['Shyamalan ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('y', 0.6007066965103149),\n",
       "  ('t', 0.16607773303985596),\n",
       "  ('i', 0.07773851603269577),\n",
       "  ('s', 0.06713780760765076),\n",
       "  ('u', 0.038869258016347885),\n",
       "  ('m', 0.017667844891548157),\n",
       "  ('d', 0.010600706562399864),\n",
       "  ('r', 0.007067137863487005),\n",
       "  ('k', 0.0035335689317435026),\n",
       "  ('l', 0.0035335689317435026)],\n",
       " [('y', 0.8568735718727112),\n",
       "  ('i', 0.06098264083266258),\n",
       "  ('u', 0.04135835915803909),\n",
       "  ('c', 0.016126777976751328),\n",
       "  ('t', 0.012861563824117184),\n",
       "  ('l', 0.0022685928270220757),\n",
       "  ('o', 0.0021818610839545727),\n",
       "  ('s', 0.0016513006994500756),\n",
       "  ('v', 0.001559981144964695),\n",
       "  ('w', 0.0011889823945239186)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(['more in jo'])[0], sample_model_outputs[string_to_idx['more in jo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_simulate_results2(strings: Sequence[str], sim_outputs: Sequence, model_outputs: Sequence):\n",
    "    \"\"\"A version of analyze_simulate_results() that computes results for the\n",
    "    full length of the returned results.\"\"\"\n",
    "\n",
    "    topn_matches = [0 for _ in range(10)]\n",
    "    topn_matches_any_order = [0 for _ in range(10)]\n",
    "    for i, s in enumerate(strings):\n",
    "        sim_output = sim_outputs[i]\n",
    "        model_output = model_outputs[i]\n",
    "        sim_tokens, _ = zip(*sim_output)\n",
    "        model_tokens, _ = zip(*model_output)\n",
    "\n",
    "        n = min(len(sim_tokens), len(model_tokens))\n",
    "        for j in range(n):\n",
    "            if sim_tokens[j] == model_tokens[j]:\n",
    "                topn_matches[j] += 1\n",
    "            if set(sim_tokens[:j+1]) == set(model_tokens[:j+1]):\n",
    "                topn_matches_any_order[j] += 1\n",
    "\n",
    "    return topn_matches, topn_matches_any_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20000\n",
    "ss_exp20k = SimilarStringsExperiment(\n",
    "    exp10.output_dir / 'similar_strings',\n",
    "    encoding_helpers\n",
    ")\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "indices = torch.randperm(len(exp10.strings))[:n_samples]\n",
    "strings = [exp10.strings[i.item()] for i in indices]\n",
    "\n",
    "ss_results20k = ss_exp20k.load_results_for_strings(strings)\n",
    "\n",
    "ss_data20k = SimilarStringsFrequencyAndDistanceData.from_results(\n",
    "    ss_results=ss_results20k,\n",
    "    next_token_map=next_token_map_all,\n",
    "    aggregate_over_t_is=[-1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs20k = get_model_outputs(strings, encoding_helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim20k = ModelSimulation(\n",
    "    ss_data=ss_data20k,\n",
    "    compute_next_token_freqs=next_token_freqs_progressive_ffwd_weight,\n",
    "    encoding_helpers=encoding_helpers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_outputs = sim20k(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.774\n",
      "Top 1 matches (any order): 0.774\n",
      "Top 2 matches: 0.398\n",
      "Top 2 matches (any order): 0.452\n",
      "Top 3 matches: 0.211\n",
      "Top 3 matches (any order): 0.238\n",
      "Top 4 matches: 0.141\n",
      "Top 4 matches (any order): 0.143\n",
      "Top 5 matches: 0.102\n",
      "Top 5 matches (any order): 0.088\n",
      "Top 6 matches: 0.081\n",
      "Top 6 matches (any order): 0.054\n",
      "Top 7 matches: 0.059\n",
      "Top 7 matches (any order): 0.028\n",
      "Top 8 matches: 0.053\n",
      "Top 8 matches (any order): 0.018\n",
      "Top 9 matches: 0.046\n",
      "Top 9 matches (any order): 0.010\n",
      "Top 10 matches: 0.037\n",
      "Top 10 matches (any order): 0.005\n"
     ]
    }
   ],
   "source": [
    "topn_matches, topn_matches_any_order = analyze_simulate_results2(strings, sim_outputs, model_outputs20k)\n",
    "for i in range(10):\n",
    "    print(f\"Top {i+1} matches: {topn_matches[i] / n_samples:.3f}\")\n",
    "    print(f\"Top {i+1} matches (any order): {topn_matches_any_order[i] / n_samples:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_is=[3, 4, 5, 6, 7, 8, 9]\n",
    "ss_results20k_all_t_is = ss_exp20k.load_results_for_strings(strings, load_t_is=t_is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_data20k_aggr = SimilarStringsFrequencyAndDistanceData.from_results(\n",
    "    ss_results=ss_results20k_all_t_is,\n",
    "    next_token_map=next_token_map_all,\n",
    "    aggregate_over_t_is=t_is,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim20k_aggr = ModelSimulation(\n",
    "    ss_data=ss_data20k_aggr,\n",
    "    compute_next_token_freqs=next_token_freqs_progressive_ffwd_weight,\n",
    "    encoding_helpers=encoding_helpers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_outputs2 = sim20k_aggr(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.742\n",
      "Top 1 matches (any order): 0.742\n",
      "Top 2 matches: 0.363\n",
      "Top 2 matches (any order): 0.415\n",
      "Top 3 matches: 0.197\n",
      "Top 3 matches (any order): 0.212\n",
      "Top 4 matches: 0.136\n",
      "Top 4 matches (any order): 0.130\n",
      "Top 5 matches: 0.098\n",
      "Top 5 matches (any order): 0.081\n",
      "Top 6 matches: 0.080\n",
      "Top 6 matches (any order): 0.048\n",
      "Top 7 matches: 0.060\n",
      "Top 7 matches (any order): 0.027\n",
      "Top 8 matches: 0.053\n",
      "Top 8 matches (any order): 0.016\n",
      "Top 9 matches: 0.044\n",
      "Top 9 matches (any order): 0.010\n",
      "Top 10 matches: 0.036\n",
      "Top 10 matches (any order): 0.004\n"
     ]
    }
   ],
   "source": [
    "n_samples = 20000\n",
    "topn_matches, topn_matches_any_order = analyze_simulate_results2(strings, sim_outputs2, model_outputs20k)\n",
    "for i in range(10):\n",
    "    print(f\"Top {i+1} matches: {topn_matches[i] / n_samples:.3f}\")\n",
    "    print(f\"Top {i+1} matches (any order): {topn_matches_any_order[i] / n_samples:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('a', 0.7179487347602844),\n",
       "  ('i', 0.09455128014087677),\n",
       "  ('e', 0.06410256773233414),\n",
       "  ('h', 0.035256411880254745),\n",
       "  ('o', 0.028846153989434242),\n",
       "  ('t', 0.025641025975346565),\n",
       "  ('u', 0.01923076994717121),\n",
       "  ('r', 0.008012820966541767),\n",
       "  ('v', 0.0032051282469183207),\n",
       "  ('c', 0.0016025641234591603)],\n",
       " [('a', 0.4602494537830353),\n",
       "  ('e', 0.35252559185028076),\n",
       "  ('o', 0.09188850224018097),\n",
       "  ('i', 0.09030349552631378),\n",
       "  ('u', 0.004192721098661423),\n",
       "  ('y', 0.0007521358784288168),\n",
       "  ('r', 6.647213740507141e-05),\n",
       "  ('l', 3.957989065384027e-06),\n",
       "  ('v', 2.812936827467638e-06),\n",
       "  ('w', 2.738903503995971e-06)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim20k_aggr(['my most gr'])[0], model_outputs20k[ss_data20k.string_to_idx['my most gr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('a', 0.74631267786026),\n",
       "  ('i', 0.14454276859760284),\n",
       "  ('o', 0.05604719743132591),\n",
       "  ('e', 0.02654867246747017),\n",
       "  ('n', 0.005899704992771149),\n",
       "  ('c', 0.005899704992771149),\n",
       "  ('v', 0.005899704992771149),\n",
       "  ('d', 0.0029498524963855743),\n",
       "  ('r', 0.0029498524963855743),\n",
       "  ('l', 0.0029498524963855743)],\n",
       " [('a', 0.4602494537830353),\n",
       "  ('e', 0.35252559185028076),\n",
       "  ('o', 0.09188850224018097),\n",
       "  ('i', 0.09030349552631378),\n",
       "  ('u', 0.004192721098661423),\n",
       "  ('y', 0.0007521358784288168),\n",
       "  ('r', 6.647213740507141e-05),\n",
       "  ('l', 3.957989065384027e-06),\n",
       "  ('v', 2.812936827467638e-06),\n",
       "  ('w', 2.738903503995971e-06)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim20k(['my most gr'])[0], model_outputs20k[ss_data20k.string_to_idx['my most gr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sim_strings(result: SimilarStringsResult, aggregate_over_t_is: Sequence[int], largest: bool = False):\n",
    "    aggr_proj_out, aggr_ffwd_out = result.aggregate_over_t_is(aggregate_over_t_is, largest=largest)\n",
    "    n_similars = len(aggr_proj_out[0].sim_strings)\n",
    "    print(\"Proj Outputs\")\n",
    "    for i in range(n_similars):\n",
    "        print(''.join([f\"{repr(aggr_proj_out[block_idx].sim_strings[i]):>14} ({aggr_proj_out[block_idx].distances[i]:.2f})\" for block_idx in range(n_layer)]))\n",
    "\n",
    "    print()\n",
    "    print(\"FFwd Outputs\")\n",
    "    for i in range(n_similars):\n",
    "        print(''.join([f\"{repr(aggr_ffwd_out[block_idx].sim_strings[i]):>14} ({aggr_ffwd_out[block_idx].distances[i]:.2f})\" for block_idx in range(n_layer)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj Outputs\n",
      "    my most gr (0.00)    my most gr (0.00)    my most gr (0.00)    my most gr (0.00)    my most gr (0.00)    my most gr (0.00)\n",
      "    ur most gr (0.79)    ur most gr (0.95)    is most gr (2.27)     y most gr (3.56)     my most r (4.90)     my most r (2.75)\n",
      "    is most gr (0.80)    ne most gr (0.96)    ur most gr (2.43)    ur most gr (3.95)     y most gr (5.00)     my most l (3.52)\n",
      "    ne most gr (0.80)    he most gr (1.05)     y most gr (2.56)     r most gr (4.34)         my gr (5.38)     my most h (3.79)\n",
      "    ilst my gr (0.82)    is most gr (1.06)    ne most gr (2.63)       most gr (4.51)         my sl (5.54)    my most st (3.91)\n",
      "    he most gr (0.84)    e, most gr (1.27)    he most gr (2.88)       most gu (4.61)     my most g (5.54)        my mos (3.97)\n",
      "    unto my gr (0.89)    o, must gr (1.35)     r most gr (2.99)       most gl (4.67)         my gh (5.69)        my mod (4.01)\n",
      "    e, most gr (0.89)    t, most gr (1.36)    e, most gr (3.16)    ne most gr (4.72)     my most l (5.76)        my mot (4.04)\n",
      "    t, most gr (0.90)    be past gr (1.37)     s most gr (3.18)     s most gr (4.85)    he most gr (5.85)        my mon (4.08)\n",
      "    yman to gr (0.92)    yet not gr (1.51)       most gr (3.20)     e most gr (4.86)    my most st (5.86)        my mou (4.14)\n",
      "\n",
      "FFwd Outputs\n",
      "    my most gr (0.00)    my most gr (0.00)    my most gr (0.00)    my most gr (0.00)    my most gr (0.00)    my most gr (0.00)\n",
      "    ne most gr (0.14)    ne most gr (0.43)    ur most gr (0.75)    ur most gr (1.77)    ur most gr (2.76)    t, most gr (3.17)\n",
      "    ur most gr (0.14)    ur most gr (0.44)    is most gr (0.78)    ne most gr (2.05)    he most gr (2.90)    e, most gr (3.23)\n",
      "    he most gr (0.14)    he most gr (0.51)    ne most gr (0.83)    is most gr (2.23)    ne most gr (2.94)    ur most gr (3.45)\n",
      "    e, most gr (0.14)    is most gr (0.57)    he most gr (0.89)    he most gr (2.56)     y most gr (2.98)     , most gr (3.49)\n",
      "    ilst my gr (0.15)    t, most gr (0.57)    e, most gr (1.23)     y most gr (3.26)     r most gr (3.41)     y most gr (3.72)\n",
      "    t, most gr (0.15)    e, most gr (0.58)    t, most gr (1.32)     r most gr (3.28)    is most gr (3.60)    ne most gr (3.88)\n",
      "    is most gr (0.15)    ver yet gr (0.66)    do them gr (2.21)    e, most gr (3.44)     e most gr (3.75)    is most gr (3.90)\n",
      "    unto my gr (0.15)     cannot gr (0.70)    im that gr (2.24)     s most gr (3.57)       most gr (3.88)    our own gr (3.95)\n",
      "    o, must gr (0.16)    o, must gr (0.71)    o, must gr (2.25)     e most gr (3.70)    e, most gr (3.89)         or gr (4.04)\n"
     ]
    }
   ],
   "source": [
    "print_sim_strings(ss_results20k_all_t_is['my most gr'], t_is)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to just looking at t_i=-1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj Outputs\n",
      "    my most gr (0.00)    my most gr (0.00)    my most gr (0.00)    my most gr (0.00)    my most gr (0.00)    my most gr (0.00)\n",
      "    ur most gr (0.79)    ur most gr (0.95)    is most gr (2.27)    ur most gr (3.95)    he most gr (5.85)    my most st (3.91)\n",
      "    is most gr (0.80)    ne most gr (0.96)    ur most gr (2.43)    ne most gr (4.72)    my most st (5.86)    my most sa (4.33)\n",
      "    ne most gr (0.80)    he most gr (1.05)    ne most gr (2.63)    nd most gu (5.16)    ur most gr (6.43)     my most r (4.56)\n",
      "    ilst my gr (0.82)    is most gr (1.06)    he most gr (2.88)    nd most gl (5.34)     my most r (6.52)     my most l (5.16)\n",
      "    he most gr (0.84)    e, most gr (1.27)    e, most gr (3.16)    he most ge (5.49)    my young r (6.56)    my high bl (5.20)\n",
      "    unto my gr (0.89)    o, must gr (1.35)    t, most gr (3.29)    is most gr (5.53)    my young p (6.58)    m thy moth (5.22)\n",
      "    e, most gr (0.89)    t, most gr (1.36)    nd most gl (3.57)    ld most gl (5.64)    my young c (6.79)    mt my mast (5.22)\n",
      "    t, most gr (0.90)    be past gr (1.37)    ld most gl (3.71)    e; most go (5.96)    my part sh (6.84)    my most he (5.34)\n",
      "    yman to gr (0.92)    yet not gr (1.51)    he most ge (4.52)    e, most gr (6.00)    my young l (6.87)    my merry m (5.45)\n",
      "\n",
      "FFwd Outputs\n",
      "    my most gr (0.00)    my most gr (0.00)    my most gr (0.00)    my most gr (0.00)    my most gr (0.00)    my most gr (0.00)\n",
      "    ne most gr (0.14)    ne most gr (0.43)    ur most gr (0.75)    ur most gr (1.77)    ur most gr (2.76)    t, most gr (3.17)\n",
      "    ur most gr (0.14)    ur most gr (0.44)    is most gr (0.78)    ne most gr (2.05)    he most gr (2.90)    e, most gr (3.23)\n",
      "    he most gr (0.14)    he most gr (0.51)    ne most gr (0.83)    is most gr (2.23)    ne most gr (2.94)    ur most gr (3.45)\n",
      "    e, most gr (0.14)    is most gr (0.57)    he most gr (0.89)    he most gr (2.56)    is most gr (3.60)    ne most gr (3.88)\n",
      "    ilst my gr (0.15)    t, most gr (0.57)    e, most gr (1.23)    e, most gr (3.44)    e, most gr (3.89)    is most gr (3.90)\n",
      "    t, most gr (0.15)    e, most gr (0.58)    t, most gr (1.32)    t, most gr (3.77)    t, most gr (4.29)    our own gr (3.95)\n",
      "    is most gr (0.15)    ver yet gr (0.66)    do them gr (2.21)     common gr (3.88)    dd more gr (5.07)     but my gr (4.61)\n",
      "    unto my gr (0.15)     cannot gr (0.70)    im that gr (2.24)    is more gr (4.06)     but my gr (5.27)    nto her gr (4.71)\n",
      "    o, must gr (0.16)    o, must gr (0.71)    o, must gr (2.25)    dd more gr (4.06)    is more gr (5.33)    dd more gr (4.73)\n"
     ]
    }
   ],
   "source": [
    "print_sim_strings(ss_results20k_all_t_is['my most gr'], [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we do if we just used the next tokens for the prompt? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_just_next_tokens_from_prompt(prompt: str, next_token_map: Dict[str, torch.Tensor], encoding_helpers: EncodingHelpers):\n",
    "    next_tokens = next_token_map[prompt]\n",
    "    return top_nonzero_tokens(\n",
    "        next_tokens.float() / next_tokens.sum(), encoding_helpers.tokenizer.itos\n",
    "    )[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3417f2f5cc46e799a02dfa3ff5e40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sim_jntfp_out = [\n",
    "    sim_just_next_tokens_from_prompt(s, next_token_map_all, encoding_helpers)\n",
    "    for s in tqdm(strings)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.606\n",
      "Top 1 matches (any order): 0.606\n",
      "Top 2 matches: 0.011\n",
      "Top 2 matches (any order): 0.012\n",
      "Top 3 matches: 0.002\n",
      "Top 3 matches (any order): 0.001\n",
      "Top 4 matches: 0.001\n",
      "Top 4 matches (any order): 0.001\n",
      "Top 5 matches: 0.001\n",
      "Top 5 matches (any order): 0.000\n",
      "Top 6 matches: 0.000\n",
      "Top 6 matches (any order): 0.000\n",
      "Top 7 matches: 0.000\n",
      "Top 7 matches (any order): 0.000\n",
      "Top 8 matches: 0.000\n",
      "Top 8 matches (any order): 0.000\n",
      "Top 9 matches: 0.000\n",
      "Top 9 matches (any order): 0.000\n",
      "Top 10 matches: 0.000\n",
      "Top 10 matches (any order): 0.000\n"
     ]
    }
   ],
   "source": [
    "n_samples = 20000\n",
    "topn_matches, topn_matches_any_order = analyze_simulate_results2(strings, sim_jntfp_out, model_outputs20k)\n",
    "for i in range(10):\n",
    "    print(f\"Top {i+1} matches: {topn_matches[i] / n_samples:.3f}\")\n",
    "    print(f\"Top {i+1} matches (any order): {topn_matches_any_order[i] / n_samples:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so 60% on the top 1 token, but it quickly falls off after that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_contributors(result: SimilarStringsResult, aggregate_over_t_is: Sequence[int]):\n",
    "    aggr_proj_out, aggr_ffwd_out = result.aggregate_over_t_is(aggregate_over_t_is)\n",
    "\n",
    "    s_to_next_tokens_map = {}\n",
    "\n",
    "    for s in result.embs.sim_strings:\n",
    "        s_to_next_tokens_map[s] = next_token_map_all[s]\n",
    "\n",
    "    for block_idx in range(n_layer):\n",
    "        for s in aggr_proj_out[block_idx].sim_strings:\n",
    "            s_to_next_tokens_map[s] = next_token_map_all[s]\n",
    "        for s in aggr_ffwd_out[block_idx].sim_strings:\n",
    "            s_to_next_tokens_map[s] = next_token_map_all[s]\n",
    "\n",
    "    def _print(item: Tuple[str, torch.Tensor]):\n",
    "        s, next_tokens = item\n",
    "        top_tokens = top_nonzero_tokens(next_tokens.float() / next_tokens.sum(), encoding_helpers.tokenizer.itos)\n",
    "        tokens_str = ', '.join([f\"{repr(t):>3} ({p:.2f})\" for t, p in top_tokens])\n",
    "        return f\"{repr(s):>14}: {tokens_str}\"\n",
    "\n",
    "    return DataWrapper(s_to_next_tokens_map.items(), _print)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  'my most gr': 'a' (1.00)\n",
      "  'my most sa': 'c' (1.00)\n",
      "  't, most gr': 'a' (1.00)\n",
      "  'my most st': 'a' (1.00)\n",
      "  'e, most gr': 'a' (1.00)\n",
      "  'ur most gr': 'a' (1.00)\n",
      "  'is most gr': 'i' (1.00)\n",
      "  'my most so': 'v' (1.00)\n",
      "  'my most re': 'd' (1.00)\n",
      "  'my most he': 'a' (1.00)\n",
      "  'ne most gr': 'a' (1.00)\n",
      "  'ilst my gr': 'o' (1.00)\n",
      "  'he most gr': 'a' (1.00)\n",
      "  'unto my gr': 'a' (1.00)\n",
      "  'yman to gr': 'i' (1.00)\n",
      "  'o, must gr': 'a' (1.00)\n",
      "  'be past gr': 'i' (1.00)\n",
      "  'yet not gr': 'e' (1.00)\n",
      "  'ver yet gr': 'e' (1.00)\n",
      "  ' cannot gr': 'e' (1.00)\n",
      "   'y most gr': 'a' (1.00)\n",
      "   'r most gr': 'a' (1.00)\n",
      "   's most gr': 'i' (1.00)\n",
      "    ' most gr': 'a' (0.89), 'i' (0.11)\n",
      "  'do them gr': 'a' (1.00)\n",
      "  'im that gr': 'a' (1.00)\n",
      "     'most gr': 'a' (0.89), 'i' (0.11)\n",
      "     'most gu': 'i' (1.00)\n",
      "    ' most gl': 'a' (1.00)\n",
      "   'e most gr': 'a' (1.00)\n",
      "   'my most r': 'e' (1.00)\n",
      "       'my gr': 'a' (0.55), 'i' (0.22), 'e' (0.16), 'o' (0.06)\n",
      "       'my sl': 'e' (0.40), 'a' (0.40), 'i' (0.20)\n",
      "   'my most g': 'r' (1.00)\n",
      "       'my gh': 'o' (1.00)\n",
      "   'my most l': 'o' (1.00)\n",
      "   'my most h': 'e' (1.00)\n",
      "      'my mos': 't' (1.00)\n",
      "      'my mod': 'e' (1.00)\n",
      "      'my mot': 'h' (1.00)\n",
      "      'my mon': 'e' (1.00)\n",
      "      'my mou': 't' (0.67), 'r' (0.33)\n",
      "   ', most gr': 'a' (1.00)\n",
      "  'our own gr': 'a' (1.00)\n",
      "       'or gr': 'a' (0.56), 'e' (0.19), 'u' (0.12), 'o' (0.06), 'i' (0.06)\n"
     ]
    }
   ],
   "source": [
    "unique_contributors(ss_results20k_all_t_is['my most gr'], t_is).print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_token_freqs_inv_distances(\n",
    "    prompt_idxs: torch.Tensor, ss_tensors: SimilarStringsFrequencyAndDistanceData\n",
    "):\n",
    "    emb_weight = torch.tensor(1.0, dtype=torch.float32)\n",
    "    proj_weights = torch.tensor(\n",
    "        [1.0 for _ in range(n_layer)], dtype=torch.float32\n",
    "    ).unsqueeze(dim=1)\n",
    "    ffwd_weights = torch.tensor(\n",
    "        [1 + block_idx for block_idx in range(n_layer)], dtype=torch.float32\n",
    "    ).unsqueeze(dim=1)\n",
    "\n",
    "    inv_emb_distances = (1 / (1 + ss_tensors.emb_distances[prompt_idxs, :])).unsqueeze(\n",
    "        dim=2\n",
    "    )\n",
    "\n",
    "    inv_proj_distances = (\n",
    "        1 / (1 + ss_data20k_aggr.proj_distances[prompt_idxs, :, :])\n",
    "    ).unsqueeze(dim=3)\n",
    "\n",
    "    inv_ffwd_distances = (\n",
    "        1 / (1 + ss_data20k_aggr.ffwd_distances[prompt_idxs, :, :])\n",
    "    ).unsqueeze(dim=3)\n",
    "\n",
    "    freqs = (\n",
    "        (emb_weight * ss_tensors.emb_freqs[prompt_idxs, :] * inv_emb_distances).sum(\n",
    "            dim=1\n",
    "        )\n",
    "        + (\n",
    "            proj_weights\n",
    "            * (ss_tensors.proj_freqs[prompt_idxs, :, :, :] * inv_proj_distances).sum(\n",
    "                dim=2\n",
    "            )\n",
    "        ).sum(dim=1)\n",
    "        + (\n",
    "            ffwd_weights\n",
    "            * (ss_tensors.ffwd_freqs[prompt_idxs, :, :, :] * inv_ffwd_distances).sum(\n",
    "                dim=2\n",
    "            )\n",
    "        ).sum(dim=1)\n",
    "    )\n",
    "    return freqs / freqs.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim20k_aggr_alt = ModelSimulation(\n",
    "    ss_data=ss_data20k_aggr,\n",
    "    compute_next_token_freqs=next_token_freqs_inv_distances,\n",
    "    encoding_helpers=encoding_helpers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_outputs_alt = sim20k_aggr_alt(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.699\n",
      "Top 1 matches (any order): 0.699\n",
      "Top 2 matches: 0.352\n",
      "Top 2 matches (any order): 0.404\n",
      "Top 3 matches: 0.208\n",
      "Top 3 matches (any order): 0.220\n",
      "Top 4 matches: 0.141\n",
      "Top 4 matches (any order): 0.136\n",
      "Top 5 matches: 0.103\n",
      "Top 5 matches (any order): 0.089\n",
      "Top 6 matches: 0.083\n",
      "Top 6 matches (any order): 0.055\n",
      "Top 7 matches: 0.065\n",
      "Top 7 matches (any order): 0.032\n",
      "Top 8 matches: 0.053\n",
      "Top 8 matches (any order): 0.020\n",
      "Top 9 matches: 0.043\n",
      "Top 9 matches (any order): 0.012\n",
      "Top 10 matches: 0.036\n",
      "Top 10 matches (any order): 0.005\n"
     ]
    }
   ],
   "source": [
    "n_samples = 20000\n",
    "topn_matches, topn_matches_any_order = analyze_simulate_results2(strings, sim_outputs_alt, model_outputs20k)\n",
    "for i in range(10):\n",
    "    print(f\"Top {i+1} matches: {topn_matches[i] / n_samples:.3f}\")\n",
    "    print(f\"Top {i+1} matches (any order): {topn_matches_any_order[i] / n_samples:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does worse at the top but a little better further down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a few other functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_token_freqs_only_last_ffwd(\n",
    "    prompt_idxs: torch.Tensor, ss_data: SimilarStringsFrequencyAndDistanceData\n",
    "):\n",
    "\n",
    "    freqs = ss_data.ffwd_freqs[prompt_idxs, -1, :, :].sum(dim=1)\n",
    "    return freqs / freqs.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_token_freqs_only_last_ffwd_with_distances(\n",
    "    prompt_idxs: torch.Tensor, ss_data: SimilarStringsFrequencyAndDistanceData\n",
    "):\n",
    "\n",
    "    inv_ffwd_distances = (\n",
    "        1 / (1 + ss_data20k_aggr.ffwd_distances[prompt_idxs, -1, :])\n",
    "    ).unsqueeze(dim=2)\n",
    "\n",
    "    freqs = (inv_ffwd_distances * ss_data.ffwd_freqs[prompt_idxs, -1, :, :]).sum(dim=1)\n",
    "    return freqs / freqs.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_next_token_freqs_function(\n",
    "    next_token_freqs_fn: ComputeNextTokenFreqs,\n",
    "    ss_data: SimilarStringsFrequencyAndDistanceData,\n",
    "    strings: Sequence[str],\n",
    "    model_outputs: Sequence[Tuple[str, float]],\n",
    "):\n",
    "    sim = ModelSimulation(\n",
    "        ss_data=ss_data,\n",
    "        compute_next_token_freqs=next_token_freqs_fn,\n",
    "        encoding_helpers=encoding_helpers,\n",
    "    )\n",
    "    sim_outputs = sim(strings)\n",
    "\n",
    "    n_samples = len(strings)\n",
    "    topn_matches, topn_matches_any_order = analyze_simulate_results2(\n",
    "        strings, sim_outputs, model_outputs\n",
    "    )\n",
    "    for i in range(10):\n",
    "        print(f\"Top {i+1} matches: {topn_matches[i] / n_samples:.3f}\")\n",
    "        print(\n",
    "            f\"Top {i+1} matches (any order): {topn_matches_any_order[i] / n_samples:.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.744\n",
      "Top 1 matches (any order): 0.744\n",
      "Top 2 matches: 0.311\n",
      "Top 2 matches (any order): 0.362\n",
      "Top 3 matches: 0.134\n",
      "Top 3 matches (any order): 0.148\n",
      "Top 4 matches: 0.071\n",
      "Top 4 matches (any order): 0.070\n",
      "Top 5 matches: 0.037\n",
      "Top 5 matches (any order): 0.030\n",
      "Top 6 matches: 0.020\n",
      "Top 6 matches (any order): 0.010\n",
      "Top 7 matches: 0.011\n",
      "Top 7 matches (any order): 0.003\n",
      "Top 8 matches: 0.007\n",
      "Top 8 matches (any order): 0.001\n",
      "Top 9 matches: 0.004\n",
      "Top 9 matches (any order): 0.001\n",
      "Top 10 matches: 0.002\n",
      "Top 10 matches (any order): 0.000\n"
     ]
    }
   ],
   "source": [
    "try_next_token_freqs_function(\n",
    "    next_token_freqs_only_last_ffwd, ss_data20k, strings, model_outputs20k\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nearly as good as the best baseline results for top1 and top2, but drops off after that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.676\n",
      "Top 1 matches (any order): 0.676\n",
      "Top 2 matches: 0.253\n",
      "Top 2 matches (any order): 0.300\n",
      "Top 3 matches: 0.109\n",
      "Top 3 matches (any order): 0.117\n",
      "Top 4 matches: 0.057\n",
      "Top 4 matches (any order): 0.054\n",
      "Top 5 matches: 0.032\n",
      "Top 5 matches (any order): 0.024\n",
      "Top 6 matches: 0.017\n",
      "Top 6 matches (any order): 0.009\n",
      "Top 7 matches: 0.010\n",
      "Top 7 matches (any order): 0.004\n",
      "Top 8 matches: 0.006\n",
      "Top 8 matches (any order): 0.002\n",
      "Top 9 matches: 0.005\n",
      "Top 9 matches (any order): 0.001\n",
      "Top 10 matches: 0.003\n",
      "Top 10 matches (any order): 0.001\n"
     ]
    }
   ],
   "source": [
    "try_next_token_freqs_function(\n",
    "    next_token_freqs_only_last_ffwd_with_distances, ss_data20k_aggr, strings, model_outputs20k\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single example, is it possible to choose weights that give you the same output as the model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is dreams,',\n",
       " 'by present',\n",
       " 's eyes may',\n",
       " 'eart of ho',\n",
       " ' man, as I',\n",
       " 'mour in a ',\n",
       " 'LLA:\\nAnd h',\n",
       " ' crave no ',\n",
       " 'o find the',\n",
       " 'e,\\nplease ']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'my most gr'\n",
    "\n",
    "prompt_idx = ss_data20k.string_to_idx[prompt]\n",
    "prompt_idxs = torch.tensor([prompt_idx], dtype=torch.long)\n",
    "\n",
    "emb_freqs = ss_data20k.emb_freqs[prompt_idxs, :, :]\n",
    "proj_freqs = ss_data20k.proj_freqs[prompt_idxs, :, :, :]\n",
    "ffwd_freqs = ss_data20k.ffwd_freqs[prompt_idxs, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = encoding_helpers.tokenize_string(prompt)\n",
    "logits, _ = m(tokens)\n",
    "model_output = F.softmax(logits[:, -1, :], dim=-1).squeeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337) # Ensure stable random values\n",
    "\n",
    "# Initialize all the learnable params\n",
    "emb_weight_param = torch.nn.Parameter(\n",
    "    torch.randn(1, dtype=torch.float32), requires_grad=True\n",
    ").to(device)\n",
    "proj_weights_param = torch.nn.Parameter(\n",
    "    torch.randn(n_layer, 1, dtype=torch.float32), requires_grad=True\n",
    ").to(device)\n",
    "ffwd_weights_param = torch.nn.Parameter(\n",
    "    torch.randn(n_layer, 1, dtype=torch.float32), requires_grad=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-3\n",
    "max_iters = 10000\n",
    "eval_interval=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2c4b57ae6f4dc9890ecce93ea64be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 0.437\n",
      "step 1000, loss: 0.017\n",
      "step 2000, loss: 0.005\n",
      "step 3000, loss: 0.005\n",
      "step 4000, loss: 0.005\n",
      "step 5000, loss: 0.005\n",
      "step 6000, loss: 0.005\n",
      "step 7000, loss: 0.005\n",
      "step 8000, loss: 0.005\n",
      "step 9000, loss: 0.005\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW([emb_weight_param, proj_weights_param, ffwd_weights_param], lr=learning_rate)\n",
    "eval_iters = max_iters // 10\n",
    "\n",
    "losses = []\n",
    "\n",
    "for step in tqdm(range(max_iters)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    freqs = (\n",
    "        (emb_weight_param * emb_freqs).sum(dim=1)\n",
    "        + (proj_weights_param * proj_freqs.sum(dim=2)).sum(dim=1)\n",
    "        + (ffwd_weights_param * ffwd_freqs.sum(dim=2)).sum(dim=1)\n",
    "    )\n",
    "    probs = freqs.squeeze(dim=0).float() / freqs.sum(dim=1)\n",
    "\n",
    "    loss = torch.norm(probs - model_output, p=2)\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if step % eval_iters == 0:\n",
    "        print(f\"step {step}, loss: {loss.item():.3f}\")\n",
    "\n",
    "    # Take a step\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    loss = loss.detach()\n",
    "    emb_freqs = emb_freqs.detach()\n",
    "    proj_freqs = proj_freqs.detach()\n",
    "    ffwd_freqs = ffwd_freqs.detach()\n",
    "    model_output = model_output.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('a', 0.4609237313270569),\n",
       "  ('e', 0.35285136103630066),\n",
       "  ('o', 0.09236107766628265),\n",
       "  ('i', 0.09082776308059692),\n",
       "  ('v', 0.0006160509656183422),\n",
       "  ('r', 0.0005453210324048996),\n",
       "  ('l', 0.0005453210324048996),\n",
       "  ('n', 0.0005146691692061722),\n",
       "  ('c', 0.0005067135789431632),\n",
       "  ('d', 0.0003080254828091711)],\n",
       " [('a', 0.4602494537830353),\n",
       "  ('e', 0.35252559185028076),\n",
       "  ('o', 0.09188850224018097),\n",
       "  ('i', 0.09030349552631378),\n",
       "  ('u', 0.004192721098661423),\n",
       "  ('y', 0.0007521358784288168),\n",
       "  ('r', 6.647213740507141e-05),\n",
       "  ('l', 3.957989065384027e-06),\n",
       "  ('v', 2.812936827467638e-06),\n",
       "  ('w', 2.738903503995971e-06)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    freqs = (\n",
    "        (emb_weight_param * emb_freqs).sum(dim=1)\n",
    "        + (proj_weights_param * proj_freqs.sum(dim=2)).sum(dim=1)\n",
    "        + (ffwd_weights_param * ffwd_freqs.sum(dim=2)).sum(dim=1)\n",
    "    )\n",
    "    probs = freqs.squeeze(dim=0).float() / freqs.sum(dim=1)\n",
    "\n",
    "(\n",
    "    top_nonzero_tokens(probs, encoding_helpers.tokenizer.itos)[:10],\n",
    "    top_nonzero_tokens(model_output, encoding_helpers.tokenizer.itos)[:10]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we tried these weights for everything? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.0925]),\n",
       " tensor([[ 0.0070],\n",
       "         [-0.7119],\n",
       "         [ 1.1149],\n",
       "         [ 0.0808],\n",
       "         [-0.0404],\n",
       "         [-0.6956]]),\n",
       " tensor([[-0.1006],\n",
       "         [ 0.2627],\n",
       "         [ 0.0467],\n",
       "         [ 0.1357],\n",
       "         [ 0.6802],\n",
       "         [-1.1193]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_weight_param.data, proj_weights_param.data, ffwd_weights_param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.548\n",
      "Top 1 matches (any order): 0.548\n",
      "Top 2 matches: 0.181\n",
      "Top 2 matches (any order): 0.184\n",
      "Top 3 matches: 0.098\n",
      "Top 3 matches (any order): 0.050\n",
      "Top 4 matches: 0.072\n",
      "Top 4 matches (any order): 0.021\n",
      "Top 5 matches: 0.060\n",
      "Top 5 matches (any order): 0.007\n",
      "Top 6 matches: 0.050\n",
      "Top 6 matches (any order): 0.004\n",
      "Top 7 matches: 0.040\n",
      "Top 7 matches (any order): 0.002\n",
      "Top 8 matches: 0.036\n",
      "Top 8 matches (any order): 0.002\n",
      "Top 9 matches: 0.031\n",
      "Top 9 matches (any order): 0.001\n",
      "Top 10 matches: 0.022\n",
      "Top 10 matches (any order): 0.000\n"
     ]
    }
   ],
   "source": [
    "# Try a run using these weights for everything\n",
    "\n",
    "def next_token_freqs_progressive_learned_weights(\n",
    "    prompt_idxs: torch.Tensor, ss_data: SimilarStringsFrequencyAndDistanceData\n",
    "):\n",
    "    emb_weight = emb_weight_param.data\n",
    "    proj_weights = proj_weights_param.data\n",
    "    ffwd_weights = ffwd_weights_param.data\n",
    "    freqs = (\n",
    "        (emb_weight * ss_data.emb_freqs[prompt_idxs, :]).sum(dim=1)\n",
    "        + (proj_weights * ss_data.proj_freqs[prompt_idxs, :, :, :].sum(dim=2)).sum(\n",
    "            dim=1\n",
    "        )\n",
    "        + (ffwd_weights * ss_data.ffwd_freqs[prompt_idxs, :, :, :].sum(dim=2)).sum(\n",
    "            dim=1\n",
    "        )\n",
    "    )\n",
    "    return freqs / freqs.sum(dim=1, keepdim=True)\n",
    "\n",
    "try_next_token_freqs_function(\n",
    "    next_token_freqs_progressive_learned_weights, ss_data20k, strings, model_outputs20k\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, clearly very bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to learn weights over a large number of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(\n",
    "    batch_size: int,\n",
    "    ss_data: SimilarStringsFrequencyAndDistanceData,\n",
    "    split: str = 'train',\n",
    "    train_pct: float = 0.5,\n",
    "):\n",
    "    n_train = int(len(ss_data.string_to_idx) * train_pct)\n",
    "    low = 0 if split == 'train' else n_train\n",
    "    high = n_train if split == 'train' else len(ss_data.string_to_idx)\n",
    "\n",
    "    prompt_idxs = torch.randint(\n",
    "        low=low, high=high, size=(batch_size,), dtype=torch.long\n",
    "    )\n",
    "    batch_strings = [ss_data.strings[i.item()] for i in prompt_idxs]\n",
    "\n",
    "    emb_freqs = ss_data.emb_freqs[prompt_idxs, :, :]\n",
    "    proj_freqs = ss_data.proj_freqs[prompt_idxs, :, :, :]\n",
    "    ffwd_freqs = ss_data.ffwd_freqs[prompt_idxs, :, :, :]\n",
    "\n",
    "    tokens = encoding_helpers.tokenize_strings(batch_strings)\n",
    "    logits, _ = m(tokens)\n",
    "    model_output = F.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "    return (\n",
    "        emb_freqs.detach(),\n",
    "        proj_freqs.detach(),\n",
    "        ffwd_freqs.detach(),\n",
    "        model_output.detach(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337) # Ensure stable random values\n",
    "\n",
    "# Initialize all the learnable params\n",
    "emb_weight_param = torch.nn.Parameter(\n",
    "    torch.randn(1, dtype=torch.float32), requires_grad=True\n",
    ").to(device)\n",
    "proj_weights_param = torch.nn.Parameter(\n",
    "    torch.randn(n_layer, 1, dtype=torch.float32), requires_grad=True\n",
    ").to(device)\n",
    "ffwd_weights_param = torch.nn.Parameter(\n",
    "    torch.randn(n_layer, 1, dtype=torch.float32), requires_grad=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-4\n",
    "max_iters = 10000\n",
    "eval_interval=500\n",
    "eval_iters = 200\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            emb_freqs, proj_freqs, ffwd_freqs, model_output = get_batch(batch_size, ss_data20k, split='train', train_pct=0.5)\n",
    "\n",
    "            freqs = (\n",
    "                (emb_weight_param * emb_freqs).sum(dim=1)\n",
    "                + (proj_weights_param * proj_freqs.sum(dim=2)).sum(dim=1)\n",
    "                + (ffwd_weights_param * ffwd_freqs.sum(dim=2)).sum(dim=1)\n",
    "            )\n",
    "            probs = freqs.float() / freqs.sum(dim=1, keepdim=True)\n",
    "\n",
    "            loss = torch.norm(probs - model_output, p=2, dim=1).sum()\n",
    "            losses[k] = loss.item()\n",
    "\n",
    "        out[split] = losses.mean()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260075cfe1d34ab7b24908f9d480bfbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train loss: 18.787, val_loss 18.656\n",
      "step 500, train loss: 18.488, val_loss 18.526\n",
      "step 1000, train loss: 18.462, val_loss 18.536\n",
      "step 1500, train loss: 18.320, val_loss 18.076\n",
      "step 2000, train loss: 18.156, val_loss 18.145\n",
      "step 2500, train loss: 18.047, val_loss 18.269\n",
      "step 3000, train loss: 18.095, val_loss 18.123\n",
      "step 3500, train loss: 18.156, val_loss 18.288\n",
      "step 4000, train loss: 18.011, val_loss 17.939\n",
      "step 4500, train loss: 17.811, val_loss 18.024\n",
      "step 5000, train loss: 18.163, val_loss 18.125\n",
      "step 5500, train loss: 18.144, val_loss 18.114\n",
      "step 6000, train loss: 18.147, val_loss 18.117\n",
      "step 6500, train loss: 17.959, val_loss 18.084\n",
      "step 7000, train loss: 18.106, val_loss 18.113\n",
      "step 7500, train loss: 17.988, val_loss 17.954\n",
      "step 8000, train loss: 18.221, val_loss 18.156\n",
      "step 8500, train loss: 18.161, val_loss 18.105\n",
      "step 9000, train loss: 18.026, val_loss 18.064\n",
      "step 9500, train loss: 18.170, val_loss 18.122\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW([emb_weight_param, proj_weights_param, ffwd_weights_param], lr=learning_rate)\n",
    "\n",
    "for step in tqdm(range(max_iters)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    emb_freqs, proj_freqs, ffwd_freqs, model_output = get_batch(batch_size, ss_data20k, split='train', train_pct=0.5)\n",
    "\n",
    "    freqs = (\n",
    "        (emb_weight_param * emb_freqs).sum(dim=1)\n",
    "        + (proj_weights_param * proj_freqs.sum(dim=2)).sum(dim=1)\n",
    "        + (ffwd_weights_param * ffwd_freqs.sum(dim=2)).sum(dim=1)\n",
    "    )\n",
    "    probs = freqs.float() / freqs.sum(dim=1, keepdim=True)\n",
    "\n",
    "    loss = torch.norm(probs - model_output, p=2, dim=1).sum()\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}, train loss: {losses['train']:.3f}, val_loss {losses['val']:.3f}\")\n",
    "\n",
    "    # Take a step\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.772\n",
      "Top 1 matches (any order): 0.772\n",
      "Top 2 matches: 0.403\n",
      "Top 2 matches (any order): 0.458\n",
      "Top 3 matches: 0.221\n",
      "Top 3 matches (any order): 0.251\n",
      "Top 4 matches: 0.149\n",
      "Top 4 matches (any order): 0.156\n",
      "Top 5 matches: 0.111\n",
      "Top 5 matches (any order): 0.102\n",
      "Top 6 matches: 0.088\n",
      "Top 6 matches (any order): 0.065\n",
      "Top 7 matches: 0.066\n",
      "Top 7 matches (any order): 0.037\n",
      "Top 8 matches: 0.052\n",
      "Top 8 matches (any order): 0.021\n",
      "Top 9 matches: 0.045\n",
      "Top 9 matches (any order): 0.013\n",
      "Top 10 matches: 0.033\n",
      "Top 10 matches (any order): 0.006\n"
     ]
    }
   ],
   "source": [
    "# Try a run using these weights for everything\n",
    "\n",
    "def next_token_freqs_progressive_learned_weights(\n",
    "    prompt_idxs: torch.Tensor, ss_data: SimilarStringsFrequencyAndDistanceData\n",
    "):\n",
    "    emb_weight = emb_weight_param.data\n",
    "    proj_weights = proj_weights_param.data\n",
    "    ffwd_weights = ffwd_weights_param.data\n",
    "    freqs = (\n",
    "        (emb_weight * ss_data.emb_freqs[prompt_idxs, :]).sum(dim=1)\n",
    "        + (proj_weights * ss_data.proj_freqs[prompt_idxs, :, :, :].sum(dim=2)).sum(\n",
    "            dim=1\n",
    "        )\n",
    "        + (ffwd_weights * ss_data.ffwd_freqs[prompt_idxs, :, :, :].sum(dim=2)).sum(\n",
    "            dim=1\n",
    "        )\n",
    "    )\n",
    "    return freqs / freqs.sum(dim=1, keepdim=True)\n",
    "\n",
    "try_next_token_freqs_function(\n",
    "    next_token_freqs_progressive_learned_weights, ss_data20k, strings, model_outputs20k\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is definitely the best result for top 2 we've seen and very nearly the best for top1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0092]),\n",
       " tensor([[ 0.0223],\n",
       "         [-0.1555],\n",
       "         [ 0.0149],\n",
       "         [ 0.0122],\n",
       "         [-0.0932],\n",
       "         [-0.0848]]),\n",
       " tensor([[-0.5732],\n",
       "         [-0.7713],\n",
       "         [-0.6631],\n",
       "         [-0.7545],\n",
       "         [-1.1689],\n",
       "         [-2.0063]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_weight_param.data, proj_weights_param.data, ffwd_weights_param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_learned_weights = ModelSimulation(\n",
    "    ss_data=ss_data20k,\n",
    "    compute_next_token_freqs=next_token_freqs_progressive_learned_weights,\n",
    "    encoding_helpers=encoding_helpers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('a', 0.7675697803497314),\n",
       "  ('i', 0.14597564935684204),\n",
       "  ('o', 0.055422890931367874),\n",
       "  ('e', 0.027286706492304802),\n",
       "  ('c', 0.0012530256062746048),\n",
       "  ('r', 0.0012431245995685458),\n",
       "  ('l', 0.0012431245995685458),\n",
       "  ('v', 0.0002444349229335785),\n",
       "  ('d', 0.00012221746146678925),\n",
       "  ('3', -0.0)],\n",
       " [('a', 0.4602494537830353),\n",
       "  ('e', 0.35252559185028076),\n",
       "  ('o', 0.09188850224018097),\n",
       "  ('i', 0.09030349552631378),\n",
       "  ('u', 0.004192721098661423),\n",
       "  ('y', 0.0007521358784288168),\n",
       "  ('r', 6.647213740507141e-05),\n",
       "  ('l', 3.957989065384027e-06),\n",
       "  ('v', 2.812936827467638e-06),\n",
       "  ('w', 2.738903503995971e-06)],\n",
       " [('a', 0.74631267786026),\n",
       "  ('i', 0.14454276859760284),\n",
       "  ('o', 0.05604719743132591),\n",
       "  ('e', 0.02654867246747017),\n",
       "  ('n', 0.005899704992771149),\n",
       "  ('c', 0.005899704992771149),\n",
       "  ('v', 0.005899704992771149),\n",
       "  ('d', 0.0029498524963855743),\n",
       "  ('r', 0.0029498524963855743),\n",
       "  ('l', 0.0029498524963855743)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_learned_weights(['my most gr'])[0], model_outputs20k[ss_data20k.string_to_idx['my most gr']], sim20k(['my most gr'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity data generation ran overnight. Let's look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_cos = SimilarStringsExperiment(\n",
    "    exp10.output_dir / 'similar_strings_cos',\n",
    "    encoding_helpers,\n",
    ")\n",
    "t_is = [7, 8, 9]\n",
    "ss_cos_results = ss_cos.load_results_for_strings(strings, load_t_is=t_is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.750\n",
      "Top 1 matches (any order): 0.750\n",
      "Top 2 matches: 0.366\n",
      "Top 2 matches (any order): 0.415\n",
      "Top 3 matches: 0.191\n",
      "Top 3 matches (any order): 0.207\n",
      "Top 4 matches: 0.134\n",
      "Top 4 matches (any order): 0.125\n",
      "Top 5 matches: 0.096\n",
      "Top 5 matches (any order): 0.077\n",
      "Top 6 matches: 0.077\n",
      "Top 6 matches (any order): 0.045\n",
      "Top 7 matches: 0.060\n",
      "Top 7 matches (any order): 0.024\n",
      "Top 8 matches: 0.050\n",
      "Top 8 matches (any order): 0.014\n",
      "Top 9 matches: 0.040\n",
      "Top 9 matches (any order): 0.008\n",
      "Top 10 matches: 0.033\n",
      "Top 10 matches (any order): 0.003\n"
     ]
    }
   ],
   "source": [
    "t_is = [7, 8, 9]\n",
    "ss_data_cos = SimilarStringsFrequencyAndDistanceData.from_results(\n",
    "    ss_results=ss_cos_results,\n",
    "    next_token_map=next_token_map_all,\n",
    "    aggregate_over_t_is=t_is,\n",
    "    largest=True,\n",
    ")\n",
    "try_next_token_freqs_function(\n",
    "    next_token_freqs_progressive_ffwd_weight, ss_data_cos, strings, model_outputs20k\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are pretty similar to the best we saw with Eucledian distance. But here's something interesting: let's flip the `largest` param so that it's not looking for the largest cosine similarity, but the smallest. This will mean we're looking at less similar samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_is = [7, 8, 9]\n",
    "ss_data_cos = SimilarStringsFrequencyAndDistanceData.from_results(\n",
    "    ss_results=ss_cos_results,\n",
    "    next_token_map=next_token_map_all,\n",
    "    aggregate_over_t_is=t_is,\n",
    "    largest=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.783\n",
      "Top 1 matches (any order): 0.783\n",
      "Top 2 matches: 0.422\n",
      "Top 2 matches (any order): 0.484\n",
      "Top 3 matches: 0.233\n",
      "Top 3 matches (any order): 0.271\n",
      "Top 4 matches: 0.170\n",
      "Top 4 matches (any order): 0.181\n",
      "Top 5 matches: 0.128\n",
      "Top 5 matches (any order): 0.119\n",
      "Top 6 matches: 0.101\n",
      "Top 6 matches (any order): 0.076\n",
      "Top 7 matches: 0.083\n",
      "Top 7 matches (any order): 0.041\n",
      "Top 8 matches: 0.069\n",
      "Top 8 matches (any order): 0.029\n",
      "Top 9 matches: 0.059\n",
      "Top 9 matches (any order): 0.018\n",
      "Top 10 matches: 0.051\n",
      "Top 10 matches (any order): 0.011\n"
     ]
    }
   ],
   "source": [
    "try_next_token_freqs_function(\n",
    "    next_token_freqs_progressive_ffwd_weight, ss_data_cos, strings, model_outputs20k\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results actually get BETTER! In fact, these are the best results yet. \n",
    "\n",
    "(I'm writing this up as if it were an intentional experiment but in fact I found this by accident. At first when I ran the cosine similarity results, I didn't have a `largest` parameter to pass into `SimilarStringsFrequencyAndDistanceData.from_results()`. So it was in fact doing the `largest=False` version. I got the results above. Then I fixed it to thread `largeest` through and the results got worse. And that inspired this line of thought.)\n",
    "\n",
    "This suggests I've gotten something fundamentally wrong thus far. I've been trying methods to find more and more similar values: looking for more similar values amongst shorter strings, investigating cosine similarity as a potentially better metric. But maybe the model is actually a much wider net i.e. the predictions from a given embedding are an aggregate of similar values from a much wider range. \n",
    "\n",
    "This explains why efforts to produce more similar values yield worse results e.g. the results from the aggregation of t_is from 3-9 produces more similar values, but worse overall results. Perhaps there just isn't enough variety in the most similar values to produce final results that resemble the model's predictions.\n",
    "\n",
    "Let's try a few experiments to test this hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at the cosine results with `largest=True` but only considering t_i=9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_is = [9]\n",
    "ss_cos_results_only9 = ss_cos.load_results_for_strings(strings, load_t_is=t_is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_data_cos = SimilarStringsFrequencyAndDistanceData.from_results(\n",
    "    ss_results=ss_cos_results,\n",
    "    next_token_map=next_token_map_all,\n",
    "    aggregate_over_t_is=t_is,\n",
    "    largest=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.775\n",
      "Top 1 matches (any order): 0.775\n",
      "Top 2 matches: 0.397\n",
      "Top 2 matches (any order): 0.452\n",
      "Top 3 matches: 0.211\n",
      "Top 3 matches (any order): 0.237\n",
      "Top 4 matches: 0.143\n",
      "Top 4 matches (any order): 0.142\n",
      "Top 5 matches: 0.103\n",
      "Top 5 matches (any order): 0.087\n",
      "Top 6 matches: 0.082\n",
      "Top 6 matches (any order): 0.055\n",
      "Top 7 matches: 0.065\n",
      "Top 7 matches (any order): 0.029\n",
      "Top 8 matches: 0.054\n",
      "Top 8 matches (any order): 0.021\n",
      "Top 9 matches: 0.047\n",
      "Top 9 matches (any order): 0.010\n",
      "Top 10 matches: 0.038\n",
      "Top 10 matches (any order): 0.005\n"
     ]
    }
   ],
   "source": [
    "try_next_token_freqs_function(\n",
    "    next_token_freqs_progressive_ffwd_weight, ss_data_cos, strings, model_outputs20k\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better than the results from aggregating across t_is 7-9 with `largest=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we try `largest=False` with t_i=9?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.783\n",
      "Top 1 matches (any order): 0.783\n",
      "Top 2 matches: 0.422\n",
      "Top 2 matches (any order): 0.484\n",
      "Top 3 matches: 0.233\n",
      "Top 3 matches (any order): 0.271\n",
      "Top 4 matches: 0.170\n",
      "Top 4 matches (any order): 0.181\n",
      "Top 5 matches: 0.128\n",
      "Top 5 matches (any order): 0.119\n",
      "Top 6 matches: 0.101\n",
      "Top 6 matches (any order): 0.076\n",
      "Top 7 matches: 0.083\n",
      "Top 7 matches (any order): 0.041\n",
      "Top 8 matches: 0.069\n",
      "Top 8 matches (any order): 0.029\n",
      "Top 9 matches: 0.059\n",
      "Top 9 matches (any order): 0.018\n",
      "Top 10 matches: 0.051\n",
      "Top 10 matches (any order): 0.011\n"
     ]
    }
   ],
   "source": [
    "ss_data_cos = SimilarStringsFrequencyAndDistanceData.from_results(\n",
    "    ss_results=ss_cos_results,\n",
    "    next_token_map=next_token_map_all,\n",
    "    aggregate_over_t_is=t_is,\n",
    "    largest=False,\n",
    ")\n",
    "try_next_token_freqs_function(\n",
    "    next_token_freqs_progressive_ffwd_weight, ss_data_cos, strings, model_outputs20k\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better than `largest=True` but worse than when we aggregated ti_s 7-9. Probably because amonst the t_i=7 and t_i=8 values there were some less similar candidates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the Euclidean distance version, but flip `largest=True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.783\n",
      "Top 1 matches (any order): 0.783\n",
      "Top 2 matches: 0.426\n",
      "Top 2 matches (any order): 0.487\n",
      "Top 3 matches: 0.236\n",
      "Top 3 matches (any order): 0.275\n",
      "Top 4 matches: 0.171\n",
      "Top 4 matches (any order): 0.185\n",
      "Top 5 matches: 0.126\n",
      "Top 5 matches (any order): 0.120\n",
      "Top 6 matches: 0.102\n",
      "Top 6 matches (any order): 0.080\n",
      "Top 7 matches: 0.082\n",
      "Top 7 matches (any order): 0.045\n",
      "Top 8 matches: 0.072\n",
      "Top 8 matches (any order): 0.032\n",
      "Top 9 matches: 0.060\n",
      "Top 9 matches (any order): 0.019\n",
      "Top 10 matches: 0.052\n",
      "Top 10 matches (any order): 0.009\n"
     ]
    }
   ],
   "source": [
    "t_is=[7, 8, 9]\n",
    "ss_data20k_aggr_backwards = SimilarStringsFrequencyAndDistanceData.from_results(\n",
    "    ss_results=ss_results20k_all_t_is,\n",
    "    next_token_map=next_token_map_all,\n",
    "    aggregate_over_t_is=t_is,\n",
    "    largest=True,\n",
    ")\n",
    "try_next_token_freqs_function(\n",
    "    next_token_freqs_progressive_ffwd_weight, ss_data20k_aggr_backwards, strings, model_outputs20k\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried this several times, and it seems the sweet spot is with t_is=[7, 8, 9]. If you include smaller t_is, the results get progressively worse. Which suggests there is some upper bound to the distance that yields best results. Let's see if we can hone in on the sweet spot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj Outputs\n",
      "    'om is gr' (3.76)    ' dost gi' (4.09)    'st my gr' (6.16)    'must giv' (6.67)    'ost my c' (7.47)  'my merry m' (5.45)\n",
      "    'at is gr' (3.75)    'uldst gi' (4.09)    ' some gr' (6.10)    ' many ge' (6.64)    'my own d' (7.46)  'my most he' (5.34)\n",
      "    'on my gu' (3.75)    'heart go' (4.07)    ' more gr' (6.00)    ' many gu' (6.64)    'ms thy c' (7.41)   'm my moth' (5.25)\n",
      "    ' dost gu' (3.72)    'canst gi' (4.04)    'ommon gr' (5.95)    'most gui' (6.42)    'y most g' (7.41)  'mt my mast' (5.22)\n",
      "    's not gu' (3.71)    ' last go' (4.03)    ' most gu' (5.83)   ', most gr' (6.13)    'most gen' (7.38)  'm thy moth' (5.22)\n",
      "    ' most gr' (3.68)    ' most go' (4.03)    ' most go' (5.79)  'e, most gr' (6.00)    's most r' (7.25)    'm thy mo' (5.21)\n",
      "    'or my gu' (3.68)    ' most gi' (4.00)   ' most\\ngl' (5.57)  'e; most go' (5.96)    'e most r' (7.16)    'my misfo' (5.20)\n",
      "    'nt is gu' (3.67)   '\\nMust gi' (3.94)   's more gr' (5.57)   'o most go' (5.81)    ' most gr' (7.08)  'my high bl' (5.20)\n",
      "    ' must gr' (3.64)    ' must gi' (3.92)   'd most gu' (5.55)  'ld most gl' (5.64)  'my young l' (6.87)   'mes my br' (5.18)\n",
      "    ' most gu' (3.55)    ' must go' (3.83)   'common gr' (5.48)    'most gla' (5.57)   'r most gr' (6.87)    'mt my ma' (5.17)\n",
      "\n",
      "FFwd Outputs\n",
      "   'e stirs: ' (6.66)   'm that gr' (6.03)    't not gr' (4.03)    ' many gr' (5.31)    'om my gr' (6.22)    'rk my gr' (5.52)\n",
      "   'h still: ' (6.66)   'n that gr' (6.03)    'es to gr' (4.03)    ' more gr' (5.27)    'ge or gr' (6.19)   'ty nor gr' (5.42)\n",
      "   's hands: ' (6.66)   ' great gr' (6.03)    ' past gr' (4.03)    'ut my gr' (5.26)    'at my gr' (6.12)   'on her gr' (5.41)\n",
      "   'o it is: ' (6.66)   'ferent gr' (6.02)    'ur to gr' (4.02)    ' good gr' (5.25)    'steel gr' (6.11)   'or old gr' (5.40)\n",
      "   ' sights: ' (6.66)   'ng but gr' (6.02)    ' must gr' (4.01)    ' made gr' (5.20)    'is my gr' (6.10)    'as my gr' (5.37)\n",
      "   ', it is: ' (6.65)   's most gr' (6.02)    'd not gr' (3.99)    'r own gr' (5.17)    'in me gr' (5.92)   'o many gr' (5.36)\n",
      "   'r it is: ' (6.65)   'r most gr' (6.01)    'l not gr' (3.98)    ' some gr' (5.14)   ' to my gr' (5.92)    'ot my gr' (5.33)\n",
      "   't it is: ' (6.64)   'ervant gr' (6.00)    ' fast gr' (3.97)    '-made gr' (5.06)    'ot my gr' (5.91)    ' many gr' (5.32)\n",
      "   ' a kiss: ' (6.63)   'mphant gr' (5.99)    ' from gr' (3.97)    'ommon gr' (4.96)    'ut my gr' (5.79)   'hat my gr' (5.29)\n",
      "   's it is: ' (6.63)   'cannot gr' (5.96)   'her to gr' (3.91)   'o many gr' (4.84)   'd more gr' (5.76)    'ommon gr' (5.27)\n"
     ]
    }
   ],
   "source": [
    "print_sim_strings(ss_results20k_all_t_is['my most gr'], aggregate_over_t_is=[7, 8, 9], largest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proj Outputs\n",
      "    '.\\nKing ' (4.46)     ' yet gr' (4.76)    'st my gr' (6.16)    'must giv' (6.67)    'ost my c' (7.47)  'my merry m' (5.45)\n",
      "     'm to gi' (4.46)    '\\nLet go' (4.75)     'some gr' (6.14)    ' many ge' (6.64)    'my own d' (7.46)  'my most he' (5.34)\n",
      "     ', or gi' (4.46)     'most gi' (4.75)    ' some gr' (6.10)    ' many gu' (6.64)     ' most g' (7.41)   'm my moth' (5.25)\n",
      "     'k to gi' (4.46)     'past gr' (4.73)     'm my gr' (6.06)    'most gui' (6.42)    'ms thy c' (7.41)  'mt my mast' (5.22)\n",
      "     '; to gr' (4.46)     'Most go' (4.72)    ' more gr' (6.00)     'm my gr' (6.37)    'y most g' (7.41)  'm thy moth' (5.22)\n",
      "    '\\nFor gi' (4.46)     'fast gr' (4.72)    'ommon gr' (5.95)     'more gr' (6.31)    'most gen' (7.38)    'm thy mo' (5.21)\n",
      "     'm is gr' (4.44)     'must gr' (4.71)    ' most gu' (5.83)     'most gi' (6.31)    'most\\ngl' (7.37)    'my misfo' (5.20)\n",
      "     ', so gi' (4.44)    '\\nBut gr' (4.70)     'most go' (5.80)     'must gr' (6.16)     ' most y' (7.36)  'my high bl' (5.20)\n",
      "     'From gi' (4.41)     'most gr' (4.69)    ' most go' (5.79)   ', most gr' (6.13)    ' most\\ng' (7.35)   'mes my br' (5.18)\n",
      "     'n, jog ' (4.33)     'Must gi' (4.69)     'more gr' (5.78)     't my gr' (6.11)    's most r' (7.25)    'mt my ma' (5.17)\n",
      "\n",
      "FFwd Outputs\n",
      "   'e stirs: ' (6.66)   'm that gr' (6.03)     'n to gr' (4.24)    ' many gr' (5.31)     'l my gr' (6.24)     'nown gr' (5.62)\n",
      "   'h still: ' (6.66)   'n that gr' (6.03)     'w to gr' (4.23)    ' more gr' (5.27)     'mmon gr' (6.24)     'many gr' (5.53)\n",
      "   's hands: ' (6.66)   ' great gr' (6.03)     't to gr' (4.22)    'ut my gr' (5.26)    'om my gr' (6.22)    'rk my gr' (5.52)\n",
      "   'o it is: ' (6.66)   'ferent gr' (6.02)     'must gr' (4.22)    ' good gr' (5.25)     'from gr' (6.21)     ' now gr' (5.44)\n",
      "   ' sights: ' (6.66)   'ng but gr' (6.02)     'mous gr' (4.20)    ' made gr' (5.20)    'ge or gr' (6.19)     ' own gr' (5.43)\n",
      "   ', it is: ' (6.65)   's most gr' (6.02)     'r to gr' (4.13)    'r own gr' (5.17)    'at my gr' (6.12)     '? or gr' (5.43)\n",
      "   'r it is: ' (6.65)   'r most gr' (6.01)     'l to gr' (4.12)    ' some gr' (5.14)    'steel gr' (6.11)   'ty nor gr' (5.42)\n",
      "   't it is: ' (6.64)   'ervant gr' (6.00)     's to gr' (4.05)     'ttle gr' (5.07)     'teel gr' (6.10)     'ndly gr' (5.41)\n",
      "   ' a kiss: ' (6.63)   'mphant gr' (5.99)     ' not gr' (4.05)     ' not gr' (5.06)    'is my gr' (6.10)   'on her gr' (5.41)\n",
      "   's it is: ' (6.63)   'cannot gr' (5.96)    't not gr' (4.03)    '-made gr' (5.06)     't or gr' (5.97)   'or old gr' (5.40)\n"
     ]
    }
   ],
   "source": [
    "print_sim_strings(ss_results20k_all_t_is['my most gr'], aggregate_over_t_is=[6, 7, 8, 9], largest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty crude analysis because we only have what we thought were the top 10 most similar values and we're flipping the whether we're looking for smallest vs largest. But these numbers give a ballpark of the distances that are worth considering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of whether there are similar strings of smaller length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slens = [3, 5, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings_n = {\n",
    "    n: all_unique_substrings(ts.text, n)\n",
    "    for n in slens\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_maps = {\n",
    "    n: build_next_token_map(\n",
    "        ts.text, prefix_len=n, vocab_size=tokenizer.vocab_size, stoi=tokenizer.stoi\n",
    "    )\n",
    "    for n in slens\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in slens:\n",
    "    if list(Path(f'../artifacts/block_internals_results/large_files/slen{n}/').glob('*')) == []:\n",
    "        print(f\"Run `make block_internals_slen{n}_dataset` in the project root to generate the required dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = {\n",
    "    n: BatchedBlockInternalsExperiment(\n",
    "        eh=encoding_helpers,\n",
    "        accessors=accessors,\n",
    "        strings=strings_n[n],\n",
    "        output_dir=Path(f'../artifacts/block_internals_results/large_files/slen{n}/'),\n",
    "        batch_size=10000,\n",
    "    )\n",
    "    for n in slens\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_similar_strings(\n",
    "    prompt: str,\n",
    "    block_idx: int,\n",
    "    compare_to_slen: int,\n",
    "):\n",
    "    prompt_accessors = BlockInternalsAccessors(prompt, encoding_helpers, accessors)\n",
    "\n",
    "    sim_strings_comp, distances_comp = exps[compare_to_slen].strings_with_topk_closest_proj_outputs(\n",
    "        block_idx=block_idx,\n",
    "        t_i=-1,\n",
    "        queries=prompt_accessors.proj_output(block_idx=block_idx)[:, -1, :],\n",
    "        k=10,\n",
    "        largest=False,\n",
    "    )\n",
    "\n",
    "    sim_strings, distances = exps[len(prompt)].strings_with_topk_closest_proj_outputs(\n",
    "        block_idx=block_idx,\n",
    "        t_i=-1,\n",
    "        queries=prompt_accessors.proj_output(block_idx=block_idx)[:, -1, :],\n",
    "        k=10,\n",
    "        largest=False,\n",
    "    )\n",
    "\n",
    "    print(f\"Length {compare_to_slen} similars:   Length {len(prompt)} similars:\")\n",
    "    for i in range(10):\n",
    "        print(f'{repr(sim_strings_comp[0][i])} {distances_comp[i].item():.3f}        {repr(sim_strings[0][i])} {distances[i].item():.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this a bunch of times, my conclusions from this are:\n",
    "\n",
    "* Yes it's possible to find similar strings with clear patterns in shorter strings\n",
    "* The distances are greater when the strings are shorter\n",
    "* But seeing which shorter strings are similar is illuminating\n",
    "\n",
    "e.g. for block_idx = 1, looking at similar strings of length 5 vs same length as the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 5 similars:   Length 10 similars:\n",
      "'st go' 4.564        'my most gr' 0.000\n",
      "'ot gl' 4.591        'ur most gr' 0.949\n",
      "'st ga' 4.595        'ne most gr' 0.958\n",
      "'ot ga' 4.609        'he most gr' 1.053\n",
      "'st gl' 4.629        'is most gr' 1.056\n",
      "'ot gr' 4.638        'e, most gr' 1.268\n",
      "'st gr' 4.639        'o, must gr' 1.352\n",
      "'rt go' 4.647        't, most gr' 1.361\n",
      "'st gi' 4.662        'be past gr' 1.372\n",
      "'et go' 4.679        'yet not gr' 1.506\n"
     ]
    }
   ],
   "source": [
    "prompt = 'my most gr'\n",
    "compare_similar_strings(prompt, block_idx=1, compare_to_slen=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the length 5 similar strings have `s t`` in common. \n",
    "\n",
    "Now look at block 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 5 similars:   Length 10 similars:\n",
      "'my mo' 4.464        'my most gr' 0.000\n",
      "'my st' 4.917        'my most st' 3.911\n",
      "'my br' 4.969        'my most sa' 4.328\n",
      "'my tr' 4.977        ' my most r' 4.556\n",
      "'my gr' 5.039        ' my most l' 5.160\n",
      "'my sw' 5.115        'my high bl' 5.198\n",
      "'my sc' 5.132        'm thy moth' 5.221\n",
      "'my wr' 5.140        'mt my mast' 5.225\n",
      "'my bl' 5.215        'my most he' 5.338\n",
      "'my tw' 5.267        'my merry m' 5.445\n"
     ]
    }
   ],
   "source": [
    "prompt = 'my most gr'\n",
    "compare_similar_strings(prompt, block_idx=5, compare_to_slen=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the common pattern in the length 5 strings is `my `."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this says something. The closest length 5 strings could have been any substring of the full prompt. Seeing what gets picked must be meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with length 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 3 similars:   Length 10 similars:\n",
      "' gn' 6.114        'my most gr' 0.000\n",
      "' gy' 6.142        'ur most gr' 0.949\n",
      "' gl' 6.156        'ne most gr' 0.958\n",
      "' gr' 6.166        'he most gr' 1.053\n",
      "' gu' 6.202        'is most gr' 1.056\n",
      "' gh' 6.218        'e, most gr' 1.268\n",
      "' go' 6.231        'o, must gr' 1.352\n",
      "' ga' 6.232        't, most gr' 1.361\n",
      "' ge' 6.267        'be past gr' 1.372\n",
      "' gi' 6.397        'yet not gr' 1.506\n"
     ]
    }
   ],
   "source": [
    "prompt = 'my most gr'\n",
    "compare_similar_strings(prompt, block_idx=1, compare_to_slen=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greater distance but still, a pattern. And block 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 3 similars:   Length 10 similars:\n",
      "'mys' 6.682        'my most gr' 0.000\n",
      "'my-' 6.722        'my most st' 3.911\n",
      "'ms-' 7.015        'my most sa' 4.328\n",
      "'myr' 7.065        ' my most r' 4.556\n",
      "'mso' 7.115        ' my most l' 5.160\n",
      "\"my'\" 7.123        'my high bl' 5.198\n",
      "'my?' 7.148        'm thy moth' 5.221\n",
      "'mfu' 7.158        'mt my mast' 5.225\n",
      "'moc' 7.168        'my most he' 5.338\n",
      "\"ms'\" 7.214        'my merry m' 5.445\n"
     ]
    }
   ],
   "source": [
    "prompt = 'my most gr'\n",
    "compare_similar_strings(prompt, block_idx=5, compare_to_slen=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different pattern, but still a pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can do this with just the length 10 data and not have to generate all the block internals data for the other string lengths from scratch. \n",
    "\n",
    "As a prereq, let's first see if the intermediate values for substrings within a longer string are the same as the values that would have been produced for those substrings on their own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that for the common letters, the intermediates for a substring\n",
    "# are the same as those in a longer string.\n",
    "prompt_short = 'my mo'\n",
    "prompt_long = 'my most gr'\n",
    "\n",
    "bia_short = BlockInternalsAccessors(prompt_short, encoding_helpers, accessors)\n",
    "bia_long = BlockInternalsAccessors(prompt_long, encoding_helpers, accessors)\n",
    "\n",
    "for t_i in range(len(prompt_short)):\n",
    "    for block_idx in range(n_layer):\n",
    "        test_close(\n",
    "            bia_short.proj_output(block_idx=block_idx)[0, t_i, :],\n",
    "            bia_long.proj_output(block_idx=block_idx)[0, t_i, :],\n",
    "        )\n",
    "        test_close(\n",
    "            bia_short.ffwd_output(block_idx=block_idx)[0, t_i, :],\n",
    "            bia_long.ffwd_output(block_idx=block_idx)[0, t_i, :],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It passes, so this shows we can use the values at the other t_i's from the slen10 dataset. Let's try it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_slen=10\n",
    "target_exp = exps[full_slen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_similar_strings2(\n",
    "    prompt: str,\n",
    "    block_idx: int,\n",
    "    compare_to_slen: int,\n",
    "):\n",
    "    \"\"\"Same as compare_similar_strings() above but just uses a single experiment.\"\"\"\n",
    "    prompt_accessors = BlockInternalsAccessors(prompt, encoding_helpers, accessors)\n",
    "\n",
    "    # indexing `compare_to_slen - 1` below because slicers2 is indexed\n",
    "    # by t_i, not string length\n",
    "    sim_strings_comp, distances_comp = target_exp.strings_with_topk_closest_proj_outputs(\n",
    "        block_idx=block_idx,\n",
    "        t_i=compare_to_slen - 1,\n",
    "        queries=prompt_accessors.proj_output(block_idx=block_idx)[:, -1, :],\n",
    "        k=10,\n",
    "        largest=False,\n",
    "    )\n",
    "\n",
    "    sim_strings, distances = exps[len(prompt)].strings_with_topk_closest_proj_outputs(\n",
    "        block_idx=block_idx,\n",
    "        t_i=-1,\n",
    "        queries=prompt_accessors.proj_output(block_idx=block_idx)[:, -1, :],\n",
    "        k=10,\n",
    "        largest=False,\n",
    "    )\n",
    "\n",
    "    print(f\"Length {compare_to_slen} similars:   Length {len(prompt)} similars:\")\n",
    "    for i in range(10):\n",
    "        print(f'{repr(sim_strings_comp[0][i][:compare_to_slen])} {distances_comp[i].item():.3f}        {repr(sim_strings[0][i])} {distances[i].item():.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 5 similars:   Length 10 similars:\n",
      "'my mo' 4.464        'my most gr' 0.000\n",
      "'my st' 4.917        'my most st' 3.911\n",
      "'my br' 4.969        'my most sa' 4.328\n",
      "'my tr' 4.977        ' my most r' 4.556\n",
      "'my gr' 5.039        ' my most l' 5.160\n",
      "'my sw' 5.115        'my high bl' 5.198\n",
      "'my sc' 5.132        'm thy moth' 5.221\n",
      "'my wr' 5.140        'mt my mast' 5.225\n",
      "'my bl' 5.215        'my most he' 5.338\n",
      "'my tw' 5.267        'my merry m' 5.445\n"
     ]
    }
   ],
   "source": [
    "prompt = 'my most gr'\n",
    "compare_similar_strings2(prompt, block_idx=5, compare_to_slen=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this is the same as the output above when we compared to a length 5 experiment's outputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can do it for other lengths without having to run experiments for all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 9 similars:   Length 10 similars:\n",
      "'my most r' 2.754        'my most gr' 0.000\n",
      "'my most l' 3.517        'my most st' 3.911\n",
      "'my most h' 3.795        'my most sa' 4.328\n",
      "'m my mout' 4.802        ' my most r' 4.556\n",
      "'m thy mot' 4.808        ' my most l' 5.160\n",
      "'m, my mot' 4.832        'my high bl' 5.198\n",
      "'y most gr' 4.976        'm thy moth' 5.221\n",
      "'my most g' 5.051        'mt my mast' 5.225\n",
      "'mes my br' 5.177        'my most he' 5.338\n",
      "'m my moth' 5.252        'my merry m' 5.445\n"
     ]
    }
   ],
   "source": [
    "prompt = 'my most gr'\n",
    "compare_similar_strings2(prompt, block_idx=5, compare_to_slen=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out loading with mmap_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Code in this section doesn't run anymore because the Slicer doesn't exist and some other internal changes have been made based on the experiments here, but I'm leaving this in for the historical record. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'my most gr'\n",
    "prompt_accessors = BlockInternalsAccessors(prompt, encoding_helpers, accessors)\n",
    "query = prompt_accessors.proj_output(block_idx=0)[:, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time loading a full batch from slen10 the regular way and subtracting the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.4 ms  348 s per loop (mean  std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch = torch.load(exps[10].output_dir / 'proj_output-000-00.pt')\n",
    "batch - query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load with `mmap=True` and try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8 ms  56.3 s per loop (mean  std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch = torch.load(str(exps[10].output_dir / 'proj_output-000-00.pt'), mmap=True)\n",
    "batch - query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooh, it's way faster. Let's see if we can load multiple batches, cat them and do the subtraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.1 ms  637 s per loop (mean  std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch1 = torch.load(str(exps[10].output_dir / 'proj_output-000-00.pt'), mmap=True)\n",
    "batch2 = torch.load(str(exps[10].output_dir / 'proj_output-001-00.pt'), mmap=True)\n",
    "batch3 = torch.load(str(exps[10].output_dir / 'proj_output-002-00.pt'), mmap=True)\n",
    "\n",
    "big_batch = torch.cat([batch1, batch2, batch3])\n",
    "big_batch - query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. OK, this is a big deal. We did 3 batches in 51ms vs 28.ms for just one batch in the regular way. And I suspect this scales non-linearly. Let's try it with all the batches:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.88 s  294 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "big_batch = torch.cat([\n",
    "    torch.load(str(exps[10]._proj_output_filename(batch_idx=batch_idx, block_idx=0)), mmap=True)\n",
    "    for batch_idx in range(exps[10].n_batches)\n",
    "])\n",
    "big_batch - query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 8.88s for 86 batches. Memory usage peaked at 25GB during the run but went up and down and settled back down to where it was before the run started. \n",
    "\n",
    "But that's 103ms per batch which seems slower than just loading each batch one at a time. But the computation is simpler (no need for `topk_across_batches()` etc). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try running through all the batches the old way (no mmap) and time it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.32 s  157 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for batch_idx in range(exps[10].n_batches):\n",
    "    batch = torch.load(str(exps[10]._proj_output_filename(batch_idx=batch_idx, block_idx=0))) # no mmap\n",
    "    batch - query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is actually faster. But let's try to do more of the complete operation and do multiple queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts and extract the query values\n",
    "prompts = ['First Citi', 'Citizen:\\nB', 'Shyamalan ', 'more in jo']\n",
    "prompts_exp = BlockInternalsExperiment(encoding_helpers, accessors, prompts)\n",
    "\n",
    "t_i = -1\n",
    "\n",
    "queries = prompts_exp.proj_output(block_idx=0)[:, t_i, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time doing the equivalent of strings_with_topk_closest_ffwd_outputs() on the mmaped data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.51 s  395 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "big_batch = torch.cat([\n",
    "    torch.load(str(exps[10]._proj_output_filename(batch_idx=batch_idx, block_idx=0)), mmap=True)\n",
    "    for batch_idx in range(exps[10].n_batches)\n",
    "])\n",
    "\n",
    "n_queries, _ = queries.shape\n",
    "B, T, _ = big_batch.shape\n",
    "distances = torch.norm(\n",
    "    big_batch[:, t_i, :].reshape(B, 1, -1).expand(-1, n_queries, -1) - queries,\n",
    "    dim=2\n",
    ")\n",
    "topk = torch.topk(distances, k=10, dim=0, largest=False)\n",
    "exps[10].strings_from_indices(topk.indices), topk.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that is slow. But I suspect there is some overhead in loading the data the first time. What if we load the data once and then process the queries on the loaded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_batch = torch.cat([\n",
    "    torch.load(str(exps[10]._proj_output_filename(batch_idx=batch_idx, block_idx=0)), mmap=True)\n",
    "    for batch_idx in range(exps[10].n_batches)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 ms  8.63 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "n_queries, _ = queries.shape\n",
    "B, T, _ = big_batch.shape\n",
    "distances = torch.norm(\n",
    "    big_batch[:, t_i, :].reshape(B, 1, -1).expand(-1, n_queries, -1) - queries,\n",
    "    dim=2\n",
    ")\n",
    "topk = torch.topk(distances, k=10, dim=0, largest=False)\n",
    "exps[10].strings_from_indices(topk.indices), topk.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare that to doing it with a slicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471 ms  3.27 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "slicers[10].strings_with_topk_closest_proj_outputs(block_idx=0, queries=queries, k=10, largest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have the required methods on the exp class anymore, but if we want to test how long this would have taken without the slicer i.e. on the same data that the mmap version is using but without using mmap, we can resurrect the relevant code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of the code from block_internals, just so we can run it below\n",
    "def batch_distances(batch: torch.Tensor, queries: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Returns the distance between each item in the batch and the queries.\"\"\"\n",
    "    assert batch.dim() == 2, f\"batch.dim() should be 2, was {batch.dim()}\"\n",
    "    assert queries.dim() == 2, f\"query.dim() should be 2, was {queries.dim()}\"\n",
    "    assert (\n",
    "        batch.shape[-1] == queries.shape[-1]\n",
    "    ), f\"last dimension of batch was {batch.shape[-1]}, which does not match last dimension of queries {queries.shape[-1]}\"\n",
    "\n",
    "    B, _ = batch.shape\n",
    "    n_queries, _ = queries.shape\n",
    "\n",
    "    distances = torch.norm(\n",
    "        # Reshape the batch to a singleton dimension, then expand that dimension\n",
    "        # by the number of queries. We can then subtract all the queries in one\n",
    "        # go.\n",
    "        batch.reshape(B, 1, -1).expand(-1, n_queries, -1) - queries,\n",
    "        dim=2\n",
    "    )\n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18 s  29.7 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "n_queries, _ = queries.shape\n",
    "values, indices = topk_across_batches(\n",
    "    n_batches=exps[10].n_batches,\n",
    "    k=10,\n",
    "    largest=False,\n",
    "    load_batch=lambda i: torch.load(exps[10]._proj_output_filename(i, block_idx=0))[:, t_i, :],\n",
    "    process_batch=lambda batch: batch_distances(batch, queries=queries),\n",
    ")\n",
    "exps[10].strings_from_indices(indices), values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mmap way is a faster than the slicer (303ms vs 471ms) and doesn't require materializing the slices. And it's waaaay faster than doing it without the slicer and without mmap (303ms vs 2.18s).\n",
    "\n",
    "In summary: \n",
    "It seems there is a one-time cost to loading all the batches via: \n",
    "\n",
    "```python\n",
    "big_batch = torch.cat([\n",
    "    torch.load(str(exps[10]._proj_output_filename(batch_idx=batch_idx, block_idx=0)), mmap=True)\n",
    "    for batch_idx in range(exps[10].n_batches)\n",
    "])\n",
    "```\n",
    "\n",
    "But this doesn't take up too much memory (fresh Jupyter kernel running just the stuff in this and the previous section has about 13 GB of memory per Activity Monitor). So we can load this once and run a lot of queries very fast. \n",
    "\n",
    "Let's check that the results are correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do for real so we can compare the results\n",
    "big_batch = torch.cat([\n",
    "    torch.load(str(exps[10]._proj_output_filename(batch_idx=batch_idx, block_idx=0)), mmap=True)\n",
    "    for batch_idx in range(exps[10].n_batches)\n",
    "])\n",
    "\n",
    "n_queries, _ = queries.shape\n",
    "B, T, _ = big_batch.shape\n",
    "distances = torch.norm(\n",
    "    big_batch[:, t_i, :].reshape(B, 1, -1).expand(-1, n_queries, -1) - queries,\n",
    "    dim=2\n",
    ")\n",
    "topk = torch.topk(distances, k=10, dim=0, largest=False)\n",
    "exps[10].strings_from_indices(topk.indices), topk.values\n",
    "\n",
    "sim_strings, distances = slicers[10].strings_with_topk_closest_proj_outputs(block_idx=0, queries=queries, k=10, largest=False)\n",
    "test_eq(exps[10].strings_from_indices(topk.indices), sim_strings)\n",
    "test_close(topk.values, distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These test pass, so the output is the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perf tests for using mmap with the slicer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis above showed that using mmap on the raw batch data is faster than using the slicer. But yesterday I found that just setting mmap=True on the load_batch function when finding closest embeddings made a huge difference. Let's try the same thing for the slicer and see if it makes a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_i = -1\n",
    "queries = prompts_exp.proj_output(block_idx=0)[:, t_i, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we have a measurement for using the slicer above, let's just replicate it for completeness. Ran this line before making any changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482 ms  14.6 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "slicers[10].strings_with_topk_closest_proj_outputs(block_idx=0, queries=queries, k=10, largest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that's in line with the measurement above. Now let's try it after changing the implementation to use `mmap=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361 ms  8.18 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "slicers[10].strings_with_topk_closest_proj_outputs(block_idx=0, queries=queries, k=10, largest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so it got faster, but it's still not as fast as using the raw batch data with `mmap=True`. So we'll go with that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perf tests for finding closest embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp10 = BatchedBlockInternalsExperiment(\n",
    "    eh=encoding_helpers,\n",
    "    accessors=accessors,\n",
    "    strings=strings_n[10],\n",
    "    output_dir=Path(f'../artifacts/block_internals_results/large_files/slen10/'),\n",
    "    batch_size=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measurement before any changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.49 s  9.11 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "exp10.strings_with_topk_closest_embeddings(queries=prompts_exp.embeddings, k=10, largest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate loading all the batch data at once and then finding the topk closest strings for all the queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_data = torch.cat([\n",
    "    torch.load(str(exp10._embeddings_filename(batch_idx=batch_idx)), mmap=True)\n",
    "    for batch_idx in range(exp10.n_batches)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent of topk_closest_embeddings - slow\n",
    "B, _, _ = embeddings_data.shape\n",
    "\n",
    "n_queries, _, _ = prompts_exp.embeddings.shape\n",
    "\n",
    "distances = batch_distances(\n",
    "    embeddings_data.reshape(B, -1),\n",
    "    prompts_exp.embeddings.reshape(n_queries, -1)\n",
    ")\n",
    "topk = torch.topk(distances, dim=0, k=k, largest=False)\n",
    "exp10.strings_from_indices(topk.indices), topk.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above was really slow and took a ton of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the equivalent thing on the proj_out data is still fast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_out_data = torch.cat([\n",
    "    torch.load(str(exp10._proj_output_filename(batch_idx=batch_idx, block_idx=0)), mmap=True)\n",
    "    for batch_idx in range(exp10.n_batches)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_queries, _ = queries.shape\n",
    "B, T, _ = proj_out_data.shape\n",
    "distances = torch.norm(\n",
    "    proj_out_data[:, t_i, :].reshape(B, 1, -1).expand(-1, n_queries, -1) - queries,\n",
    "    dim=2\n",
    ")\n",
    "topk = torch.topk(distances, k=10, dim=0, largest=False)\n",
    "exp10.strings_from_indices(topk.indices), topk.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes it is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suspecting the issue is the reshape needed. Show that we can calculate the norm we want without the reshape. \n",
    "\n",
    "What the reshape does is stack the embeddings across the T dimension. We have embeddings\n",
    "\n",
    "$$\n",
    "e_1, e_2, \\ldots, e_T \\in \\mathbb{R}^{n\\_embed}\n",
    "$$\n",
    "\n",
    "By stacking them, we get one big embedding:\n",
    "\n",
    "$$\n",
    "e_{1:T} \\in \\mathbb{R}^{T * n\\_embed}\n",
    "$$\n",
    "\n",
    "We do the same with the queries:\n",
    "\n",
    "$$\n",
    "q_1, q_2, \\ldots, q_T \\in \\mathbb{R}^{n\\_embed} \\rightarrow\n",
    "q_{1:T} \\in \\mathbb{R}^{T * n\\_embed}\n",
    "$$\n",
    "\n",
    "We then want to compute\n",
    "\n",
    "$$\n",
    "\\Vert e_{1:T} - q_{1:T} \\Vert_2 = \\sqrt{\\sum_{i=1}^{T * n\\_embed} (e_{1:T} - q_{1:T})_i^2}\n",
    "$$\n",
    "\n",
    "Can we get to this if we only have $e_1, e_2, \\ldots, e_T$ and $q_1, q_2, \\ldots, q_T$? Yes, we can.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Vert e_{1:T} - q_{1:T} \\Vert_2 &= \\sqrt{\\sum_{i=1}^{T * n\\_embed} (e_{1:T} - q_{1:T})_i^2} \\\\\n",
    "\\Vert e_{1:T} - q_{1:T} \\Vert_2^2 &= \\sum_{i=1}^{T * n\\_embed} (e_{1:T} - q_{1:T})_i^2 \\\\\n",
    "&=\\sum_{i=1}^{n\\_embed}(e_{1:T} - q_{1:T})_i^2 + \\sum_{i=n\\_embed+1}^{2*n\\_embed}(e_{1:T} - q_{1:T})_i^2 + \\ldots + \\sum_{i=(T-1)*n\\_embed+1}^{T*n\\_embed}(e_{1:T} - q_{1:T})_i^2 \\\\\n",
    "&=\\sum_{i=1}^{n\\_embed}(e_{1} - q_{1})_i^2 + \\sum_{i=1}^{n\\_embed}(e_{2} - q_{2})_i^2 + \\ldots + \\sum_{i=1}^{n\\_embed}(e_{T} - q_{T})_i^2 \\\\\n",
    "&=\\Vert e_1 - q_1 \\Vert_2^2 + \\Vert e_2 - q_2 \\Vert_2^2 + \\ldots + \\Vert e_T - q_T \\Vert_2^2 \\\\\n",
    "&=\\sum_{i=1}^{T}\\Vert e_i - q_i \\Vert_2^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Vert e_{1:T} - q_{1:T} \\Vert_2 &= \\sqrt{\\sum_{i=1}^{T}\\Vert e_i - q_i \\Vert_2^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let's check it in code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that we can effectively compute the norm without reshaping\n",
    "x = torch.randn((100, 5, 384))\n",
    "B, T, _ = x.shape\n",
    "q = torch.randn(5, 384)\n",
    "\n",
    "norm1 = torch.norm(x.reshape(B, -1) - q.reshape(-1), dim=-1)\n",
    "norm2 = (torch.norm(x - q, dim=-1) ** 2).sum(dim=-1).sqrt()\n",
    "test_close(norm1, norm2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do it with the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Version without reshaping\n",
    "B, _, _ = embeddings_data.shape\n",
    "\n",
    "n_queries, _, _ = prompts_exp.embeddings.shape\n",
    "\n",
    "distances = (\n",
    "    torch.norm(embeddings_data.unsqueeze(dim=1).expand(-1, n_queries, -1, -1) - prompts_exp.embeddings, dim=-1) ** 2\n",
    ").sum(dim=-1).sqrt()\n",
    "\n",
    "topk = torch.topk(distances, dim=0, k=10, largest=False)\n",
    "exp10.strings_from_indices(topk.indices), topk.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above used so much memory that it crashed the kernel. So this is a no go. I think the fundamental problem is we're computing over a lot more data: all elements of the T dimension vs just one with the proj_outputs/ffwd_outputs. So let's go back to the original way of doing it in batches, but let's see if it helps to load \"super batches\" by combining several of the batches on disk into one batch in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for creating super batches\n",
    "\n",
    "k=10\n",
    "largest=False\n",
    "combine_n_batches = 5\n",
    "\n",
    "batch_size = exp10.batch_size * combine_n_batches\n",
    "n_batches = math.ceil(len(exp10.strings) / batch_size)\n",
    "queries = prompts_exp.embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.19 s  35.9 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "def _load_batch(batch_idx: int):\n",
    "    start = batch_idx * combine_n_batches\n",
    "    end = min((batch_idx + 1) * combine_n_batches, exp10.n_batches)\n",
    "\n",
    "    batch = torch.cat([\n",
    "        torch.load(\n",
    "            str(exp10._embeddings_filename(batch_idx=i)),\n",
    "            mmap=True,\n",
    "        )\n",
    "        for i in range(start, end)\n",
    "    ])\n",
    "    return batch\n",
    "\n",
    "n_queries, _, _ = queries.shape\n",
    "\n",
    "def _process_batch(batch: torch.Tensor) -> torch.Tensor:\n",
    "    B, _, _ = batch.shape\n",
    "    # Batch and queries and both shape (B, s_len, n_embed).\n",
    "    # For the purposes of finding the closest values, we\n",
    "    # reshape both the batch and queries to eliminate the\n",
    "    # s_len dimension, effectively concatenating all the\n",
    "    # embedding tensors across positions.\n",
    "    return batch_distances(batch.reshape(B, -1), queries.reshape(n_queries, -1))\n",
    "\n",
    "values, indices = topk_across_batches(\n",
    "    n_batches=n_batches,\n",
    "    k=k,\n",
    "    largest=largest,\n",
    "    load_batch=_load_batch,\n",
    "    process_batch=_process_batch,\n",
    ")\n",
    "\n",
    "exp10.strings_from_indices(indices), values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helps only a tiny amount. And it seems to get faster the fewer number of batches we combine. So let's just do it with one batch at a time. I added code to load the one batch in the existing implementation with mmap=True and timed it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.34 s  46.2 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "exp10.strings_with_topk_closest_embeddings(queries=prompts_exp.embeddings, k=10, largest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best result so far so we'll go with this. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
