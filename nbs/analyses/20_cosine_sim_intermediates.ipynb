{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of Cosine Similarity of Block Intermediates\n",
    "\n",
    "> Thus far, all of the similarity investigations have been based on Euclidean distance. In this notebook, we look at whether cosine similarity might be a better measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Optional, Iterable, Protocol, Sequence, Tuple, TypeVar, Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "from matplotlib.axes import Axes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "from transformer_experiments.common.substring_generator import all_unique_substrings\n",
    "from transformer_experiments.common.text_analysis import (\n",
    "    build_next_token_map,\n",
    "    SubstringFrequencyAnalysis,\n",
    "    top_nonzero_tokens\n",
    ")\n",
    "from transformer_experiments.common.utils import (\n",
    "    aggregate_by_string_key,\n",
    "    DataWrapper,\n",
    "    topk_across_batches,\n",
    ")\n",
    "from transformer_experiments.dataset_split import split_text_dataset\n",
    "from transformer_experiments.datasets.tinyshakespeare import (\n",
    "    TinyShakespeareDataSet,\n",
    ")\n",
    "from transformer_experiments.models.transformer import (\n",
    "    n_layer,\n",
    "    TransformerLanguageModel\n",
    ")\n",
    "from transformer_experiments.models.transformer_helpers import (\n",
    "    unsqueeze_emb,\n",
    "    EncodingHelpers,\n",
    "    LogitsWrapper,\n",
    "    TransformerAccessors\n",
    ")\n",
    "from transformer_experiments.trained_models.tinyshakespeare_transformer import (\n",
    "    create_model_and_tokenizer\n",
    ")\n",
    "from transformer_experiments.experiments.block_internals import (\n",
    "    BlockInternalsAccessors,\n",
    "    BlockInternalsExperiment,\n",
    "    BatchedBlockInternalsExperiment,\n",
    "    BlockInternalsAnalysis,\n",
    "    batch_cosine_sim,\n",
    ")\n",
    "from transformer_experiments.experiments.similar_strings import (\n",
    "    SimilarStringsData,\n",
    "    SimilarStringsExperiment,\n",
    "    SimilarStringsResult\n",
    ")\n",
    "from transformer_experiments.experiments.logit_lens import LogitLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ts = TinyShakespeareDataSet(cache_file='../artifacts/input.txt')\n",
    "m, tokenizer = create_model_and_tokenizer(\n",
    "    saved_model_filename='../artifacts/shakespeare.pt',\n",
    "    dataset=ts,\n",
    "    device=device,\n",
    ")\n",
    "_, val_data = split_text_dataset(ts.text, tokenizer, train_pct=0.9)\n",
    "encoding_helpers = EncodingHelpers(tokenizer, device)\n",
    "accessors = TransformerAccessors(m, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"device is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if list(Path('../artifacts/block_internals_results/large_files/slen10/').glob('*')) == []:\n",
    "    print(\"Run `make block_internals_slen10_dataset` in the project root to generate the required dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings10 = all_unique_substrings(ts.text, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp10 = BatchedBlockInternalsExperiment(\n",
    "    eh=encoding_helpers,\n",
    "    accessors=accessors,\n",
    "    strings=strings10,\n",
    "    output_dir=Path('../artifacts/block_internals_results/large_files/slen10/'),\n",
    "    batch_size=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, investigate whether there is a lot of variance in the norms of the block intermediates. If so, it suggests that cosine similarity may be a better measure than Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: mean 27.912761688232422, std 1.1137561798095703\n",
      "Layer 1: mean 33.36977767944336, std 1.7041479349136353\n",
      "Layer 2: mean 39.782466888427734, std 2.0023622512817383\n",
      "Layer 3: mean 46.48314666748047, std 3.300010919570923\n",
      "Layer 4: mean 53.44303894042969, std 6.607938289642334\n",
      "Layer 5: mean 61.70024871826172, std 11.696634292602539\n"
     ]
    }
   ],
   "source": [
    "for block_idx in range(n_layer):\n",
    "    proj_out_batch = torch.load(str(exp10._block_output_filename(batch_idx=0, block_idx=block_idx)), mmap=True)\n",
    "    proj_out_norms = torch.norm(proj_out_batch[:, -1, :], dim=-1)\n",
    "    print(f\"Layer {block_idx}: mean {proj_out_norms.mean()}, std {proj_out_norms.std()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: mean 6.409949779510498, std 1.142516851425171\n",
      "Layer 1: mean 8.440470695495605, std 0.9452682137489319\n",
      "Layer 2: mean 9.34270191192627, std 1.0641635656356812\n",
      "Layer 3: mean 11.903395652770996, std 1.5840272903442383\n",
      "Layer 4: mean 13.59791374206543, std 2.9059391021728516\n",
      "Layer 5: mean 19.13654136657715, std 5.285085201263428\n"
     ]
    }
   ],
   "source": [
    "for block_idx in range(n_layer):\n",
    "    ffwd_out_batch = torch.load(str(exp10._ffwd_output_filename(batch_idx=0, block_idx=block_idx)), mmap=True)\n",
    "    ffwd_out_norms = torch.norm(ffwd_out_batch[:, -1, :], dim=-1)\n",
    "    print(f\"Layer {block_idx}: mean {ffwd_out_norms.mean()}, std {ffwd_out_norms.std()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so for both proj_out and ffwd_out, norm goes up in the later layers and so does std dev. So, cosine similarity is probably a better measure than Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['my most gr', 'is dreams,']\n",
    "prompts_exp = BlockInternalsExperiment(encoding_helpers, accessors, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = prompts_exp.proj_output(block_idx=0)[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mock up of what a cosine similarity function would look like\n",
    "batch = proj_out_batch[:, -1, :]\n",
    "B, _ = batch.shape\n",
    "n_queries, _ = queries.shape\n",
    "sims = F.cosine_similarity(batch.reshape(B, 1, -1).expand(-1, n_queries, -1), queries, dim=-1)\n",
    "sims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest to 'my most gr': \n",
      "\t  'my most gr' (1.000)\n",
      "\t  'ur most gr' (0.995)\n",
      "\t  'is most gr' (0.995)\n",
      "\t  'ne most gr' (0.995)\n",
      "\t  'ilst my gr' (0.995)\n",
      "\t  'he most gr' (0.994)\n",
      "\t  'unto my gr' (0.994)\n",
      "\t  'e, most gr' (0.994)\n",
      "\t  't, most gr' (0.994)\n",
      "\t  'yman to gr' (0.993)\n",
      "\n",
      "Closest to 'is dreams,': \n",
      "\t  'is dreams,' (1.000)\n",
      "\t  'ly dreams,' (0.995)\n",
      "\t  'en dreams,' (0.994)\n",
      "\t  'he dreams,' (0.994)\n",
      "\t  'ur dreams,' (0.994)\n",
      "\t  'nd dreams,' (0.993)\n",
      "\t  'ery beams,' (0.992)\n",
      "\t  'of dreams,' (0.991)\n",
      "\t  \"n's beams,\" (0.990)\n",
      "\t  'hese arms,' (0.989)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "block_idx = 0\n",
    "sims, distances = exp10.strings_with_topk_closest_proj_outputs(\n",
    "    block_idx=block_idx,\n",
    "    t_i = -1,\n",
    "    queries=prompts_exp.proj_output(block_idx=block_idx)[:, -1, :],\n",
    "    k=10,\n",
    "    largest=True,\n",
    "    distance_function=batch_cosine_sim,\n",
    ")\n",
    "\n",
    "for idx, strings in enumerate(sims):\n",
    "    print(f\"Closest to {repr(prompts[idx])}: \")\n",
    "\n",
    "    for i, s in enumerate(strings):\n",
    "        print(f\"\\t{repr(s):>14} ({distances[i][idx]:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest to 'my most gr': \n",
      "\t  'at unsubst' (0.454)\n",
      "\t  't in subst' (0.454)\n",
      "\t  'ften burst' (0.454)\n",
      "\t  'd by subst' (0.454)\n",
      "\t  'n of subst' (0.454)\n",
      "\t  'most burst' (0.453)\n",
      "\t  'it unconst' (0.453)\n",
      "\t  'l, inconst' (0.453)\n",
      "\t  'o be subst' (0.453)\n",
      "\t  're unconst' (0.453)\n",
      "\n",
      "Closest to 'is dreams,': \n",
      "\t  'Schoolmast' (0.473)\n",
      "\t  'schoolmast' (0.472)\n",
      "\t  'hath chast' (0.471)\n",
      "\t  'Stand fast' (0.470)\n",
      "\t  ' notwithst' (0.470)\n",
      "\t 'ng\\nfantast' (0.470)\n",
      "\t  'stand fast' (0.470)\n",
      "\t 'thou\\nhadst' (0.469)\n",
      "\t  'ough chast' (0.469)\n",
      "\t  'ch fantast' (0.469)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "block_idx = 0\n",
    "sims, distances = exp10.strings_with_topk_closest_ffwd_outputs(\n",
    "    block_idx=block_idx,\n",
    "    t_i = -1,\n",
    "    queries=prompts_exp.proj_output(block_idx=block_idx)[:, -1, :],\n",
    "    k=10,\n",
    "    largest=True,\n",
    "    distance_function=batch_cosine_sim,\n",
    ")\n",
    "for idx, strings in enumerate(sims):\n",
    "    print(f\"Closest to {repr(prompts[idx])}: \")\n",
    "\n",
    "    for i, s in enumerate(strings):\n",
    "        print(f\"\\t{repr(s):>14} ({distances[i][idx]:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest to 'my most gr': \n",
      "\t  'my most gr' (1.000)\n",
      "\t  'my most st' (0.897)\n",
      "\t  'my most sa' (0.864)\n",
      "\t  ' my most r' (0.849)\n",
      "\t  ' my most l' (0.809)\n",
      "\t  'my high bl' (0.799)\n",
      "\t  'mt my mast' (0.799)\n",
      "\t  'm thy moth' (0.795)\n",
      "\t  'm, my mour' (0.788)\n",
      "\t  'my most he' (0.784)\n",
      "\n",
      "Closest to 'is dreams,': \n",
      "\t  'is dreams,' (1.000)\n",
      "\t  'is hoarse,' (0.932)\n",
      "\t  'ish hairs,' (0.924)\n",
      "\t  'ith oaths,' (0.914)\n",
      "\t  'is events,' (0.912)\n",
      "\t  'ish tears,' (0.911)\n",
      "\t  'ir mouths,' (0.911)\n",
      "\t  'ir plumes,' (0.905)\n",
      "\t  'is throne,' (0.899)\n",
      "\t  'is mother,' (0.896)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "block_idx = 5\n",
    "sims, distances = exp10.strings_with_topk_closest_proj_outputs(\n",
    "    block_idx=block_idx,\n",
    "    t_i = -1,\n",
    "    queries=prompts_exp.proj_output(block_idx=block_idx)[:, -1, :],\n",
    "    k=10,\n",
    "    largest=True,\n",
    "    distance_function=batch_cosine_sim,\n",
    ")\n",
    "for idx, strings in enumerate(sims):\n",
    "    print(f\"Closest to {repr(prompts[idx])}: \")\n",
    "\n",
    "    for i, s in enumerate(strings):\n",
    "        print(f\"\\t{repr(s):>14} ({distances[i][idx]:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest to 'my most gr': \n",
      "\t  's fast bel' (0.347)\n",
      "\t  's part bel' (0.337)\n",
      "\t  's that bel' (0.334)\n",
      "\t  ' drops bel' (0.331)\n",
      "\t  'assage bel' (0.331)\n",
      "\t  'e step bel' (0.325)\n",
      "\t  'y best bel' (0.325)\n",
      "\t  'w then bel' (0.323)\n",
      "\t  'myself bel' (0.319)\n",
      "\t 'ts:\\nSometi' (0.317)\n",
      "\n",
      "Closest to 'is dreams,': \n",
      "\t  ' may she--' (0.402)\n",
      "\t  '! should--' (0.397)\n",
      "\t 'tantly,\\n--' (0.396)\n",
      "\t  ' it were--' (0.391)\n",
      "\t  's is she--' (0.390)\n",
      "\t 'LO:\\nAnd,--' (0.390)\n",
      "\t  \"ty in't,--\" (0.390)\n",
      "\t  'im--dead--' (0.390)\n",
      "\t  't are so--' (0.389)\n",
      "\t  \": here's--\" (0.389)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "block_idx = 5\n",
    "sims, distances = exp10.strings_with_topk_closest_ffwd_outputs(\n",
    "    block_idx=block_idx,\n",
    "    t_i = -1,\n",
    "    queries=prompts_exp.proj_output(block_idx=block_idx)[:, -1, :],\n",
    "    k=10,\n",
    "    largest=True,\n",
    "    distance_function=batch_cosine_sim,\n",
    ")\n",
    "for idx, strings in enumerate(sims):\n",
    "    print(f\"Closest to {repr(prompts[idx])}: \")\n",
    "\n",
    "    for i, s in enumerate(strings):\n",
    "        print(f\"\\t{repr(s):>14} ({distances[i][idx]:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest to 'my most gr': \n",
      "\t   'ssage bel' (0.346)\n",
      "\t   ' step bel' (0.339)\n",
      "\t   ' fast bel' (0.337)\n",
      "\t  'g\\ninto so' (0.322)\n",
      "\t   'drops bel' (0.320)\n",
      "\t   'grave bel' (0.320)\n",
      "\t  ' it\\nTo so' (0.317)\n",
      "\t   ' part bel' (0.311)\n",
      "\t   ' best bel' (0.310)\n",
      "\t   'place bel' (0.307)\n",
      "\n",
      "Closest to 'is dreams,': \n",
      "\t   ' should--' (0.400)\n",
      "\t   'who has--' (0.397)\n",
      "\t   'may she--' (0.395)\n",
      "\t   ' so mad--' (0.391)\n",
      "\t   'derates--' (0.387)\n",
      "\t  'antly,\\n--' (0.386)\n",
      "\t   ' is she--' (0.386)\n",
      "\t   'no soul--' (0.384)\n",
      "\t   ' camest--' (0.384)\n",
      "\t   'f these--' (0.384)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "block_idx = 5\n",
    "sims, distances = exp10.strings_with_topk_closest_ffwd_outputs(\n",
    "    block_idx=block_idx,\n",
    "    t_i = 8,\n",
    "    queries=prompts_exp.proj_output(block_idx=block_idx)[:, -1, :],\n",
    "    k=10,\n",
    "    largest=True,\n",
    "    distance_function=batch_cosine_sim,\n",
    ")\n",
    "for idx, strings in enumerate(sims):\n",
    "    print(f\"Closest to {repr(prompts[idx])}: \")\n",
    "\n",
    "    for i, s in enumerate(strings):\n",
    "        print(f\"\\t{repr(s):>14} ({distances[i][idx]:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it for embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest to 'my most gr': \n",
      "\t  'my most gr' (1.000)\n",
      "\t  'my most sa' (0.912)\n",
      "\t  't, most gr' (0.909)\n",
      "\t  'my most st' (0.909)\n",
      "\t  'my most so' (0.906)\n",
      "\t  'e, most gr' (0.905)\n",
      "\t  'my most re' (0.905)\n",
      "\t  'ur most gr' (0.905)\n",
      "\t  'my most he' (0.905)\n",
      "\t  'is most gr' (0.904)\n",
      "\n",
      "Closest to 'is dreams,': \n",
      "\t  'is dreams,' (1.000)\n",
      "\t  'is dream o' (0.906)\n",
      "\t  'ur dreams,' (0.906)\n",
      "\t  'of dreams,' (0.905)\n",
      "\t  'us dreams.' (0.904)\n",
      "\t  'he dreams,' (0.903)\n",
      "\t  'ly dreams,' (0.902)\n",
      "\t  'en dreams,' (0.902)\n",
      "\t  'nd dreams,' (0.896)\n",
      "\t 'as dream\\nS' (0.865)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sims, distances = exp10.strings_with_topk_closest_embeddings(\n",
    "    queries=prompts_exp.embeddings,\n",
    "    k=10,\n",
    "    largest=True,\n",
    "    distance_function=batch_cosine_sim,\n",
    ")\n",
    "for idx, strings in enumerate(sims):\n",
    "    print(f\"Closest to {repr(prompts[idx])}: \")\n",
    "\n",
    "    for i, s in enumerate(strings):\n",
    "        print(f\"\\t{repr(s):>14} ({distances[i][idx]:.3f})\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
