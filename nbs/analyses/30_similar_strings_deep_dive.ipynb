{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Dive into Similar Strings Progress through the Model\n",
    "\n",
    "> Strings like `my most gr` and `ur most gr` produce very similar block intermediates at the start of the model but diverge before the end. This notebook investigates what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Optional, Iterable, Protocol, Sequence, Tuple, TypeVar, Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "from matplotlib.axes import Axes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "from transformer_experiments.common.substring_generator import all_unique_substrings\n",
    "from transformer_experiments.common.text_analysis import (\n",
    "    build_next_token_map,\n",
    "    SubstringFrequencyAnalysis,\n",
    "    top_nonzero_tokens\n",
    ")\n",
    "from transformer_experiments.common.utils import (\n",
    "    aggregate_by_string_key,\n",
    "    DataWrapper,\n",
    "    topk_across_batches,\n",
    ")\n",
    "from transformer_experiments.dataset_split import split_text_dataset\n",
    "from transformer_experiments.datasets.tinyshakespeare import (\n",
    "    TinyShakespeareDataSet,\n",
    ")\n",
    "from transformer_experiments.models.transformer import (\n",
    "    n_layer,\n",
    "    TransformerLanguageModel\n",
    ")\n",
    "from transformer_experiments.models.transformer_helpers import (\n",
    "    unsqueeze_emb,\n",
    "    EncodingHelpers,\n",
    "    LogitsWrapper,\n",
    "    TransformerAccessors\n",
    ")\n",
    "from transformer_experiments.trained_models.tinyshakespeare_transformer import (\n",
    "    create_model_and_tokenizer\n",
    ")\n",
    "from transformer_experiments.experiments.block_internals import (\n",
    "    BlockInternalsAccessors,\n",
    "    BlockInternalsExperiment,\n",
    "    BatchedBlockInternalsExperiment,\n",
    "    BlockInternalsAnalysis,\n",
    "    batch_cosine_sim,\n",
    ")\n",
    "from transformer_experiments.experiments.similar_strings import (\n",
    "    SimilarStringsData,\n",
    "    SimilarStringsExperiment,\n",
    "    SimilarStringsResult\n",
    ")\n",
    "from transformer_experiments.experiments.logit_lens import LogitLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ts = TinyShakespeareDataSet(cache_file='../artifacts/input.txt')\n",
    "m, tokenizer = create_model_and_tokenizer(\n",
    "    saved_model_filename='../artifacts/shakespeare.pt',\n",
    "    dataset=ts,\n",
    "    device=device,\n",
    ")\n",
    "_, val_data = split_text_dataset(ts.text, tokenizer, train_pct=0.9)\n",
    "encoding_helpers = EncodingHelpers(tokenizer, device)\n",
    "accessors = TransformerAccessors(m, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"device is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if list(Path('../artifacts/block_internals_results/large_files/slen10/').glob('*')) == []:\n",
    "    print(\"Run `make block_internals_slen10_dataset` in the project root to generate the required dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings10 = all_unique_substrings(ts.text, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp10 = BatchedBlockInternalsExperiment(\n",
    "    eh=encoding_helpers,\n",
    "    accessors=accessors,\n",
    "    strings=strings10,\n",
    "    output_dir=Path('../artifacts/block_internals_results/large_files/slen10/'),\n",
    "    batch_size=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = 'my most gr'\n",
    "prompt2 = 'ur most gr'\n",
    "\n",
    "bia1 = BlockInternalsAccessors(prompt1, encoding_helpers, accessors)\n",
    "bia2 = BlockInternalsAccessors(prompt2, encoding_helpers, accessors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how the proj outputs for these diverge across the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0: distance 0.792, cosine sim 0.995\n",
      "Block 1: distance 0.949, cosine sim 0.997\n",
      "Block 2: distance 2.432, cosine sim 0.968\n",
      "Block 3: distance 3.950, cosine sim 0.932\n",
      "Block 4: distance 6.427, cosine sim 0.780\n",
      "Block 5: distance 5.856, cosine sim 0.740\n"
     ]
    }
   ],
   "source": [
    "for block_idx in range(n_layer):\n",
    "    proj_out1 = bia1.proj_output(block_idx)[0, -1, :]\n",
    "    proj_out2 = bia2.proj_output(block_idx)[0, -1, :]\n",
    "\n",
    "    print(f\"Block {block_idx}: distance {torch.norm(proj_out1-proj_out2):.3f}, cosine sim {F.cosine_similarity(proj_out1, proj_out2, dim=-1):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They get further apart and less cosine similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And same for ffwd output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0: distance 0.140, cosine sim 1.000\n",
      "Block 1: distance 0.443, cosine sim 0.999\n",
      "Block 2: distance 0.746, cosine sim 0.997\n",
      "Block 3: distance 1.767, cosine sim 0.991\n",
      "Block 4: distance 2.762, cosine sim 0.985\n",
      "Block 5: distance 3.446, cosine sim 0.995\n"
     ]
    }
   ],
   "source": [
    "for block_idx in range(n_layer):\n",
    "    ffwd_out1 = bia1.ffwd_output(block_idx)[0, -1, :]\n",
    "    ffwd_out2 = bia2.ffwd_output(block_idx)[0, -1, :]\n",
    "\n",
    "    print(f\"Block {block_idx}: distance {torch.norm(ffwd_out1-ffwd_out2):.3f}, cosine sim {F.cosine_similarity(ffwd_out1, ffwd_out2, dim=-1):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They do get further apart but cosine similarity stays very close to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the model predict for these? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('a', 0.4602494537830353),\n",
       "  ('e', 0.35252559185028076),\n",
       "  ('o', 0.09188850224018097),\n",
       "  ('i', 0.09030349552631378),\n",
       "  ('u', 0.004192721098661423),\n",
       "  ('y', 0.0007521358784288168),\n",
       "  ('r', 6.647213740507141e-05),\n",
       "  ('l', 3.957989065384027e-06),\n",
       "  ('v', 2.812936827467638e-06),\n",
       "  ('w', 2.738903503995971e-06)],\n",
       " [('a', 0.42030981183052063),\n",
       "  ('e', 0.3680994510650635),\n",
       "  ('o', 0.10530176013708115),\n",
       "  ('i', 0.1002618744969368),\n",
       "  ('u', 0.0053354003466665745),\n",
       "  ('y', 0.0005905701545998454),\n",
       "  ('r', 6.933557597221807e-05),\n",
       "  ('l', 6.059422503312817e-06),\n",
       "  ('w', 4.522385552263586e-06),\n",
       "  ('n', 2.6605287075653905e-06)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the predicted outputs for these two:\n",
    "tokens1 = encoding_helpers.tokenize_string(prompt1)\n",
    "logits1, _ = m(tokens1)\n",
    "logits1 = LogitsWrapper(logits1.detach(), tokenizer)\n",
    "\n",
    "tokens2 = encoding_helpers.tokenize_string(prompt2)\n",
    "logits2, _ = m(tokens2)\n",
    "logits2 = LogitsWrapper(logits2.detach(), tokenizer)\n",
    "\n",
    "logits1.topk_tokens(k=10)[0][-1], logits2.topk_tokens(k=10)[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are extremely close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7939), tensor(20.7882))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(logits1.probs() - logits2.probs()), torch.norm(logits1.logits - logits2.logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
