{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Widening the Space of Similar Values\n",
    "\n",
    "> A major finding was that the current approaches are considering values that are too similar. This notebook investigates ways to search a wider space.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Optional, Iterable, Protocol, Sequence, Tuple, TypeVar, Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "import math\n",
    "from matplotlib.axes import Axes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import tempfile\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "from transformer_experiments.common.substring_generator import all_unique_substrings\n",
    "from transformer_experiments.common.text_analysis import (\n",
    "    build_next_token_map,\n",
    "    SubstringFrequencyAnalysis,\n",
    "    top_nonzero_tokens\n",
    ")\n",
    "from transformer_experiments.common.utils import (\n",
    "    aggregate_by_string_key,\n",
    "    DataWrapper,\n",
    "    topk_across_batches,\n",
    ")\n",
    "from transformer_experiments.dataset_split import split_text_dataset\n",
    "from transformer_experiments.datasets.tinyshakespeare import (\n",
    "    TinyShakespeareDataSet,\n",
    ")\n",
    "from transformer_experiments.environments import get_environment\n",
    "from transformer_experiments.models.transformer import (\n",
    "    block_size,\n",
    "    n_embed,\n",
    "    n_layer,\n",
    "    TransformerLanguageModel\n",
    ")\n",
    "from transformer_experiments.models.transformer_helpers import (\n",
    "    unsqueeze_emb,\n",
    "    EncodingHelpers,\n",
    "    LogitsWrapper,\n",
    "    TransformerAccessors\n",
    ")\n",
    "from transformer_experiments.trained_models.tinyshakespeare_transformer import (\n",
    "    create_model_and_tokenizer\n",
    ")\n",
    "from transformer_experiments.training_utils import CheckPointer, GetBatchFunction, Trainer\n",
    "from transformer_experiments.experiments.block_internals import (\n",
    "    BlockInternalsAccessors,\n",
    "    BlockInternalsExperiment,\n",
    "    BatchedBlockInternalsExperiment,\n",
    "    BlockInternalsAnalysis,\n",
    "    batch_cosine_sim,\n",
    ")\n",
    "from transformer_experiments.experiments.cosine_sims import pre_filter_cosine_sim_results\n",
    "from transformer_experiments.experiments.final_ffwd import FinalFFWDExperiment\n",
    "from transformer_experiments.experiments.similar_strings import (\n",
    "    SimilarStringsData,\n",
    "    SimilarStringsExperiment,\n",
    "    SimilarStringsResult\n",
    ")\n",
    "from transformer_experiments.experiments.logit_lens import LogitLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment is local_mac\n"
     ]
    }
   ],
   "source": [
    "environment = get_environment()\n",
    "print(f\"environment is {environment.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ts = TinyShakespeareDataSet(cache_file=environment.code_root / 'nbs/artifacts/input.txt')\n",
    "m, tokenizer = create_model_and_tokenizer(\n",
    "    saved_model_filename=environment.code_root / 'nbs/artifacts/shakespeare-20231112.pt',\n",
    "    dataset=ts,\n",
    "    device=device,\n",
    ")\n",
    "_, val_data = split_text_dataset(ts.text, tokenizer, train_pct=0.9, device=device)\n",
    "encoding_helpers = EncodingHelpers(tokenizer, device)\n",
    "accessors = TransformerAccessors(m, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"device is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if list((environment.data_root / 'block_internals_results/large_files/slen10/').glob('*')) == []:\n",
    "    print(\"Run `make block_internals_slen10_dataset` in the project root to generate the required dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings10 = all_unique_substrings(ts.text, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp10 = BatchedBlockInternalsExperiment(\n",
    "    eh=encoding_helpers,\n",
    "    accessors=accessors,\n",
    "    strings=strings10,\n",
    "    output_dir=environment.data_root / 'block_internals_results/large_files/slen10/',\n",
    "    batch_size=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "n_samples = 20000\n",
    "indices = torch.randperm(len(strings10))[:n_samples]\n",
    "strings20k = [strings10[i.item()] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample of 500 strings\n",
    "sample_size = 500\n",
    "strings_sample = strings20k[:sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put this in a common component\n",
    "def get_model_outputs(prompts: Sequence[str], encoding_helpers: EncodingHelpers):\n",
    "    # Compute the model's predictions:\n",
    "    tokens = encoding_helpers.tokenize_strings(prompts)\n",
    "    logits, _ = m(tokens)\n",
    "\n",
    "    logits = LogitsWrapper(logits, encoding_helpers.tokenizer)\n",
    "    return [topk_tokens[-1] for topk_tokens in logits.topk_tokens(k=10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs_sample = get_model_outputs(strings_sample, encoding_helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_exp = BlockInternalsExperiment(encoding_helpers, accessors, strings_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by examining what we get when we ask for a much larger top k values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sims, emb_distances = exp10.strings_with_topk_closest_embeddings(\n",
    "    prompts_exp.embeddings[:5, :, :],\n",
    "    k=200,\n",
    "    largest=True,\n",
    "    distance_function=batch_cosine_sim,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "         [0.9062, 0.9470, 0.9045, 0.9493, 0.9544],\n",
       "         [0.9062, 0.9462, 0.8602, 0.9083, 0.9443],\n",
       "         [0.9053, 0.9429, 0.8593, 0.9082, 0.9064],\n",
       "         [0.9037, 0.9429, 0.8586, 0.9055, 0.9057],\n",
       "         [0.9027, 0.9404, 0.8566, 0.9052, 0.9045],\n",
       "         [0.9020, 0.9027, 0.8562, 0.9044, 0.8996],\n",
       "         [0.9017, 0.9009, 0.8548, 0.9035, 0.8982],\n",
       "         [0.8962, 0.9009, 0.8545, 0.9034, 0.8656],\n",
       "         [0.8651, 0.9008, 0.8532, 0.9032, 0.8631]]),\n",
       " tensor([[0.7591, 0.7566, 0.7604, 0.8064, 0.8047],\n",
       "         [0.7591, 0.7566, 0.7604, 0.8063, 0.8047],\n",
       "         [0.7591, 0.7566, 0.7603, 0.8063, 0.8042],\n",
       "         [0.7590, 0.7564, 0.7602, 0.8063, 0.8041],\n",
       "         [0.7589, 0.7563, 0.7601, 0.8062, 0.8041],\n",
       "         [0.7589, 0.7563, 0.7601, 0.8061, 0.8040],\n",
       "         [0.7588, 0.7557, 0.7601, 0.8061, 0.8039],\n",
       "         [0.7587, 0.7557, 0.7600, 0.8060, 0.8039],\n",
       "         [0.7587, 0.7557, 0.7599, 0.8060, 0.8038],\n",
       "         [0.7587, 0.7557, 0.7599, 0.8060, 0.8037]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_distances[:10, :], emb_distances[-10:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'is dreams,'   'by present'   's eyes may'   'eart of ho'   ' man, as I'\n",
      "'is dream o'   'My present'   's eye, mak'   'eart of mo'   ' men, as I'\n",
      "'ur dreams,'   'be present'   's eyes in '   'earn of hi'   ' man, as y'\n",
      "'of dreams,'   'dy present'   'l eyes can'   'ears of ha'   ' man, if I'\n",
      "'us dreams.'   'my present'   's eyes to '   'park of ho'   'oman, as t'\n",
      "'he dreams,'   'ry present'   'l eyes gaz'   'eart of ge'   ' men, as i'\n",
      "'ly dreams,'   'y, present'   'r foes may'   'eard of hi'   ' many as y'\n",
      "'en dreams,'   'on present'   'r ever may'   'east of yo'   'cian, as I'\n",
      "'nd dreams,'   't, present'   's eyes do '   'part of hi'   ' son, as t'\n",
      "'as dream\\nS'   'in present'   's eye; tal'   'earn of yo'   ' long as I'\n",
      "\n",
      "'is presenc'   ' a prisone'   'g over mas'   'efit of se'   ' man: we s'\n",
      "'is prowess'   'ot prone t'   'l even tak'   'wist of ro'   ' wind as s'\n",
      "'is be all,'   'is project'   't so I may'   'e is of so'   ' man; all '\n",
      "'is the mad'   'my person.'   'n thou may'   'gent of hi'   ' man, they'\n",
      "\"'s great s\"   'ay prove.\\n'   'rosper may'   'ture of hu'   'tion, as f'\n",
      "'is deed do'   'by herself'   'e that may'   'ents of so'   ' her, as w'\n",
      "'is present'   'or prisone'   'speaks my '   'eral of yo'   ' maid is m'\n",
      "'rs dry; sc'   'be broken:'   'o not, may'   'mark of ot'   \" man, 'tis\"\n",
      "'ish reason'   'ay prove p'   'ounsel may'   'ents of yo'   ':\\nNo, as I'\n",
      "'py drinks,'   'ng prisone'   'n pity may'   'e is of go'   'sland as a'\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    print(f\"{'   '.join([repr(emb_sims[i][j]) for i in range(len(emb_sims))])}\")\n",
    "\n",
    "print()\n",
    "for j in range(-10, 0):\n",
    "    print(f\"{'   '.join([repr(emb_sims[i][j]) for i in range(len(emb_sims))])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_idx = 0\n",
    "proj_sims, proj_distances = exp10.strings_with_topk_closest_proj_outputs(\n",
    "    block_idx=block_idx,\n",
    "    t_i=-1,\n",
    "    queries=prompts_exp.proj_output(block_idx=block_idx)[:5, -1, :],\n",
    "    k=200,\n",
    "    largest=True,\n",
    "    distance_function=batch_cosine_sim,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "         [0.9948, 0.9967, 0.9947, 0.9974, 0.9956],\n",
       "         [0.9942, 0.9967, 0.9945, 0.9960, 0.9933],\n",
       "         [0.9937, 0.9964, 0.9921, 0.9958, 0.9897],\n",
       "         [0.9935, 0.9963, 0.9917, 0.9957, 0.9896],\n",
       "         [0.9928, 0.9962, 0.9910, 0.9956, 0.9871],\n",
       "         [0.9922, 0.9950, 0.9905, 0.9955, 0.9857],\n",
       "         [0.9911, 0.9944, 0.9900, 0.9954, 0.9856],\n",
       "         [0.9897, 0.9942, 0.9895, 0.9950, 0.9850],\n",
       "         [0.9891, 0.9939, 0.9894, 0.9943, 0.9846]]),\n",
       " tensor([[0.9709, 0.9826, 0.9796, 0.9855, 0.9726],\n",
       "         [0.9709, 0.9825, 0.9795, 0.9855, 0.9726],\n",
       "         [0.9709, 0.9824, 0.9794, 0.9854, 0.9726],\n",
       "         [0.9708, 0.9824, 0.9793, 0.9854, 0.9725],\n",
       "         [0.9708, 0.9824, 0.9793, 0.9854, 0.9725],\n",
       "         [0.9708, 0.9823, 0.9793, 0.9854, 0.9724],\n",
       "         [0.9708, 0.9823, 0.9791, 0.9854, 0.9724],\n",
       "         [0.9708, 0.9823, 0.9791, 0.9853, 0.9724],\n",
       "         [0.9707, 0.9823, 0.9791, 0.9853, 0.9723],\n",
       "         [0.9707, 0.9823, 0.9790, 0.9853, 0.9723]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_distances[:10, :], proj_distances[-10:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'is dreams,'   'by present'   's eyes may'   'eart of ho'   ' man, as I'\n",
      "'ly dreams,'   'my present'   'e case may'   'ster of ho'   ' men, as I'\n",
      "'en dreams,'   'dy present'   ' sense may'   'ffer to ha'   ' and, as I'\n",
      "'he dreams,'   'ry present'   'ied as may'   'ruth of ho'   ' not, as I'\n",
      "'ur dreams,'   'be present'   'ay she may'   'otes of ho'   'o me, as I'\n",
      "'nd dreams,'   'My present'   'r foes may'   'anes of ho'   'nd I, as I'\n",
      "'ery beams,'   'y; present'   'So she may'   'oint of ho'   'cian, as I'\n",
      "'of dreams,'   'in present'   ' haste may'   'yers of ho'   'I am, as t'\n",
      "\"n's beams,\"   'is present'   'odesty may'   'ains of ho'   '-day, as I'\n",
      "'hese arms,'   'im present'   'esence may'   'ound of ho'   '\\nAnd, as I'\n",
      "\n",
      "'sires most'   'ast ungent'   \"'s some am\"   'lf with ho'   ' be, was l'\n",
      "'teous mass'   'What scene'   'e they mad'   'tell of hi'   'Look, as I'\n",
      "'m to kiss,'   'lest scent'   ' am to say'   'Thus to ha'   'wick, as o'\n",
      "'much amiss'   'ondon sent'   ' early mad'   's of a tho'   'aith, as y'\n",
      "'wings misd'   'ng presenc'   'If you may'   'ven for ha'   'ut I was a'\n",
      "'from himse'   ' this sent'   'es the mai'   ' of our ho'   'y, is as a'\n",
      "'isdom hast'   'viest cens'   'et him say'   'tars of he'   'early as m'\n",
      "'hat seems '   'ou dissent'   'lords, may'   's other ho'   ' Yet, as t'\n",
      "'er bosoms!'   'tion spend'   'o them say'   'now the ho'   ' duke as I'\n",
      "' so, himse'   'have spent'   'nd now may'   'keep at ho'   's low as t'\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    print(f\"{'   '.join([repr(proj_sims[i][j]) for i in range(len(proj_sims))])}\")\n",
    "\n",
    "print()\n",
    "for j in range(-10, 0):\n",
    "    print(f\"{'   '.join([repr(proj_sims[i][j]) for i in range(len(proj_sims))])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_idx = 0\n",
    "ffwd_sims, ffwd_distances = exp10.strings_with_topk_closest_ffwd_outputs(\n",
    "    block_idx=block_idx,\n",
    "    t_i=-1,\n",
    "    queries=prompts_exp.ffwd_output(block_idx=block_idx)[:5, -1, :],\n",
    "    k=200,\n",
    "    largest=True,\n",
    "    distance_function=batch_cosine_sim,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "         [0.9998, 0.9999, 0.9997, 0.9998, 0.9999],\n",
       "         [0.9998, 0.9998, 0.9996, 0.9998, 0.9997],\n",
       "         [0.9998, 0.9998, 0.9996, 0.9998, 0.9997],\n",
       "         [0.9998, 0.9998, 0.9995, 0.9998, 0.9997],\n",
       "         [0.9997, 0.9998, 0.9995, 0.9998, 0.9996],\n",
       "         [0.9997, 0.9997, 0.9995, 0.9998, 0.9995],\n",
       "         [0.9997, 0.9997, 0.9995, 0.9998, 0.9995],\n",
       "         [0.9996, 0.9997, 0.9995, 0.9997, 0.9994],\n",
       "         [0.9996, 0.9997, 0.9995, 0.9997, 0.9994]]),\n",
       " tensor([[0.9988, 0.9990, 0.9989, 0.9992, 0.9983],\n",
       "         [0.9988, 0.9990, 0.9989, 0.9992, 0.9983],\n",
       "         [0.9988, 0.9990, 0.9989, 0.9992, 0.9983],\n",
       "         [0.9988, 0.9990, 0.9989, 0.9992, 0.9983],\n",
       "         [0.9988, 0.9990, 0.9989, 0.9992, 0.9983],\n",
       "         [0.9987, 0.9990, 0.9989, 0.9992, 0.9983],\n",
       "         [0.9987, 0.9990, 0.9989, 0.9992, 0.9983],\n",
       "         [0.9987, 0.9990, 0.9989, 0.9992, 0.9983],\n",
       "         [0.9987, 0.9990, 0.9989, 0.9992, 0.9983],\n",
       "         [0.9987, 0.9990, 0.9989, 0.9992, 0.9983]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffwd_distances[:10, :], ffwd_distances[-10:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'is dreams,'   'by present'   's eyes may'   'eart of ho'   ' man, as I'\n",
      "'en dreams,'   'my present'   ' sense may'   'ster of ho'   ' men, as I'\n",
      "'ly dreams,'   'dy present'   'e case may'   'otes of ho'   ' and, as I'\n",
      "'he dreams,'   'ry present'   'r foes may'   'anes of ho'   ' not, as I'\n",
      "'nd dreams,'   'My present'   'ied as may'   'ains of ho'   'o me, as I'\n",
      "'ur dreams,'   'be present'   'ay she may'   'oint of ho'   'nd I, as I'\n",
      "'of dreams,'   'y, present'   ' bones may'   'ound of ho'   '-day, as I'\n",
      "'ery beams,'   'y; present'   'So she may'   'fear to ho'   ' but, as I'\n",
      "'rate arms,'   'im present'   ' grace may'   'ally of ho'   'rd me as I'\n",
      "'hese arms,'   'in present'   'e more may'   'yers of ho'   '\\nYes, as I'\n",
      "\n",
      "'ck groans,'   'll\\nPresent'   'en you say'   'ace our ho'   'r sakes, I'\n",
      "'te builds,'   's innocent'   'Hope I may'   'he that ho'   'early as I'\n",
      "'she finds,'   'me Florent'   'e must say'   'Half an ho'   'nes, and I'\n",
      "'reat loss,'   'their gent'   'n thou may'   'as mine ho'   'fe,--\\nAs I'\n",
      "'ld cramps,'   ', insolent'   'ctions may'   'When it ho'   ' see it, I'\n",
      "'he bleeds,'   'say I\\nsent'   'How he may'   '\\nIf so tho'   'ess woe, I'\n",
      "'our cross,'   'fore, gent'   'ame,\\nI say'   'us, the ho'   ' he does I'\n",
      "'han tears,'   'ldness ent'   'no way say'   'ale and ho'   'mad,--as I'\n",
      "'hese wars,'   'ch garment'   'e now, say'   'out any ho'   ' way can I'\n",
      "'to adders,'   'ic garment'   'ple,\\nI may'   'here\\nat ho'   'rant, an I'\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    print(f\"{'   '.join([repr(ffwd_sims[i][j]) for i in range(len(ffwd_sims))])}\")\n",
    "\n",
    "print()\n",
    "for j in range(-10, 0):\n",
    "    print(f\"{'   '.join([repr(ffwd_sims[i][j]) for i in range(len(ffwd_sims))])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Cosine Sim Data for 20,000 Strings Against all Length 10 Strings\n",
    "\n",
    "This uses the output of ConsineSimilarityExperiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = environment.data_root / 'cosine_sim_results/large_files/slen10/'\n",
    "n_batches = 8590\n",
    "n_queries = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_map10 = build_next_token_map(ts.text, 10, tokenizer.vocab_size, tokenizer.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is slow - prefer using pre-filtering and then using filter_on_prefiltered_results()\n",
    "def filter_across_batches(\n",
    "    get_batch: Callable[[int], torch.Tensor],\n",
    "    n_batches: int,\n",
    "    filter_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "    n_queries: int,\n",
    "):\n",
    "    total_count = 0\n",
    "    matching_indices = [[] for _ in range(n_queries)]\n",
    "    for batch_idx in range(n_batches):\n",
    "        batch = get_batch(batch_idx)\n",
    "        batch_size, n_queries_batch = batch.shape\n",
    "        assert n_queries_batch == n_queries\n",
    "\n",
    "        filtered = filter_fn(batch)\n",
    "        nonzeros = torch.nonzero(filtered)\n",
    "        for i in range(nonzeros.shape[0]):\n",
    "            idx_in_batch, query_idx = nonzeros[i, :]\n",
    "            matching_indices[query_idx.item()].append(total_count + idx_in_batch.item())\n",
    "\n",
    "        total_count += batch_size\n",
    "\n",
    "    return matching_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for filter_across_batches()\n",
    "\n",
    "batches = [\n",
    "    torch.tensor([\n",
    "        [0.0, 0.6, 0.4, 0.3],\n",
    "        [0.1, 0.3, 0.5, 0.1],\n",
    "        [0.0, 0.1, 0.8, 0.0],\n",
    "    ]),\n",
    "    torch.tensor([\n",
    "        [0.7, 0.2, 0.6, 0.3],\n",
    "        [0.1, 0.8, 0.2, 0.8],\n",
    "    ]),\n",
    "]\n",
    "\n",
    "result = filter_across_batches(\n",
    "    get_batch=lambda i: batches[i],\n",
    "    n_batches=len(batches),\n",
    "    filter_fn=lambda batch: batch > 0.5,\n",
    "    n_queries=4,\n",
    ")\n",
    "test_eq(result, [\n",
    "    [3,],\n",
    "    [0, 4],\n",
    "    [2, 3],\n",
    "    [4],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefiltering Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, pre-filter the results for the first 500 queries to just the values > 0.7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_batch = lambda batch_idx: torch.load(results_folder / f'cosine_sim_ffwd_out_{batch_idx:05d}.pt')\n",
    "q_idx_start = 0\n",
    "q_idx_end = 500\n",
    "threshold = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slow, takes ~15 mins to run\n",
    "prefiltered_result = pre_filter_cosine_sim_results(\n",
    "    load_batch=load_batch,\n",
    "    n_batches=n_batches,\n",
    "    q_idx_start=q_idx_start,\n",
    "    q_idx_end=q_idx_end,\n",
    "    threshold=threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the prefiltered results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefiltered_results_folder = results_folder / f'prefiltered_{threshold}'\n",
    "prefiltered_results_folder.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefiltered_filename(q_idx: int, block_idx: int) -> Path:\n",
    "    return prefiltered_results_folder / f'cosine_sim_ffwd_out_{q_idx:05d}_{block_idx:02d}.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd2789d021044e38969cd3c0322818c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for q_idx in tqdm(range(q_idx_start, q_idx_end)):\n",
    "    for block_idx in range(n_layer):\n",
    "        torch.save(\n",
    "            prefiltered_result[q_idx - q_idx_start][block_idx],\n",
    "            prefiltered_filename(q_idx, block_idx),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadPrefilteredFunction(Protocol):\n",
    "    def __call__(self, q_idx: int) -> torch.Tensor:\n",
    "        ...\n",
    "\n",
    "\n",
    "def filter_on_prefiltered_results(\n",
    "    load_prefiltered: LoadPrefilteredFunction,\n",
    "    q_idx_start: int,\n",
    "    q_idx_end: int,\n",
    "    filter_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "):\n",
    "    matching_indices = []\n",
    "    for q_idx in range(q_idx_start, q_idx_end):\n",
    "        prefiltered = load_prefiltered(q_idx)\n",
    "        indices, values = itemgetter('indices', 'values')(prefiltered)\n",
    "        indices_into_values = torch.nonzero(filter_fn(values)).squeeze(dim=-1)\n",
    "        matching_indices.append(indices[indices_into_values])\n",
    "\n",
    "    return matching_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, collect some stats about the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_result_stats(\n",
    "    filter_results: List[List[int]],\n",
    "):\n",
    "    lens = [len(result) for result in filter_results]\n",
    "    return {\n",
    "        'min': min(lens),\n",
    "        'max': max(lens),\n",
    "        'mean': np.mean(lens),\n",
    "        'std': np.std(lens),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_strings(\n",
    "    filter_result: List[List[int]],\n",
    "    strings: Sequence[str],\n",
    "):\n",
    "    return [\n",
    "        [\n",
    "            strings[j]\n",
    "            for j in filter_result[i]\n",
    "        ]\n",
    "        for i in range(len(filter_result))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put this in a common component\n",
    "def analyze_simulate_results(sim_freqs, model_outputs):\n",
    "    assert len(sim_freqs) == len(model_outputs)\n",
    "    topn_matches = [0 for _ in range(10)]\n",
    "    topn_matches_any_order = [0 for _ in range(10)]\n",
    "    for i, sim_freq in enumerate(sim_freqs):\n",
    "        sim_output = top_nonzero_tokens(sim_freq, encoding_helpers.tokenizer.itos)[:10]\n",
    "        model_output = model_outputs[i]\n",
    "\n",
    "        sim_tokens, _ = zip(*sim_output)\n",
    "        model_tokens, _ = zip(*model_output)\n",
    "\n",
    "        n = min(len(sim_tokens), len(model_tokens))\n",
    "        for j in range(n):\n",
    "            if sim_tokens[j] == model_tokens[j]:\n",
    "                topn_matches[j] += 1\n",
    "            if set(sim_tokens[:j+1]) == set(model_tokens[:j+1]):\n",
    "                topn_matches_any_order[j] += 1\n",
    "\n",
    "    return topn_matches, topn_matches_any_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(\n",
    "    matching_indices: Sequence[Sequence[int]],\n",
    "    n_queries: int,\n",
    "    all_strings: Sequence[str],\n",
    "    next_token_map: Dict[str, torch.Tensor],\n",
    "    model_outputs: Sequence[Sequence[Tuple[str, float]]],\n",
    "):\n",
    "    print(filter_result_stats(matching_indices))\n",
    "\n",
    "    filter_results_strings = get_matching_strings(matching_indices, all_strings)\n",
    "    filter_result_freqs = [\n",
    "        torch.stack([\n",
    "            next_token_map[matching_string]\n",
    "            for matching_string in matching_strings\n",
    "        ]).sum(dim=0)\n",
    "        for matching_strings in filter_results_strings\n",
    "    ]\n",
    "\n",
    "    filter_result_probs = [\n",
    "        freqs / freqs.sum()\n",
    "        for freqs in filter_result_freqs\n",
    "    ]\n",
    "\n",
    "    topn_matches, topn_matches_any_order = analyze_simulate_results(filter_result_probs, model_outputs)\n",
    "    for i in range(10):\n",
    "        print(f\"Top {i+1} matches: {topn_matches[i] / n_queries:.3f}\")\n",
    "        print(f\"Top {i+1} matches (any order): {topn_matches_any_order[i] / n_queries:.3f}\")\n",
    "\n",
    "    return filter_result_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_queries_sample = 500\n",
    "q_idx_start = 0\n",
    "q_idx_end = n_queries_sample\n",
    "model_outputs_sample = get_model_outputs(strings20k[:n_queries_sample], encoding_helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min': 1, 'max': 30527, 'mean': 2143.642, 'std': 5838.287069495299}\n",
      "Top 1 matches: 0.788\n",
      "Top 1 matches (any order): 0.788\n",
      "Top 2 matches: 0.424\n",
      "Top 2 matches (any order): 0.504\n",
      "Top 3 matches: 0.268\n",
      "Top 3 matches (any order): 0.334\n",
      "Top 4 matches: 0.186\n",
      "Top 4 matches (any order): 0.260\n",
      "Top 5 matches: 0.164\n",
      "Top 5 matches (any order): 0.198\n",
      "Top 6 matches: 0.088\n",
      "Top 6 matches (any order): 0.134\n",
      "Top 7 matches: 0.076\n",
      "Top 7 matches (any order): 0.102\n",
      "Top 8 matches: 0.058\n",
      "Top 8 matches (any order): 0.086\n",
      "Top 9 matches: 0.054\n",
      "Top 9 matches (any order): 0.062\n",
      "Top 10 matches: 0.046\n",
      "Top 10 matches (any order): 0.040\n"
     ]
    }
   ],
   "source": [
    "block_idx = 5\n",
    "ffwd5_freqs = analyze_dataset(\n",
    "    matching_indices=filter_on_prefiltered_results(\n",
    "        load_prefiltered=lambda q_idx: torch.load(prefiltered_filename(q_idx, block_idx)),\n",
    "        q_idx_start=q_idx_start,\n",
    "        q_idx_end=q_idx_end,\n",
    "        filter_fn=lambda values: values > 0.89,\n",
    "    ),\n",
    "    n_queries=n_queries_sample,\n",
    "    all_strings=strings10,\n",
    "    next_token_map=next_token_map10,\n",
    "    model_outputs=model_outputs_sample,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min': 1, 'max': 61657, 'mean': 3283.636, 'std': 9950.066907488814}\n",
      "Top 1 matches: 0.790\n",
      "Top 1 matches (any order): 0.790\n",
      "Top 2 matches: 0.450\n",
      "Top 2 matches (any order): 0.518\n",
      "Top 3 matches: 0.270\n",
      "Top 3 matches (any order): 0.318\n",
      "Top 4 matches: 0.202\n",
      "Top 4 matches (any order): 0.232\n",
      "Top 5 matches: 0.170\n",
      "Top 5 matches (any order): 0.176\n",
      "Top 6 matches: 0.106\n",
      "Top 6 matches (any order): 0.142\n",
      "Top 7 matches: 0.096\n",
      "Top 7 matches (any order): 0.108\n",
      "Top 8 matches: 0.072\n",
      "Top 8 matches (any order): 0.080\n",
      "Top 9 matches: 0.048\n",
      "Top 9 matches (any order): 0.056\n",
      "Top 10 matches: 0.036\n",
      "Top 10 matches (any order): 0.032\n"
     ]
    }
   ],
   "source": [
    "block_idx = 4\n",
    "ffwd4_freqs = analyze_dataset(\n",
    "    matching_indices=filter_on_prefiltered_results(\n",
    "        load_prefiltered=lambda q_idx: torch.load(prefiltered_filename(q_idx, block_idx)),\n",
    "        q_idx_start=q_idx_start,\n",
    "        q_idx_end=q_idx_end,\n",
    "        filter_fn=lambda values: values > 0.81,\n",
    "    ),\n",
    "    n_queries=n_queries_sample,\n",
    "    all_strings=strings10,\n",
    "    next_token_map=next_token_map10,\n",
    "    model_outputs=model_outputs_sample,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min': 1, 'max': 9100, 'mean': 624.342, 'std': 1387.415385901425}\n",
      "Top 1 matches: 0.804\n",
      "Top 1 matches (any order): 0.804\n",
      "Top 2 matches: 0.478\n",
      "Top 2 matches (any order): 0.542\n",
      "Top 3 matches: 0.304\n",
      "Top 3 matches (any order): 0.380\n",
      "Top 4 matches: 0.234\n",
      "Top 4 matches (any order): 0.274\n",
      "Top 5 matches: 0.192\n",
      "Top 5 matches (any order): 0.228\n",
      "Top 6 matches: 0.134\n",
      "Top 6 matches (any order): 0.184\n",
      "Top 7 matches: 0.112\n",
      "Top 7 matches (any order): 0.114\n",
      "Top 8 matches: 0.084\n",
      "Top 8 matches (any order): 0.104\n",
      "Top 9 matches: 0.066\n",
      "Top 9 matches (any order): 0.082\n",
      "Top 10 matches: 0.050\n",
      "Top 10 matches (any order): 0.060\n"
     ]
    }
   ],
   "source": [
    "block_idx = 3\n",
    "ffwd3_freqs = analyze_dataset(\n",
    "    matching_indices=filter_on_prefiltered_results(\n",
    "        load_prefiltered=lambda q_idx: torch.load(prefiltered_filename(q_idx, block_idx)),\n",
    "        q_idx_start=q_idx_start,\n",
    "        q_idx_end=q_idx_end,\n",
    "        filter_fn=lambda values: values > 0.76,\n",
    "    ),\n",
    "    n_queries=n_queries_sample,\n",
    "    all_strings=strings10,\n",
    "    next_token_map=next_token_map10,\n",
    "    model_outputs=model_outputs_sample,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min': 1, 'max': 7457, 'mean': 446.792, 'std': 995.0116103523616}\n",
      "Top 1 matches: 0.808\n",
      "Top 1 matches (any order): 0.808\n",
      "Top 2 matches: 0.458\n",
      "Top 2 matches (any order): 0.496\n",
      "Top 3 matches: 0.298\n",
      "Top 3 matches (any order): 0.376\n",
      "Top 4 matches: 0.222\n",
      "Top 4 matches (any order): 0.256\n",
      "Top 5 matches: 0.182\n",
      "Top 5 matches (any order): 0.196\n",
      "Top 6 matches: 0.102\n",
      "Top 6 matches (any order): 0.158\n",
      "Top 7 matches: 0.112\n",
      "Top 7 matches (any order): 0.132\n",
      "Top 8 matches: 0.090\n",
      "Top 8 matches (any order): 0.096\n",
      "Top 9 matches: 0.074\n",
      "Top 9 matches (any order): 0.070\n",
      "Top 10 matches: 0.042\n",
      "Top 10 matches (any order): 0.056\n"
     ]
    }
   ],
   "source": [
    "block_idx = 2\n",
    "ffwd2_freqs = analyze_dataset(\n",
    "    matching_indices=filter_on_prefiltered_results(\n",
    "        load_prefiltered=lambda q_idx: torch.load(prefiltered_filename(q_idx, block_idx)),\n",
    "        q_idx_start=q_idx_start,\n",
    "        q_idx_end=q_idx_end,\n",
    "        filter_fn=lambda values: values > 0.85,\n",
    "    ),\n",
    "    n_queries=n_queries_sample,\n",
    "    all_strings=strings10,\n",
    "    next_token_map=next_token_map10,\n",
    "    model_outputs=model_outputs_sample,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min': 1, 'max': 5070, 'mean': 369.31, 'std': 755.4999099271952}\n",
      "Top 1 matches: 0.750\n",
      "Top 1 matches (any order): 0.750\n",
      "Top 2 matches: 0.416\n",
      "Top 2 matches (any order): 0.470\n",
      "Top 3 matches: 0.236\n",
      "Top 3 matches (any order): 0.312\n",
      "Top 4 matches: 0.192\n",
      "Top 4 matches (any order): 0.218\n",
      "Top 5 matches: 0.172\n",
      "Top 5 matches (any order): 0.172\n",
      "Top 6 matches: 0.102\n",
      "Top 6 matches (any order): 0.126\n",
      "Top 7 matches: 0.086\n",
      "Top 7 matches (any order): 0.102\n",
      "Top 8 matches: 0.068\n",
      "Top 8 matches (any order): 0.078\n",
      "Top 9 matches: 0.060\n",
      "Top 9 matches (any order): 0.076\n",
      "Top 10 matches: 0.050\n",
      "Top 10 matches (any order): 0.060\n"
     ]
    }
   ],
   "source": [
    "block_idx = 1\n",
    "ffwd1_freqs = analyze_dataset(\n",
    "    matching_indices=filter_on_prefiltered_results(\n",
    "        load_prefiltered=lambda q_idx: torch.load(prefiltered_filename(q_idx, block_idx)),\n",
    "        q_idx_start=q_idx_start,\n",
    "        q_idx_end=q_idx_end,\n",
    "        filter_fn=lambda values: values > 0.94,\n",
    "    ),\n",
    "    n_queries=n_queries_sample,\n",
    "    all_strings=strings10,\n",
    "    next_token_map=next_token_map10,\n",
    "    model_outputs=model_outputs_sample,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min': 1, 'max': 5078, 'mean': 333.072, 'std': 663.3523097238751}\n",
      "Top 1 matches: 0.754\n",
      "Top 1 matches (any order): 0.754\n",
      "Top 2 matches: 0.410\n",
      "Top 2 matches (any order): 0.472\n",
      "Top 3 matches: 0.252\n",
      "Top 3 matches (any order): 0.316\n",
      "Top 4 matches: 0.192\n",
      "Top 4 matches (any order): 0.216\n",
      "Top 5 matches: 0.146\n",
      "Top 5 matches (any order): 0.170\n",
      "Top 6 matches: 0.108\n",
      "Top 6 matches (any order): 0.128\n",
      "Top 7 matches: 0.092\n",
      "Top 7 matches (any order): 0.100\n",
      "Top 8 matches: 0.072\n",
      "Top 8 matches (any order): 0.072\n",
      "Top 9 matches: 0.068\n",
      "Top 9 matches (any order): 0.072\n",
      "Top 10 matches: 0.046\n",
      "Top 10 matches (any order): 0.056\n"
     ]
    }
   ],
   "source": [
    "block_idx = 0\n",
    "ffwd0_freqs = analyze_dataset(\n",
    "    matching_indices=filter_on_prefiltered_results(\n",
    "        load_prefiltered=lambda q_idx: torch.load(prefiltered_filename(q_idx, block_idx)),\n",
    "        q_idx_start=q_idx_start,\n",
    "        q_idx_end=q_idx_end,\n",
    "        filter_fn=lambda values: values > 0.95,\n",
    "    ),\n",
    "    n_queries=n_queries_sample,\n",
    "    all_strings=strings10,\n",
    "    next_token_map=next_token_map10,\n",
    "    model_outputs=model_outputs_sample,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.796\n",
      "Top 1 matches (any order): 0.796\n",
      "Top 2 matches: 0.500\n",
      "Top 2 matches (any order): 0.572\n",
      "Top 3 matches: 0.326\n",
      "Top 3 matches (any order): 0.382\n",
      "Top 4 matches: 0.262\n",
      "Top 4 matches (any order): 0.308\n",
      "Top 5 matches: 0.192\n",
      "Top 5 matches (any order): 0.226\n",
      "Top 6 matches: 0.144\n",
      "Top 6 matches (any order): 0.186\n",
      "Top 7 matches: 0.122\n",
      "Top 7 matches (any order): 0.144\n",
      "Top 8 matches: 0.092\n",
      "Top 8 matches (any order): 0.104\n",
      "Top 9 matches: 0.076\n",
      "Top 9 matches (any order): 0.084\n",
      "Top 10 matches: 0.048\n",
      "Top 10 matches (any order): 0.054\n"
     ]
    }
   ],
   "source": [
    "total_freqs = [\n",
    "    (\n",
    "        0.01*ffwd0_freqs[i] +\n",
    "        0.01*ffwd1_freqs[i] +\n",
    "        0.1*ffwd2_freqs[i] +\n",
    "        1.5*ffwd3_freqs[i] +\n",
    "        4*ffwd4_freqs[i] +\n",
    "        0.01*ffwd5_freq\n",
    "    )\n",
    "    for i, ffwd5_freq in enumerate(ffwd5_freqs)\n",
    "]\n",
    "total_probs = [\n",
    "    freqs / freqs.sum()\n",
    "    for freqs in total_freqs\n",
    "]\n",
    "topn_matches, topn_matches_any_order = analyze_simulate_results(total_probs, model_outputs_sample)\n",
    "for i in range(10):\n",
    "    print(f\"Top {i+1} matches: {topn_matches[i] / n_queries_sample:.3f}\")\n",
    "    print(f\"Top {i+1} matches (any order): {topn_matches_any_order[i] / n_queries_sample:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporate data from the rest of the strings (everything above just looked at first 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = get_model_outputs(strings20k, encoding_helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = environment.data_root / 'cosine_sim_results/large_files/slen10/'\n",
    "n_batches = 8590\n",
    "n_queries = 20000\n",
    "\n",
    "next_token_map = next_token_map10\n",
    "all_strings = strings10\n",
    "\n",
    "ffwd_thresholds = [0.95, 0.94, 0.85, 0.76, 0.81, 0.89]\n",
    "ffwd_freqs = [[] for _ in range(n_layer)]\n",
    "\n",
    "for block_idx in tqdm(range(n_layer)):\n",
    "    get_batch = lambda batch_idx: torch.load(\n",
    "        str(results_folder / f'cosine_sim_ffwd_out_{batch_idx:05d}.pt'),\n",
    "        mmap=True,\n",
    "    )[block_idx, :, :n_queries]\n",
    "\n",
    "    filter_results = filter_across_batches(\n",
    "        get_batch=get_batch,\n",
    "        n_batches=n_batches,\n",
    "        filter_fn=lambda batch: batch > ffwd_thresholds[block_idx],\n",
    "        n_queries=n_queries,\n",
    "    )\n",
    "    filter_results_strings = get_matching_strings(filter_results, all_strings)\n",
    "    ffwd_freqs[block_idx] = [\n",
    "        torch.stack(\n",
    "            [\n",
    "                next_token_map[matching_string]\n",
    "                for matching_string in matching_strings\n",
    "            ]\n",
    "        ).sum(dim=0)\n",
    "        for matching_strings in filter_results_strings\n",
    "    ]\n",
    "\n",
    "ffwd_freqs = torch.stack(\n",
    "    [torch.stack(ffwd_freqs[block_idx]) for block_idx in range(n_layer)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 20000, 65]), 20000)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffwd_freqs.shape, len(model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(results_folder / 'learn_coefficients').mkdir(exist_ok=True)\n",
    "torch.save(ffwd_freqs, str(results_folder / 'learn_coefficients/ffwd_freqs.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ffwd_freqs from disk\n",
    "ffwd_freqs = torch.load(results_folder / 'learn_coefficients/ffwd_freqs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.796\n",
      "Top 1 matches (any order): 0.796\n",
      "Top 2 matches: 0.500\n",
      "Top 2 matches (any order): 0.572\n",
      "Top 3 matches: 0.326\n",
      "Top 3 matches (any order): 0.382\n",
      "Top 4 matches: 0.262\n",
      "Top 4 matches (any order): 0.308\n",
      "Top 5 matches: 0.192\n",
      "Top 5 matches (any order): 0.226\n",
      "Top 6 matches: 0.144\n",
      "Top 6 matches (any order): 0.186\n",
      "Top 7 matches: 0.122\n",
      "Top 7 matches (any order): 0.144\n",
      "Top 8 matches: 0.092\n",
      "Top 8 matches (any order): 0.104\n",
      "Top 9 matches: 0.076\n",
      "Top 9 matches (any order): 0.084\n",
      "Top 10 matches: 0.048\n",
      "Top 10 matches (any order): 0.054\n"
     ]
    }
   ],
   "source": [
    "# Check that we still get the same results for the first 500\n",
    "hand_rolled_coeffs = torch.tensor([0.01, 0.01, 0.1, 1.5, 4, 0.01]).unsqueeze(dim=1).unsqueeze(dim=2) # (n_layer, 1, 1)\n",
    "total_freqs = (ffwd_freqs[:, :500, :] * hand_rolled_coeffs).sum(dim=0)\n",
    "total_probs = total_freqs / total_freqs.sum(dim=-1, keepdim=True)\n",
    "topn_matches, topn_matches_any_order = analyze_simulate_results(total_probs, model_outputs[:500])\n",
    "for i in range(10):\n",
    "    print(f\"Top {i+1} matches: {topn_matches[i] / n_queries_sample:.3f}\")\n",
    "    print(f\"Top {i+1} matches (any order): {topn_matches_any_order[i] / n_queries_sample:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.806\n",
      "Top 1 matches (any order): 0.806\n",
      "Top 2 matches: 0.501\n",
      "Top 2 matches (any order): 0.568\n",
      "Top 3 matches: 0.342\n",
      "Top 3 matches (any order): 0.405\n",
      "Top 4 matches: 0.273\n",
      "Top 4 matches (any order): 0.337\n",
      "Top 5 matches: 0.209\n",
      "Top 5 matches (any order): 0.250\n",
      "Top 6 matches: 0.168\n",
      "Top 6 matches (any order): 0.197\n",
      "Top 7 matches: 0.129\n",
      "Top 7 matches (any order): 0.153\n",
      "Top 8 matches: 0.105\n",
      "Top 8 matches (any order): 0.124\n",
      "Top 9 matches: 0.086\n",
      "Top 9 matches (any order): 0.091\n",
      "Top 10 matches: 0.055\n",
      "Top 10 matches (any order): 0.057\n"
     ]
    }
   ],
   "source": [
    "# Look at it for all samples\n",
    "hand_rolled_coeffs = torch.tensor([0.01, 0.01, 0.1, 1.5, 4, 0.01]).unsqueeze(dim=1).unsqueeze(dim=2) # (n_layer, 1, 1)\n",
    "total_freqs = (ffwd_freqs * hand_rolled_coeffs).sum(dim=0)\n",
    "total_probs = total_freqs / total_freqs.sum(dim=-1, keepdim=True)\n",
    "topn_matches, topn_matches_any_order = analyze_simulate_results(total_probs, model_outputs)\n",
    "for i in range(10):\n",
    "    print(f\"Top {i+1} matches: {topn_matches[i] / ffwd_freqs.shape[1]:.3f}\")\n",
    "    print(f\"Top {i+1} matches (any order): {topn_matches_any_order[i] / ffwd_freqs.shape[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But can we tweak these hand-rolled coefficients for the full data set and get better results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.806\n",
      "Top 1 matches (any order): 0.806\n",
      "Top 2 matches: 0.502\n",
      "Top 2 matches (any order): 0.568\n",
      "Top 3 matches: 0.342\n",
      "Top 3 matches (any order): 0.405\n",
      "Top 4 matches: 0.273\n",
      "Top 4 matches (any order): 0.336\n",
      "Top 5 matches: 0.209\n",
      "Top 5 matches (any order): 0.248\n",
      "Top 6 matches: 0.167\n",
      "Top 6 matches (any order): 0.196\n",
      "Top 7 matches: 0.127\n",
      "Top 7 matches (any order): 0.152\n",
      "Top 8 matches: 0.105\n",
      "Top 8 matches (any order): 0.124\n",
      "Top 9 matches: 0.086\n",
      "Top 9 matches (any order): 0.090\n",
      "Top 10 matches: 0.054\n",
      "Top 10 matches (any order): 0.057\n"
     ]
    }
   ],
   "source": [
    "hand_rolled_coeffs = torch.tensor([0.01, 0.01, 0.1, 1.5, 6, 0.01]).unsqueeze(dim=1).unsqueeze(dim=2) # (n_layer, 1, 1)\n",
    "total_freqs = (ffwd_freqs * hand_rolled_coeffs).sum(dim=0)\n",
    "total_probs = total_freqs / total_freqs.sum(dim=-1, keepdim=True)\n",
    "topn_matches, topn_matches_any_order = analyze_simulate_results(total_probs, model_outputs)\n",
    "for i in range(10):\n",
    "    print(f\"Top {i+1} matches: {topn_matches[i] / ffwd_freqs.shape[1]:.3f}\")\n",
    "    print(f\"Top {i+1} matches (any order): {topn_matches_any_order[i] / ffwd_freqs.shape[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best yet for the full data set, with hand-rolled coefficients. Let's see if we can learn better values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything below has not yet been re-run with the new model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSim(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.coeffs = torch.nn.Parameter(\n",
    "            torch.randn(n_layer, 1, 1, dtype=torch.float32, requires_grad=True)\n",
    "        )\n",
    "        torch.nn.init.normal_(self.coeffs.data, mean=0.0, std=0.2)\n",
    "\n",
    "    def forward(self, freqs: torch.Tensor, model_output: Optional[torch.Tensor]=None):\n",
    "        total_freqs = (freqs * self.coeffs).sum(dim=0)\n",
    "        total_probs = total_freqs / total_freqs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        if model_output is not None:\n",
    "            loss = torch.norm(total_probs - model_output, dim=-1).sum()\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "\n",
    "        return total_probs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(\n",
    "    model: ModelSim, get_batch_func: GetBatchFunction, eval_iters: int=100\n",
    "):\n",
    "    out = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch_func(split=split)\n",
    "\n",
    "            _, loss = model(X, Y)\n",
    "\n",
    "            losses[k] = loss.item()\n",
    "\n",
    "        out[split] = losses.mean()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size: int, freqs: torch.Tensor, split: str='train', train_pct: float=0.9):\n",
    "    n = freqs.shape[1]\n",
    "    assert split in ['train', 'val']\n",
    "    n_train = int(n * train_pct)\n",
    "    low = 0 if split == 'train' else n_train\n",
    "    high = n_train if split == 'train' else n\n",
    "\n",
    "    batch_indices = torch.randint(low=low, high=high, size=(batch_size,), dtype=torch.long)\n",
    "    batch_strings = [strings20k[i.item()] for i in batch_indices]\n",
    "\n",
    "    tokens = encoding_helpers.tokenize_strings(batch_strings)\n",
    "    logits, _ = m(tokens)\n",
    "    model_output = F.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "    return freqs[:, batch_indices, :].clone(), model_output.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=500\n",
    "eval_iters = 100\n",
    "\n",
    "get_batch_func = partial(get_batch, batch_size=batch_size, freqs=ffwd_freqs, train_pct=0.9)\n",
    "estimate_loss_func = partial(estimate_loss, get_batch_func=get_batch_func, eval_iters=eval_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(1337) # Ensure stable random values\n",
    "m2 = ModelSim()\n",
    "_ = m2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = results_folder / 'learn_coefficients'\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "checkpointer = CheckPointer(output_dir, 'coeff_model_checkpoint', start_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=m2,\n",
    "    checkpointer=checkpointer,\n",
    "    get_batch_func=get_batch_func,\n",
    "    estimate_loss_func=estimate_loss_func,\n",
    "    iters_trained=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(355.8277), 'val': tensor(390.0248)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a starting point for the loss\n",
    "estimate_loss_func(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56229a8f8f3b4439be223b41f16c3a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 499: train loss 92.6609, val loss 94.5815\n",
      "step 999: train loss 98.4017, val loss 101.1536\n"
     ]
    }
   ],
   "source": [
    "# Start with a pretty high learning rate and go for 1000 iterations\n",
    "learning_rate = 3e-2\n",
    "optimizer = torch.optim.AdamW(m2.parameters(), lr=learning_rate)\n",
    "\n",
    "trainer.train(1000, optimizer, eval_interval=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it got to a good place and then quickly overshot. But we don't know if it would have recovered because we stopped after that. Let's keep going and see what happens. We can always backtrack if it keeps getting worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9058ef56c0d1498fb4fcc6b83b87e94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 499: train loss 98.9964, val loss 99.9345\n",
      "step 999: train loss 96.9539, val loss 97.9114\n",
      "step 1499: train loss 90.8162, val loss 92.1447\n",
      "step 1999: train loss 101.1968, val loss 103.3944\n",
      "step 2499: train loss 101.8853, val loss 104.0021\n",
      "step 2999: train loss 100.8928, val loss 104.1331\n",
      "step 3499: train loss 101.3843, val loss 103.6743\n",
      "step 3999: train loss 101.1491, val loss 104.2174\n",
      "step 4499: train loss 101.8255, val loss 103.3381\n",
      "step 4999: train loss 100.8471, val loss 103.2514\n"
     ]
    }
   ],
   "source": [
    "trainer.train(5000, optimizer, eval_interval=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that got better but then overshot. Let's go back to the good point and try a smaller learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, tensor(90.8162), tensor(92.1447))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(checkpointer.output_dir / 'coeff_model_checkpoint_000004.pt')\n",
    "checkpoint['iters'], checkpoint['train_loss'], checkpoint['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are back at a good state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0998]],\n",
       "\n",
       "        [[0.0701]],\n",
       "\n",
       "        [[0.1811]],\n",
       "\n",
       "        [[0.1651]],\n",
       "\n",
       "        [[0.3011]],\n",
       "\n",
       "        [[0.3028]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.coeffs.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.781\n",
      "Top 1 matches (any order): 0.781\n",
      "Top 2 matches: 0.474\n",
      "Top 2 matches (any order): 0.549\n",
      "Top 3 matches: 0.331\n",
      "Top 3 matches (any order): 0.393\n",
      "Top 4 matches: 0.266\n",
      "Top 4 matches (any order): 0.334\n",
      "Top 5 matches: 0.206\n",
      "Top 5 matches (any order): 0.245\n",
      "Top 6 matches: 0.163\n",
      "Top 6 matches (any order): 0.197\n",
      "Top 7 matches: 0.129\n",
      "Top 7 matches (any order): 0.153\n",
      "Top 8 matches: 0.104\n",
      "Top 8 matches (any order): 0.124\n",
      "Top 9 matches: 0.087\n",
      "Top 9 matches (any order): 0.092\n",
      "Top 10 matches: 0.057\n",
      "Top 10 matches (any order): 0.059\n"
     ]
    }
   ],
   "source": [
    "total_probs, _ = m2(ffwd_freqs)\n",
    "topn_matches, topn_matches_any_order = analyze_simulate_results(total_probs, model_outputs)\n",
    "for i in range(10):\n",
    "    print(f\"Top {i+1} matches: {topn_matches[i] / ffwd_freqs.shape[1]:.3f}\")\n",
    "    print(f\"Top {i+1} matches (any order): {topn_matches_any_order[i] / ffwd_freqs.shape[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad but we know we can do better. Let's reduce the learning rate and keep going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95e52ccd54843f9936156885abc9a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 499: train loss 86.7844, val loss 85.1847\n",
      "step 999: train loss 89.7584, val loss 85.5004\n",
      "step 1499: train loss 86.8556, val loss 85.5568\n",
      "step 1999: train loss 87.6787, val loss 85.1027\n"
     ]
    }
   ],
   "source": [
    "# Reduce the learning rate by one order of magnitude\n",
    "learning_rate = 3e-3\n",
    "optimizer = torch.optim.AdamW(m2.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train some more\n",
    "trainer.train(2000, optimizer, eval_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0063]],\n",
       "\n",
       "        [[-0.0217]],\n",
       "\n",
       "        [[ 0.1207]],\n",
       "\n",
       "        [[ 0.2180]],\n",
       "\n",
       "        [[ 0.3632]],\n",
       "\n",
       "        [[ 0.3682]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.coeffs.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.795\n",
      "Top 1 matches (any order): 0.795\n",
      "Top 2 matches: 0.486\n",
      "Top 2 matches (any order): 0.558\n",
      "Top 3 matches: 0.333\n",
      "Top 3 matches (any order): 0.397\n",
      "Top 4 matches: 0.268\n",
      "Top 4 matches (any order): 0.338\n",
      "Top 5 matches: 0.206\n",
      "Top 5 matches (any order): 0.250\n",
      "Top 6 matches: 0.166\n",
      "Top 6 matches (any order): 0.199\n",
      "Top 7 matches: 0.127\n",
      "Top 7 matches (any order): 0.151\n",
      "Top 8 matches: 0.104\n",
      "Top 8 matches (any order): 0.124\n",
      "Top 9 matches: 0.085\n",
      "Top 9 matches (any order): 0.090\n",
      "Top 10 matches: 0.052\n",
      "Top 10 matches (any order): 0.058\n"
     ]
    }
   ],
   "source": [
    "total_probs, _ = m2(ffwd_freqs)\n",
    "topn_matches, topn_matches_any_order = analyze_simulate_results(total_probs, model_outputs)\n",
    "for i in range(10):\n",
    "    print(f\"Top {i+1} matches: {topn_matches[i] / ffwd_freqs.shape[1]:.3f}\")\n",
    "    print(f\"Top {i+1} matches (any order): {topn_matches_any_order[i] / ffwd_freqs.shape[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing well. Let's try a few more rounds at the same rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3902c4b2ea4a5ba64ce490b5edfeeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 499: train loss 88.2698, val loss 85.8976\n",
      "step 999: train loss 111.8359, val loss 105.2479\n",
      "step 1499: train loss 84.7038, val loss 85.9471\n",
      "step 1999: train loss 84.9063, val loss 85.6737\n"
     ]
    }
   ],
   "source": [
    "trainer.train(2000, optimizer, eval_interval=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It got lost but came back to a good spot. It hasn't made a ton of progress, though, so may be stuck. Let's try an even smaller learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5883fb58a404984807fbb402c8702c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 499: train loss 80.4703, val loss 81.8304\n",
      "step 999: train loss 80.1729, val loss 80.9812\n",
      "step 1499: train loss 79.5038, val loss 81.0121\n",
      "step 1999: train loss 79.6113, val loss 81.1012\n"
     ]
    }
   ],
   "source": [
    "# Go down one more order of magnitude\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.AdamW(m2.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train some more\n",
    "trainer.train(2000, optimizer, eval_interval=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going well. Let's let it run a bit more to see if there is more to be gained at this learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b033abf8b84109bd355f8213e8b018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 499: train loss 79.9726, val loss 80.9605\n",
      "step 999: train loss 79.8885, val loss 80.7976\n",
      "step 1499: train loss 79.4328, val loss 80.5349\n",
      "step 1999: train loss 79.6406, val loss 80.7106\n",
      "step 2499: train loss 78.8512, val loss 79.3027\n",
      "step 2999: train loss 92.4660, val loss 84.4240\n",
      "step 3499: train loss 88.8365, val loss 85.2693\n",
      "step 3999: train loss 89.9178, val loss 84.7041\n",
      "step 4499: train loss 89.9341, val loss 85.5066\n",
      "step 4999: train loss 90.7773, val loss 84.8867\n"
     ]
    }
   ],
   "source": [
    "# Train some more\n",
    "trainer.train(5000, optimizer, eval_interval=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That got better but then went off the rails. Let's go back to the best checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14500, tensor(78.8512), tensor(79.3027))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(checkpointer.output_dir / 'coeff_model_checkpoint_000028.pt')\n",
    "checkpoint['iters'], checkpoint['train_loss'], checkpoint['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.8330e-03]],\n",
       "\n",
       "        [[-1.5296e-03]],\n",
       "\n",
       "        [[-2.2343e-04]],\n",
       "\n",
       "        [[ 9.7601e-02]],\n",
       "\n",
       "        [[ 7.6696e-01]],\n",
       "\n",
       "        [[ 1.0677e-01]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.coeffs.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.802\n",
      "Top 1 matches (any order): 0.802\n",
      "Top 2 matches: 0.497\n",
      "Top 2 matches (any order): 0.566\n",
      "Top 3 matches: 0.344\n",
      "Top 3 matches (any order): 0.405\n",
      "Top 4 matches: 0.275\n",
      "Top 4 matches (any order): 0.335\n",
      "Top 5 matches: 0.207\n",
      "Top 5 matches (any order): 0.246\n",
      "Top 6 matches: 0.166\n",
      "Top 6 matches (any order): 0.196\n",
      "Top 7 matches: 0.127\n",
      "Top 7 matches (any order): 0.148\n",
      "Top 8 matches: 0.106\n",
      "Top 8 matches (any order): 0.123\n",
      "Top 9 matches: 0.085\n",
      "Top 9 matches (any order): 0.086\n",
      "Top 10 matches: 0.052\n",
      "Top 10 matches (any order): 0.055\n"
     ]
    }
   ],
   "source": [
    "total_probs, _ = m2(ffwd_freqs)\n",
    "topn_matches, topn_matches_any_order = analyze_simulate_results(total_probs, model_outputs)\n",
    "for i in range(10):\n",
    "    print(f\"Top {i+1} matches: {topn_matches[i] / ffwd_freqs.shape[1]:.3f}\")\n",
    "    print(f\"Top {i+1} matches (any order): {topn_matches_any_order[i] / ffwd_freqs.shape[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's nearly as good as the hand-rolled coefficients. Let's try a few more rounds at a smaller learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef7b322868940a5b1f6eadbd9012726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 499: train loss 79.0625, val loss 79.1941\n",
      "step 999: train loss 78.2563, val loss 79.1195\n",
      "step 1499: train loss 78.3124, val loss 78.8700\n",
      "step 1999: train loss 78.4172, val loss 79.0324\n",
      "step 2499: train loss 77.7076, val loss 78.5610\n",
      "step 2999: train loss 78.0883, val loss 77.9647\n",
      "step 3499: train loss 76.2281, val loss 76.7733\n",
      "step 3999: train loss 76.0516, val loss 77.5841\n",
      "step 4499: train loss 76.8096, val loss 77.2572\n",
      "step 4999: train loss 76.8454, val loss 77.1808\n"
     ]
    }
   ],
   "source": [
    "# Go down one more order of magnitude\n",
    "learning_rate = 3e-5\n",
    "optimizer = torch.optim.AdamW(m2.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train some more\n",
    "trainer.train(5000, optimizer, eval_interval=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some improvement! Not clear if it's stabilized or not, so let's keep going. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0715f6372e24d249e0515b2b1f53a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 499: train loss 76.4978, val loss 77.1828\n",
      "step 999: train loss 77.1313, val loss 77.3532\n",
      "step 1499: train loss 76.6558, val loss 77.2998\n",
      "step 1999: train loss 76.8978, val loss 77.4199\n",
      "step 2499: train loss 76.5466, val loss 77.0944\n",
      "step 2999: train loss 76.1288, val loss 76.6329\n",
      "step 3499: train loss 76.5751, val loss 77.8199\n",
      "step 3999: train loss 76.8902, val loss 77.1575\n",
      "step 4499: train loss 76.8581, val loss 77.5016\n",
      "step 4999: train loss 76.1585, val loss 77.7288\n"
     ]
    }
   ],
   "source": [
    "trainer.train(5000, optimizer, eval_interval=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, doesn't seem to be going anywhere. Let's go back to the best checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21000, tensor(76.0516), tensor(77.5841))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(checkpointer.output_dir / 'coeff_model_checkpoint_000041.pt')\n",
    "checkpoint['iters'], checkpoint['train_loss'], checkpoint['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.6905e-03]],\n",
       "\n",
       "        [[-1.5016e-03]],\n",
       "\n",
       "        [[ 2.6866e-04]],\n",
       "\n",
       "        [[ 1.2218e-01]],\n",
       "\n",
       "        [[ 8.1634e-01]],\n",
       "\n",
       "        [[ 7.6261e-05]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.coeffs.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.806\n",
      "Top 1 matches (any order): 0.806\n",
      "Top 2 matches: 0.501\n",
      "Top 2 matches (any order): 0.566\n",
      "Top 3 matches: 0.341\n",
      "Top 3 matches (any order): 0.402\n",
      "Top 4 matches: 0.270\n",
      "Top 4 matches (any order): 0.334\n",
      "Top 5 matches: 0.205\n",
      "Top 5 matches (any order): 0.244\n",
      "Top 6 matches: 0.163\n",
      "Top 6 matches (any order): 0.193\n",
      "Top 7 matches: 0.123\n",
      "Top 7 matches (any order): 0.148\n",
      "Top 8 matches: 0.103\n",
      "Top 8 matches (any order): 0.122\n",
      "Top 9 matches: 0.085\n",
      "Top 9 matches (any order): 0.088\n",
      "Top 10 matches: 0.053\n",
      "Top 10 matches (any order): 0.056\n"
     ]
    }
   ],
   "source": [
    "total_probs, _ = m2(ffwd_freqs)\n",
    "topn_matches, topn_matches_any_order = analyze_simulate_results(total_probs, model_outputs)\n",
    "for i in range(10):\n",
    "    print(f\"Top {i+1} matches: {topn_matches[i] / ffwd_freqs.shape[1]:.3f}\")\n",
    "    print(f\"Top {i+1} matches (any order): {topn_matches_any_order[i] / ffwd_freqs.shape[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really, really close to the best hand-rolled coefficients. It doesn't seem to be getting better so let's stop here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we initialize with the hand-rolled weights and see if it can improve that? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3 = ModelSim()\n",
    "m3.coeffs = torch.nn.Parameter(torch.tensor([0.01, 0.01, 0.1, 1.5, 6, 0.01]).unsqueeze(dim=1).unsqueeze(dim=2))\n",
    "m3.coeffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = results_folder / 'learn_coefficients'\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "checkpointer = CheckPointer(output_dir, 'coeff_model_hand_rolled_checkpoint', start_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=m3,\n",
    "    checkpointer=checkpointer,\n",
    "    get_batch_func=get_batch_func,\n",
    "    estimate_loss_func=estimate_loss_func,\n",
    "    iters_trained=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(77.4965), 'val': tensor(78.0544)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a starting point for the loss\n",
    "estimate_loss_func(m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3548daae1b49b39364d83e75c1451d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 499: train loss 76.9864, val loss 77.7938\n",
      "step 999: train loss 76.9611, val loss 77.3218\n"
     ]
    }
   ],
   "source": [
    "# Begin with a moderate learning rate\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.AdamW(m3.parameters(), lr=learning_rate)\n",
    "\n",
    "trainer.train(1000, optimizer, eval_interval=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very slightly better, but doesn't seem to be going anywhere. Let's reduce the learning rate and run for longer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46080e55ab48467d8e18ecb56f7ed2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 499: train loss 76.7579, val loss 77.3010\n",
      "step 999: train loss 76.2980, val loss 77.7475\n",
      "step 1499: train loss 76.2058, val loss 77.4439\n",
      "step 1999: train loss 76.5784, val loss 76.7124\n",
      "step 2499: train loss 76.6791, val loss 77.5712\n",
      "step 2999: train loss 76.7577, val loss 77.4296\n",
      "step 3499: train loss 76.3487, val loss 77.2223\n",
      "step 3999: train loss 76.9857, val loss 77.4700\n",
      "step 4499: train loss 77.6480, val loss 77.1776\n",
      "step 4999: train loss 76.2195, val loss 76.9608\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-5\n",
    "optimizer = torch.optim.AdamW(m3.parameters(), lr=learning_rate)\n",
    "\n",
    "trainer.train(5000, optimizer, eval_interval=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly better but does not seem to be improving. This is probably about as good as it gets. Let's see where we ended up. | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.4375e-02]],\n",
       "\n",
       "        [[-1.1588e-02]],\n",
       "\n",
       "        [[-6.7676e-04]],\n",
       "\n",
       "        [[ 1.2653e+00]],\n",
       "\n",
       "        [[ 6.1962e+00]],\n",
       "\n",
       "        [[ 4.2160e-04]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.coeffs.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.806\n",
      "Top 1 matches (any order): 0.806\n",
      "Top 2 matches: 0.501\n",
      "Top 2 matches (any order): 0.566\n",
      "Top 3 matches: 0.340\n",
      "Top 3 matches (any order): 0.402\n",
      "Top 4 matches: 0.271\n",
      "Top 4 matches (any order): 0.335\n",
      "Top 5 matches: 0.206\n",
      "Top 5 matches (any order): 0.245\n",
      "Top 6 matches: 0.163\n",
      "Top 6 matches (any order): 0.193\n",
      "Top 7 matches: 0.123\n",
      "Top 7 matches (any order): 0.147\n",
      "Top 8 matches: 0.104\n",
      "Top 8 matches (any order): 0.121\n",
      "Top 9 matches: 0.084\n",
      "Top 9 matches (any order): 0.087\n",
      "Top 10 matches: 0.053\n",
      "Top 10 matches (any order): 0.055\n"
     ]
    }
   ],
   "source": [
    "total_probs, _ = m3(ffwd_freqs)\n",
    "topn_matches, topn_matches_any_order = analyze_simulate_results(total_probs, model_outputs)\n",
    "for i in range(10):\n",
    "    print(f\"Top {i+1} matches: {topn_matches[i] / ffwd_freqs.shape[1]:.3f}\")\n",
    "    print(f\"Top {i+1} matches (any order): {topn_matches_any_order[i] / ffwd_freqs.shape[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar to what the model got to from random initialization. And also just a hair worse than the hand-rolled coefficients. There's probably not much more juice to squeeze out of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at Results for Strings Used in Training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_histories_dir = environment.data_root / 'model-training/20231112-training/batch_histories'\n",
    "test_eq(batch_histories_dir.exists(), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the training log, the final model we ended up using is from the checkpoint file `shakespeare_checkpoint_000008.pt`. And it was a straight succession of training runs to get to that point (no backtracking to intermediate checkpoints). So the relevant history is just all the batch histories up to the final checkpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_history_files = [\n",
    "    'batch_history_0000.pt',\n",
    "    'batch_history_0001.pt',\n",
    "    'batch_history_0002.pt',\n",
    "    'batch_history_0003.pt',\n",
    "    'batch_history_0004.pt',\n",
    "    'batch_history_0005.pt',\n",
    "    'batch_history_0006.pt',\n",
    "    'batch_history_0007.pt',\n",
    "    'batch_history_0008.pt',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4500, 64, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_history = torch.cat([\n",
    "    torch.load(batch_histories_dir / batch_history_file)['batch_history']\n",
    "    for batch_history_file in batch_history_files\n",
    "], dim=0)\n",
    "full_history.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process used a batch size of 64. So the shape above indicates we did 4500 iterations with 64 strings per iteration. The grouping into batches isn't meaningful, so we can just combine the first two dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([288000, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert full_history.shape[-1] == block_size\n",
    "full_history = full_history.reshape(-1, block_size)\n",
    "full_history.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The history data consists of tokens. Now  we turn those into strings.\n",
    "# We'll create a map of unique strings to the count of times they were\n",
    "# trained on.\n",
    "\n",
    "strings_trained_on = defaultdict(int)\n",
    "n_strings, _ = full_history.shape\n",
    "for i in range(n_strings):\n",
    "    string = encoding_helpers.stringify_tokens(full_history[i, :])\n",
    "    strings_trained_on[string] += 1\n",
    "strings_trained_on.default_factory = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250403"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(strings_trained_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221305, 221236, 780165)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-compute some things that will make later lookups faster\n",
    "\n",
    "# All the evaluation in this notebook is done on length 10 strings\n",
    "slen = 10\n",
    "\n",
    "# All length 10 strings that appeared at the start of any training string\n",
    "prefixes_trained_on = defaultdict(int)\n",
    "for string, count in strings_trained_on.items():\n",
    "    prefixes_trained_on[string[:slen]] += count\n",
    "prefixes_trained_on.default_factory = None\n",
    "\n",
    "# All length 10 strings that appeared at the end of any training string\n",
    "suffixes_trained_on = defaultdict(int)\n",
    "for string, count in strings_trained_on.items():\n",
    "    suffixes_trained_on[string[-slen:]] += count\n",
    "suffixes_trained_on.default_factory = None\n",
    "\n",
    "# All length 10 strings that appeared anywhere in any training string\n",
    "substrings_trained_on = defaultdict(int)\n",
    "for string, count in strings_trained_on.items():\n",
    "    for substring in all_unique_substrings(string, 10):\n",
    "        substrings_trained_on[substring] += count\n",
    "substrings_trained_on.default_factory = None\n",
    "\n",
    "len(prefixes_trained_on), len(suffixes_trained_on), len(substrings_trained_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_trained_on(\n",
    "    strings: Sequence[str],\n",
    "    trained_on: Dict[str, int],\n",
    ") -> Tuple[List[str], torch.Tensor]:\n",
    "    strings_used_in_training = []\n",
    "    indicies_used_in_training = []\n",
    "\n",
    "    for i, string in enumerate(strings):\n",
    "        if string in trained_on:\n",
    "            strings_used_in_training.append(string)\n",
    "            indicies_used_in_training.append(i)\n",
    "\n",
    "    return strings_used_in_training, torch.tensor(\n",
    "        indicies_used_in_training, dtype=torch.long\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_subset(\n",
    "    indices_to_consider: torch.Tensor,\n",
    "    model_outputs: Sequence[Sequence[Tuple[str, float]]],\n",
    "    ffwd_freqs: torch.Tensor,\n",
    "    coeffs: torch.Tensor,\n",
    "):\n",
    "    model_outputs_subset = []\n",
    "    for i in indices_to_consider:\n",
    "        model_outputs_subset.append(model_outputs[i.item()])\n",
    "\n",
    "    n_items = len(indices_to_consider)\n",
    "    total_freqs = (ffwd_freqs[:, indices_to_consider, :] * coeffs).sum(dim=0)\n",
    "    total_probs = total_freqs / total_freqs.sum(dim=-1, keepdim=True)\n",
    "    topn_matches, topn_matches_any_order = analyze_simulate_results(total_probs, model_outputs_subset)\n",
    "    for i in range(10):\n",
    "        print(f\"Top {i+1} matches: {topn_matches[i] / n_items:.3f}\")\n",
    "        print(f\"Top {i+1} matches (any order): {topn_matches_any_order[i] / n_items:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_rolled_coeffs = torch.tensor([0.01, 0.01, 0.1, 1.5, 4, 0.01]).unsqueeze(dim=1).unsqueeze(dim=2) # (n_layer, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5132 of 20000 strings used in training\n",
      "Top 1 matches: 0.809\n",
      "Top 1 matches (any order): 0.809\n",
      "Top 2 matches: 0.499\n",
      "Top 2 matches (any order): 0.569\n",
      "Top 3 matches: 0.337\n",
      "Top 3 matches (any order): 0.403\n",
      "Top 4 matches: 0.272\n",
      "Top 4 matches (any order): 0.334\n",
      "Top 5 matches: 0.208\n",
      "Top 5 matches (any order): 0.256\n",
      "Top 6 matches: 0.171\n",
      "Top 6 matches (any order): 0.204\n",
      "Top 7 matches: 0.130\n",
      "Top 7 matches (any order): 0.157\n",
      "Top 8 matches: 0.105\n",
      "Top 8 matches (any order): 0.128\n",
      "Top 9 matches: 0.088\n",
      "Top 9 matches (any order): 0.090\n",
      "Top 10 matches: 0.056\n",
      "Top 10 matches (any order): 0.059\n"
     ]
    }
   ],
   "source": [
    "# Prefixes\n",
    "strings_used_in_training, indices_used_in_training = filter_by_trained_on(\n",
    "    strings20k,\n",
    "    prefixes_trained_on,\n",
    ")\n",
    "print(f\"{len(strings_used_in_training)} of {len(strings20k)} strings used in training\")\n",
    "\n",
    "eval_on_subset(indices_used_in_training, model_outputs, ffwd_freqs, hand_rolled_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5132 of 20000 strings used in training\n",
      "Top 1 matches: 0.810\n",
      "Top 1 matches (any order): 0.810\n",
      "Top 2 matches: 0.498\n",
      "Top 2 matches (any order): 0.568\n",
      "Top 3 matches: 0.345\n",
      "Top 3 matches (any order): 0.402\n",
      "Top 4 matches: 0.274\n",
      "Top 4 matches (any order): 0.329\n",
      "Top 5 matches: 0.207\n",
      "Top 5 matches (any order): 0.246\n",
      "Top 6 matches: 0.170\n",
      "Top 6 matches (any order): 0.194\n",
      "Top 7 matches: 0.131\n",
      "Top 7 matches (any order): 0.157\n",
      "Top 8 matches: 0.108\n",
      "Top 8 matches (any order): 0.129\n",
      "Top 9 matches: 0.087\n",
      "Top 9 matches (any order): 0.090\n",
      "Top 10 matches: 0.061\n",
      "Top 10 matches (any order): 0.062\n"
     ]
    }
   ],
   "source": [
    "# Suffixes\n",
    "strings_used_in_training, indices_used_in_training = filter_by_trained_on(\n",
    "    strings20k,\n",
    "    suffixes_trained_on,\n",
    ")\n",
    "print(f\"{len(strings_used_in_training)} of {len(strings20k)} strings used in training\")\n",
    "\n",
    "eval_on_subset(indices_used_in_training, model_outputs, ffwd_freqs, hand_rolled_coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a huge coincidence that the number of the 20,000 eval strings that appeared as prefixes and the number that appeared as suffixes is 5132. Let's double check that these aren't exactly the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "strings_used_in_training_prefix, indices_used_in_training_prefix = filter_by_trained_on(\n",
    "    strings20k,\n",
    "    prefixes_trained_on,\n",
    ")\n",
    "strings_used_in_training_suffix, indices_used_in_training_suffix = filter_by_trained_on(\n",
    "    strings20k,\n",
    "    suffixes_trained_on,\n",
    ")\n",
    "\n",
    "test_eq(all(indices_used_in_training_prefix == indices_used_in_training_suffix), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These aren't the same 5132 strings, so maybe it is just a coincidence. Carrying on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18171 of 20000 strings used in training\n",
      "Top 1 matches: 0.809\n",
      "Top 1 matches (any order): 0.809\n",
      "Top 2 matches: 0.502\n",
      "Top 2 matches (any order): 0.570\n",
      "Top 3 matches: 0.344\n",
      "Top 3 matches (any order): 0.407\n",
      "Top 4 matches: 0.274\n",
      "Top 4 matches (any order): 0.338\n",
      "Top 5 matches: 0.210\n",
      "Top 5 matches (any order): 0.251\n",
      "Top 6 matches: 0.169\n",
      "Top 6 matches (any order): 0.198\n",
      "Top 7 matches: 0.129\n",
      "Top 7 matches (any order): 0.154\n",
      "Top 8 matches: 0.106\n",
      "Top 8 matches (any order): 0.125\n",
      "Top 9 matches: 0.087\n",
      "Top 9 matches (any order): 0.091\n",
      "Top 10 matches: 0.056\n",
      "Top 10 matches (any order): 0.058\n"
     ]
    }
   ],
   "source": [
    "# Substrings anywhere\n",
    "strings_used_in_training, indices_used_in_training = filter_by_trained_on(\n",
    "    strings20k,\n",
    "    substrings_trained_on,\n",
    ")\n",
    "print(f\"{len(strings_used_in_training)} of {len(strings20k)} strings used in training\")\n",
    "\n",
    "eval_on_subset(indices_used_in_training, model_outputs, ffwd_freqs, hand_rolled_coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "1. Most of the eval strings appeared somewhere in the training strings. \n",
    "2. Filtering down to eval strings that were just prefixes or suffixes of training strings greatly reduces the number of strings in the sample, and while this does give a small lift to the top 1 accuracy, it slightly reduces everything else. \n",
    "3. Considering the strings that appeared anywhere in the training strings very slightly lifts accuracy.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Characteristics of the Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n', 0.5250111222267151),\n",
       " (' ', 0.46654149889945984),\n",
       " ('-', 0.007815942168235779),\n",
       " (\"'\", 0.00047109558363445103),\n",
       " ('.', 2.7687412512023002e-05),\n",
       " (',', 1.7208614735864103e-05),\n",
       " (':', 1.5419789633597247e-05),\n",
       " ('?', 1.3534416211768985e-05),\n",
       " (';', 9.159703949990217e-06),\n",
       " ('l', 8.144122148223687e-06)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What do the model outputs look like?\n",
    "model_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_model_outputs(\n",
    "    model_outputs: Sequence[Sequence[Tuple[str, float]]],\n",
    "    filter_fn: Callable[[Sequence[Tuple[str, float]]], bool],\n",
    ") -> torch.Tensor:\n",
    "    matching_indices = []\n",
    "    for i, model_output in enumerate(model_outputs):\n",
    "        if filter_fn(model_output):\n",
    "            matching_indices.append(i)\n",
    "    return torch.tensor(matching_indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3057 of 20000 string\n",
      "Top 1 matches: 0.998\n",
      "Top 1 matches (any order): 0.998\n",
      "Top 2 matches: 0.349\n",
      "Top 2 matches (any order): 0.351\n",
      "Top 3 matches: 0.137\n",
      "Top 3 matches (any order): 0.149\n",
      "Top 4 matches: 0.060\n",
      "Top 4 matches (any order): 0.062\n",
      "Top 5 matches: 0.028\n",
      "Top 5 matches (any order): 0.026\n",
      "Top 6 matches: 0.020\n",
      "Top 6 matches (any order): 0.022\n",
      "Top 7 matches: 0.013\n",
      "Top 7 matches (any order): 0.012\n",
      "Top 8 matches: 0.011\n",
      "Top 8 matches (any order): 0.010\n",
      "Top 9 matches: 0.010\n",
      "Top 9 matches (any order): 0.008\n",
      "Top 10 matches: 0.005\n",
      "Top 10 matches (any order): 0.004\n"
     ]
    }
   ],
   "source": [
    "matching_indices = filter_model_outputs(\n",
    "    model_outputs,\n",
    "    lambda model_output: model_output[0][1] > 0.99,\n",
    ")\n",
    "print(f\"{len(matching_indices)} of {len(model_outputs)} string\")\n",
    "eval_on_subset(matching_indices, model_outputs, ffwd_freqs, hand_rolled_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10710 of 20000 string\n",
      "Top 1 matches: 0.936\n",
      "Top 1 matches (any order): 0.936\n",
      "Top 2 matches: 0.601\n",
      "Top 2 matches (any order): 0.652\n",
      "Top 3 matches: 0.367\n",
      "Top 3 matches (any order): 0.421\n",
      "Top 4 matches: 0.256\n",
      "Top 4 matches (any order): 0.318\n",
      "Top 5 matches: 0.148\n",
      "Top 5 matches (any order): 0.182\n",
      "Top 6 matches: 0.110\n",
      "Top 6 matches (any order): 0.136\n",
      "Top 7 matches: 0.082\n",
      "Top 7 matches (any order): 0.098\n",
      "Top 8 matches: 0.069\n",
      "Top 8 matches (any order): 0.087\n",
      "Top 9 matches: 0.061\n",
      "Top 9 matches (any order): 0.069\n",
      "Top 10 matches: 0.023\n",
      "Top 10 matches (any order): 0.022\n"
     ]
    }
   ],
   "source": [
    "matching_indices = filter_model_outputs(\n",
    "    model_outputs,\n",
    "    lambda model_output: sum([model_output[i][1] for i in range(2)]) > 0.85,\n",
    ")\n",
    "print(f\"{len(matching_indices)} of {len(model_outputs)} string\")\n",
    "eval_on_subset(matching_indices, model_outputs, ffwd_freqs, hand_rolled_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10710 of 20000 string\n",
      "Top 1 matches: 0.939\n",
      "Top 1 matches (any order): 0.939\n",
      "Top 2 matches: 0.603\n",
      "Top 2 matches (any order): 0.652\n",
      "Top 3 matches: 0.366\n",
      "Top 3 matches (any order): 0.414\n",
      "Top 4 matches: 0.252\n",
      "Top 4 matches (any order): 0.315\n",
      "Top 5 matches: 0.145\n",
      "Top 5 matches (any order): 0.178\n",
      "Top 6 matches: 0.106\n",
      "Top 6 matches (any order): 0.133\n",
      "Top 7 matches: 0.077\n",
      "Top 7 matches (any order): 0.095\n",
      "Top 8 matches: 0.066\n",
      "Top 8 matches (any order): 0.085\n",
      "Top 9 matches: 0.061\n",
      "Top 9 matches (any order): 0.068\n",
      "Top 10 matches: 0.023\n",
      "Top 10 matches (any order): 0.022\n"
     ]
    }
   ],
   "source": [
    "custom_coeffs = torch.tensor([0.1, 0.1, 15, 10, 1000, 0.1]).unsqueeze(dim=1).unsqueeze(dim=2) # (n_layer, 1, 1)\n",
    "matching_indices = filter_model_outputs(\n",
    "    model_outputs,\n",
    "    lambda model_output: sum([model_output[i][1] for i in range(2)]) > 0.85,\n",
    ")\n",
    "print(f\"{len(matching_indices)} of {len(model_outputs)} string\")\n",
    "eval_on_subset(matching_indices, model_outputs, ffwd_freqs, custom_coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at some specific examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.95, 0.94, 0.85, 0.76, 0.81, 0.89]\n",
    "block_idx = 3\n",
    "q_idx_start = 0\n",
    "q_idx_end = 10\n",
    "matching_strings = []\n",
    "for block_idx in range(n_layer):\n",
    "    matching_strings.append(\n",
    "        get_matching_strings(\n",
    "            filter_on_prefiltered_results(\n",
    "                load_prefiltered=lambda q_idx: torch.load(prefiltered_filename(q_idx, block_idx)),\n",
    "                q_idx_start=q_idx_start,\n",
    "                q_idx_end=q_idx_end,\n",
    "                filter_fn=lambda values: values > thresholds[block_idx],\n",
    "            ),\n",
    "            strings10,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(q_idx: int, query_strings, matching_strings):\n",
    "    print(f\"Query string: {repr(query_strings[q_idx])[1:-1]}\")\n",
    "    max_len = max(\n",
    "        [len(matching_strings[block_idx][q_idx]) for block_idx in range(n_layer)]\n",
    "    )\n",
    "    for i in range(max_len):\n",
    "        line_items = [\n",
    "            matching_strings[block_idx][q_idx][i]\n",
    "            if i < len(matching_strings[block_idx][q_idx])\n",
    "            else '          '\n",
    "            for block_idx in range(n_layer)\n",
    "        ]\n",
    "        print(' | '.join([f'{repr(line_item)[1:-1]:>15}' for line_item in line_items]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query string: s eyes may\n",
      "     h, you may |      h, you may |      h, you may |      swords may |      swords may |      swords may\n",
      "     le: it may |      le: it may |      le: it may |       craft may |       craft may |       craft may\n",
      "     you, I may |      you, I may |      swords may |      office may |      office may |       if it may\n",
      "     swords may |      swords may |      at you may |      itness may |      ods he may |      itness may\n",
      "     at you may |      at you may |     ering,\\nMay |      rvices may |      itness may |      rvices may\n",
      "     hom we may |      hom we may |      hom we may |      r sort may |      rvices may |       souls may\n",
      "      craft may |       craft may |       craft may |       grant may |      pliant may |      hat it may\n",
      "    f it\\nI may |     f it\\nI may |      office may |      pliant may |     erhaps\\nMay |      but it may\n",
      "     office may |      office may |      e that may |       never may |       souls may |      ourses may\n",
      "     e that may |      e that may |      s: you may |      le joy may |       haply may |      boughs may\n",
      "     That I may |      That I may |      y, you may |       souls may |       grace may |      n pity may\n",
      "    en:\\nWe may |     en:\\nWe may |       if it may |       haply may |       alive may |       sense may\n",
      "     s: you may |      s: you may |      ods he may |      e lord may |      ourses may |      s eyes may\n",
      "     y, you may |      y, you may |      'd; we may |       grace may |      tongue may |      e case may\n",
      "      if it may |       if it may |      t thou may |      growth may |      tments may |       as it may\n",
      "     that I may |      that I may |      itness may |       sword may |      boughs may |      y oath may\n",
      "    you,\\nI may |     you,\\nI may |      rvices may |       alive may |      esence may |      heaven may\n",
      "    S:\\nYou may |     S:\\nYou may |       sort, may |      ourses may |      n pity may |      al man may\n",
      "     ods he may |      ods he may |      r sort may |      tongue may |      years\\nMay |      nd men may\n",
      "     'd; we may |      'd; we may |     l time\\nMay |      tments may |      rosper may |      reason may\n",
      "    :\\nWhat may |     :\\nWhat may |       grant may |      boughs may |     ,\\nPity may |      awares may\n",
      "    ple,\\nI may |     ple,\\nI may |      blame\\nMay |      aft'st may |      taking may |      bjects may\n",
      "     r: you may |      r: you may |     olsces\\nMay |      esence may |     \\nWomen may |      nd now may\n",
      "     t thou may |      t thou may |      other, may |     of men\\nMay |      liance may |      r foes may\n",
      "     itness may |      itness may |      ence?  may |      n pity may |       write may |       bones may\n",
      "     rvices may |      rvices may |     y,\\nYou may |     ,\\nPity may |      e jest may |       women may\n",
      "      sort, may |       sort, may |      honour may |      ounsel may |       lover may |      a time may\n",
      "     r sort may |      r sort may |     aspect\\nMay |     \\nWomen may |       sense may |      bility may\n",
      "    r:\\nYou may |     r:\\nYou may |     o\\nThou may |      liance may |      s eyes may |      ffairs may\n",
      "      grant may |       grant may |      pliant may |      e jest may |     es\\nAnd may |      inkers may\n",
      "     other, may |      other, may |       never may |       lover may |      en and may |      tleman may\n",
      "     ence?  may |      ence?  may |     S:\\nShe may |       sense may |     \\nFlies may |      y life may\n",
      "    y,\\nYou may |     y,\\nYou may |     R:\\nShe may |      s eyes may |      othing may |      at men may\n",
      "     honour may |      honour may |     o?\\nShe may |      en and may |      e case may |      nd, it may\n",
      "    o\\nThou may |     o\\nThou may |     t:\\nShe may |       Romeo may |       taker may |      th him may\n",
      "     pliant may |      pliant may |      t? She may |     \\nFlies may |      y oath may |      e time may\n",
      "     eby he may |      eby he may |      le joy may |      e case may |     \\nNe'er may |      ctions may\n",
      "      never may |       never may |     !\\nLong may |       taker may |      heaven may |      n time may\n",
      "    R:\\nYou may |     R:\\nYou may |      of you may |      ul war may |      nd men may |       woman may\n",
      "    S:\\nShe may |     S:\\nShe may |      hat we may |       as it may |      partly may |      ied as may\n",
      "    R:\\nShe may |     R:\\nShe may |     erhaps\\nMay |      y oath may |      branch may |      angers may\n",
      "    o?\\nShe may |     o?\\nShe may |       souls may |      o York may |      reason may |      tripes may\n",
      "    t:\\nShe may |     t:\\nShe may |      moon,\\nMay |     \\nNe'er may |     ems\\nAs may |                \n",
      "    .\\nWhat may |     .\\nWhat may |     n,\\nAnd may |      heaven may |      awares may |                \n",
      "     t? She may |      t? She may |     h.\\nAll may |      al man may |      bjects may |                \n",
      "     marry, may |      marry, may |      te you may |      o both may |      prayer may |                \n",
      "     le joy may |      le joy may |      d that may |      nd men may |      nd now may |                \n",
      "    !\\nLong may |     !\\nLong may |     \\nwhich may |      partly may |      master may |                \n",
      "     of you may |      of you may |      lords, may |      reason may |       crown may |                \n",
      "     hat we may |      hat we may |     :\\nLord may |      awares may |      r foes may |                \n",
      "      souls may |       souls may |       haply may |      bjects may |       bones may |                \n",
      "    n,\\nAnd may |     n,\\nAnd may |      d lord may |      prayer may |       women may |                \n",
      "    h.\\nAll may |     h.\\nAll may |      hat it may |      nd now may |      a time may |                \n",
      "     d, the may |      d, the may |      e lord may |      master may |      bility may |                \n",
      "     te you may |      te you may |      nd you may |       crown may |      chance may |                \n",
      "    .\\nIf I may |     .\\nIf I may |      heart\\nMay |      r foes may |       I now may |                \n",
      "    ER:\\nHe may |     ER:\\nHe may |      r ever may |       bones may |      ffairs may |                \n",
      "     heed I may |      heed I may |       grace may |       heart may |      inkers may |                \n",
      "    rds\\nWe may |     rds\\nWe may |      growth may |       women may |      eating may |                \n",
      "     d that may |      d that may |      So she may |      a time may |      iction may |                \n",
      "    \\nwhich may |     \\nwhich may |       which may |      bility may |       there may |                \n",
      "    y:\\nYou may |     y:\\nYou may |      but it may |      chance may |      tleman may |                \n",
      "     lords, may |      lords, may |      n thou may |       I now may |      y life may |                \n",
      "     gs the may |      gs the may |      t they may |      ffairs may |       haste may |                \n",
      "    :\\nLord may |     :\\nLord may |      ou, we may |      inkers may |      faults may |                \n",
      "      haply may |       haply may |      at she may |      iction may |      th him may |                \n",
      "     d lord may |      d lord may |       sword may |       there may |      e time may |                \n",
      "    m.\\nThe may |     m.\\nThe may |       alive may |      tleman may |      are he may |                \n",
      "     hat it may |      hat it may |       alas, may |      y life may |      r they may |                \n",
      "     'd the may |      'd the may |     ,\\nThey may |      ay she may |      ctions may |                \n",
      "     ot the may |      ot the may |      ent we may |      at men may |      n time may |                \n",
      "    M:\\nThe may |     M:\\nThe may |      God we may |      faults may |      enants may |                \n",
      "     e lord may |      e lord may |      d tale may |      th him may |       woman may |                \n",
      "     f, the may |      f, the may |      ourses may |      e time may |      ds she may |                \n",
      "     nd you may |      nd you may |      ll, we may |      ctions may |      ied as may |                \n",
      "    nce,\\nI may |     nce,\\nI may |      ng, it may |      ven so may |      angers may |                \n",
      "      no; I may |       no; I may |     RK:\\nIt may |      n time may |      tripes may |                \n",
      "     r ever may |      r ever may |      tongue may |     esence\\nMay |                 |                \n",
      "      grace may |       grace may |     ay:\\nSo may |      enants may |                 |                \n",
      "     growth may |      growth may |      tments may |       woman may |                 |                \n",
      "    r,\\nThe may |     r,\\nThe may |      l that may |      ds she may |                 |                \n",
      "      how I may |       how I may |     ' feet\\nMay |      and so may |                 |                \n",
      "     So she may |      So she may |      boughs may |      ied as may |                 |                \n",
      "      which may |       which may |      aft'st may |      angers may |                 |                \n",
      "     but it may |      but it may |      o not, may |      tripes may |                 |                \n",
      "     n thou may |      n thou may |     ies\\nAs may |                 |                 |                \n",
      "     , as I may |      , as I may |      esence may |                 |                 |                \n",
      "     side I may |      side I may |     of men\\nMay |                 |                 |                \n",
      "     t they may |      t they may |      n pity may |                 |                 |                \n",
      "     ou, we may |      ou, we may |      years\\nMay |                 |                 |                \n",
      "     at she may |      at she may |      rosper may |                 |                 |                \n",
      "      sword may |       sword may |     e.\\nIll may |                 |                 |                \n",
      "      alive may |       alive may |     ,\\nPity may |                 |                 |                \n",
      "      for I may |       for I may |      nails\\nMay |                 |                 |                \n",
      "      alas, may |       alas, may |      taking may |                 |                 |                \n",
      "    ,\\nThey may |     ,\\nThey may |      ounsel may |                 |                 |                \n",
      "     ent we may |      ent we may |      Ere we may |                 |                 |                \n",
      "     God we may |      God we may |     ng one\\nMay |                 |                 |                \n",
      "     d tale may |      d tale may |       trick may |                 |                 |                \n",
      "     ourses may |      ourses may |      e thou may |                 |                 |                \n",
      "    EN:\\nIt may |     EN:\\nIt may |     reath,\\nMay |                 |                 |                \n",
      "     ll, we may |      ll, we may |      e, and may |                 |                 |                \n",
      "     ng, it may |      ng, it may |     \\nWomen may |                 |                 |                \n",
      "    RK:\\nIt may |     RK:\\nIt may |      liance may |                 |                 |                \n",
      "     tongue may |      tongue may |       write may |                 |                 |                \n",
      "    ay:\\nSo may |     ay:\\nSo may |      e jest may |                 |                 |                \n",
      "     tments may |      tments may |     y,\\nTwo may |                 |                 |                \n",
      "     l that may |      l that may |       lover may |                 |                 |                \n",
      "     h you; may |      h you; may |       sense may |                 |                 |                \n",
      "     boughs may |      boughs may |      s eyes may |                 |                 |                \n",
      "     aft'st may |      aft'st may |     es\\nAnd may |                 |                 |                \n",
      "     o not, may |      o not, may |      en and may |                 |                 |                \n",
      "    ies\\nAs may |     ies\\nAs may |       Romeo may |                 |                 |                \n",
      "    \\nIf he may |     \\nIf he may |     \\nFlies may |                 |                 |                \n",
      "     esence may |      esence may |     te,\\nIt may |                 |                 |                \n",
      "    iew\\nHe may |     iew\\nHe may |     y\\nThat may |                 |                 |                \n",
      "    e:\\nYou may |     e:\\nYou may |     alone,\\nMay |                 |                 |                \n",
      "     n pity may |      n pity may |     :\\nThat may |                 |                 |                \n",
      "      me; I may |       me; I may |      othing may |                 |                 |                \n",
      "      a man may |       a man may |      th, we may |                 |                 |                \n",
      "     rosper may |      rosper may |      e case may |                 |                 |                \n",
      "    e.\\nIll may |     e.\\nIll may |       taker may |                 |                 |                \n",
      "    ,\\nPity may |     ,\\nPity may |       trunk may |                 |                 |                \n",
      "     taking may |      taking may |     ing it\\nMay |                 |                 |                \n",
      "     ounsel may |      ounsel may |      O, how may |                 |                 |                \n",
      "     here I may |      here I may |       if he may |                 |                 |                \n",
      "     Ere we may |      Ere we may |      ul war may |                 |                 |                \n",
      "    :\\nWhy, may |       trick may |       as it may |                 |                 |                \n",
      "      trick may |      oe, he may |      ay; we may |                 |                 |                \n",
      "     oe, he may |     ,\\nThou may |      venged may |                 |                 |                \n",
      "    ,\\nThou may |      e thou may |      y oath may |                 |                 |                \n",
      "     e thou may |      e, and may |      oenix, may |                 |                 |                \n",
      "     e, and may |     \\nWomen may |      o York may |                 |                 |                \n",
      "    \\nWomen may |      liance may |     beside\\nMay |                 |                 |                \n",
      "     liance may |       write may |     \\nNe'er may |                 |                 |                \n",
      "      write may |      e jest may |      usband may |                 |                 |                \n",
      "     e jest may |     y,\\nTwo may |      heaven may |                 |                 |                \n",
      "    y,\\nTwo may |      ough I may |     :\\nThis may |                 |                 |                \n",
      "     ough I may |       lover may |      al man may |                 |                 |                \n",
      "      lover may |       sense may |     fight,\\nMay |                 |                 |                \n",
      "      sense may |      s eyes may |     m now,\\nMay |                 |                 |                \n",
      "     s eyes may |     es\\nAnd may |      o both may |                 |                 |                \n",
      "    es\\nAnd may |      en and may |      rgaret may |                 |                 |                \n",
      "     en and may |       Romeo may |      nd men may |                 |                 |                \n",
      "      Romeo may |     \\nFlies may |      partly may |                 |                 |                \n",
      "    \\nFlies may |     te,\\nIt may |      branch may |                 |                 |                \n",
      "    te,\\nIt may |     y\\nThat may |      reason may |                 |                 |                \n",
      "    y\\nThat may |      hich I may |     ems\\nAs may |                 |                 |                \n",
      "     hich I may |     :\\nThat may |     ak,\\nAs may |                 |                 |                \n",
      "    :\\nThat may |      when I may |      awares may |                 |                 |                \n",
      "     when I may |     ET:\\nIt may |      as you may |                 |                 |                \n",
      "    ET:\\nIt may |      othing may |      arwick may |                 |                 |                \n",
      "     othing may |      ll, he may |      ile we may |                 |                 |                \n",
      "     ll, he may |      th, we may |      bjects may |                 |                 |                \n",
      "     th, we may |      e case may |      prayer may |                 |                 |                \n",
      "     e case may |     :\\nIf I may |     d land\\nMay |                 |                 |                \n",
      "    :\\nIf I may |       taker may |      nd now may |                 |                 |                \n",
      "      taker may |       trunk may |     s\\nWhat may |                 |                 |                \n",
      "      trunk may |      O, how may |      master may |                 |                 |                \n",
      "     O, how may |       if he may |       crown may |                 |                 |                \n",
      "      if he may |      ul war may |       as we may |                 |                 |                \n",
      "     ul war may |       as it may |      r foes may |                 |                 |                \n",
      "      as it may |      ay; we may |       bones may |                 |                 |                \n",
      "     ay; we may |      venged may |      : long may |                 |                 |                \n",
      "     venged may |      y oath may |       heart may |                 |                 |                \n",
      "     y oath may |      oenix, may |     ience,\\nmay |                 |                 |                \n",
      "     oenix, may |      o York may |      f what may |                 |                 |                \n",
      "     o York may |     \\nNe'er may |      ; that may |                 |                 |                \n",
      "    \\nNe'er may |      usband may |     t\\nThou may |                 |                 |                \n",
      "     usband may |      heaven may |      p, who may |                 |                 |                \n",
      "     heaven may |     :\\nThis may |      r that may |                 |                 |                \n",
      "    :\\nThis may |      al man may |      s, you may |                 |                 |                \n",
      "     al man may |      o both may |       There may |                 |                 |                \n",
      "     o both may |      rgaret may |      nd one may |                 |                 |                \n",
      "     rgaret may |      nd men may |       women may |                 |                 |                \n",
      "     nd men may |      partly may |     orlorn\\nMay |                 |                 |                \n",
      "     partly may |      branch may |      How he may |                 |                 |                \n",
      "     branch may |      reason may |      a time may |                 |                 |                \n",
      "     reason may |     ems\\nAs may |      bility may |                 |                 |                \n",
      "    ems\\nAs may |     ak,\\nAs may |      chance may |                 |                 |                \n",
      "    ak,\\nAs may |     iage\\nI may |     e dead\\nMay |                 |                 |                \n",
      "    iage\\nI may |     rd,\\nWe may |     \\nWhich may |                 |                 |                \n",
      "    rd,\\nWe may |      awares may |      d what may |                 |                 |                \n",
      "     awares may |      as you may |     !\\nWell may |                 |                 |                \n",
      "     as you may |      arwick may |      f thou may |                 |                 |                \n",
      "     arwick may |      ile we may |       I now may |                 |                 |                \n",
      "     ile we may |      bjects may |      ffairs may |                 |                 |                \n",
      "     bjects may |      prayer may |      inkers may |                 |                 |                \n",
      "     prayer may |      nd now may |      I well may |                 |                 |                \n",
      "     nd now may |     s\\nWhat may |      eating may |                 |                 |                \n",
      "    s\\nWhat may |      master may |      p that may |                 |                 |                \n",
      "     master may |       crown may |      is you may |                 |                 |                \n",
      "      crown may |       as we may |      If you may |                 |                 |                \n",
      "      as we may |      r foes may |     roject\\nMay |                 |                 |                \n",
      "     r foes may |       bones may |      re you may |                 |                 |                \n",
      "      bones may |      : long may |     millo,\\nMay |                 |                 |                \n",
      "     : long may |       heart may |      iction may |                 |                 |                \n",
      "      heart may |      f what may |      and we may |                 |                 |                \n",
      "    th!\\nO, may |      ; that may |     w\\nthat may |                 |                 |                \n",
      "     f what may |     NES:\\nI may |       there may |                 |                 |                \n",
      "     ; that may |     t\\nThou may |     issue,\\nMay |                 |                 |                \n",
      "    NES:\\nI may |     ord,\\nI may |     d:\\nYou may |                 |                 |                \n",
      "    t\\nThou may |      p, who may |      tleman may |                 |                 |                \n",
      "    ord,\\nI may |      r that may |      y life may |                 |                 |                \n",
      "     p, who may |      s, you may |      fancy\\nMay |                 |                 |                \n",
      "    LLO:\\nI may |       There may |      ere we may |                 |                 |                \n",
      "     r that may |      nd one may |       haste may |                 |                 |                \n",
      "     s, you may |       women may |      And we may |                 |                 |                \n",
      "      There may |     ler:\\nI may |      , this may |                 |                 |                \n",
      "     nd one may |      How he may |      love,\\nmay |                 |                 |                \n",
      "      women may |      a time may |      ay she may |                 |                 |                \n",
      "    ler:\\nI may |      bility may |     e;\\nWho may |                 |                 |                \n",
      "     How he may |      chance may |     m;\\nYou may |                 |                 |                \n",
      "     a time may |     \\nWhich may |      life,\\nMay |                 |                 |                \n",
      "     bility may |      d what may |      asure; may |                 |                 |                \n",
      "     chance may |     !\\nWell may |      at men may |                 |                 |                \n",
      "    \\nWhich may |      f thou may |      odesty may |                 |                 |                \n",
      "     d what may |       I now may |     O:\\nYet may |                 |                 |                \n",
      "    !\\nWell may |      ffairs may |      nd, it may |                 |                 |                \n",
      "     f thou may |      inkers may |      faults may |                 |                 |                \n",
      "      I now may |      I well may |      r, you may |                 |                 |                \n",
      "     ffairs may |      eating may |      th him may |                 |                 |                \n",
      "     inkers may |     ,\\nIf I may |      e time may |                 |                 |                \n",
      "     I well may |      p that may |      er, it may |                 |                 |                \n",
      "     eating may |      is you may |     hat\\nit may |                 |                 |                \n",
      "     d you; may |      If you may |      are he may |                 |                 |                \n",
      "    ,\\nIf I may |      re you may |      uance, may |                 |                 |                \n",
      "     p that may |      iction may |      r they may |                 |                 |                \n",
      "     is you may |      and we may |     e!\\nHow may |                 |                 |                \n",
      "     If you may |      who, I may |      k, how may |                 |                 |                \n",
      "     re you may |      , if I may |      ctions may |                 |                 |                \n",
      "     iction may |     w\\nthat may |      ven so may |                 |                 |                \n",
      "     and we may |       there may |      n time may |                 |                 |                \n",
      "     who, I may |     rd:\\nWe may |      d, she may |                 |                 |                \n",
      "     , if I may |     d:\\nYou may |      And he may |                 |                 |                \n",
      "    w\\nthat may |      tleman may |      ad: so may |                 |                 |                \n",
      "      there may |      y life may |      t, you may |                 |                 |                \n",
      "    rd:\\nWe may |      ere we may |     e wife\\nMay |                 |                 |                \n",
      "    d:\\nYou may |       haste may |     esence\\nMay |                 |                 |                \n",
      "     tleman may |      And we may |      ! thou may |                 |                 |                \n",
      "     y life may |      , this may |      a, you may |                 |                 |                \n",
      "     ere we may |      ay she may |      but we may |                 |                 |                \n",
      "      haste may |     e;\\nWho may |      vato,' may |                 |                 |                \n",
      "     And we may |     \\nHow I may |     m:\\nshe may |                 |                 |                \n",
      "     , this may |     m;\\nYou may |     s,\\nShe may |                 |                 |                \n",
      "     ay she may |      may, I may |      e more may |                 |                 |                \n",
      "    e;\\nWho may |     t.\\nYou may |       ye we may |                 |                 |                \n",
      "    \\nHow I may |      asure; may |      enants may |                 |                 |                \n",
      "    m;\\nYou may |      at men may |     :\\nWell may |                 |                 |                \n",
      "     may, I may |      odesty may |     r\\nIron may |                 |                 |                \n",
      "    t.\\nYou may |     O:\\nYet may |     NA:\\nSo may |                 |                 |                \n",
      "     asure; may |      nd, it may |      o? how may |                 |                 |                \n",
      "     at men may |      er, he may |     y;\\nYou may |                 |                 |                \n",
      "     odesty may |      faults may |       woman may |                 |                 |                \n",
      "    O:\\nYet may |      r, you may |      , thou may |                 |                 |                \n",
      "     nd, it may |      th him may |     A:\\nAnd may |                 |                 |                \n",
      "     er, he may |      e time may |      ell we may |                 |                 |                \n",
      "     faults may |      er, it may |      ptista may |                 |                 |                \n",
      "     r, you may |      are he may |      ds she may |                 |                 |                \n",
      "     th him may |      uance, may |      and so may |                 |                 |                \n",
      "     e time may |      r they may |      at hap may |                 |                 |                \n",
      "     er, it may |      , what may |      ied as may |                 |                 |                \n",
      "    hat\\nit may |     e!\\nHow may |      angers may |                 |                 |                \n",
      "     are he may |      k, how may |      ready; may |                 |                 |                \n",
      "     uance, may |      elo? I may |      tripes may |                 |                 |                \n",
      "     r they may |      ctions may |      so you may |                 |                 |                \n",
      "     , what may |      rning, may |                 |                 |                 |                \n",
      "    e!\\nHow may |      ven so may |                 |                 |                 |                \n",
      "     ittle: may |      n time may |                 |                 |                 |                \n",
      "     k, how may |      d, she may |                 |                 |                 |                \n",
      "     elo? I may |      And he may |                 |                 |                 |                \n",
      "     ctions may |      ad: so may |                 |                 |                 |                \n",
      "     rning, may |      t, you may |                 |                 |                 |                \n",
      "     ven so may |      ! thou may |                 |                 |                 |                \n",
      "    IO:\\nIt may |      a, you may |                 |                 |                 |                \n",
      "     n time may |      rust I may |                 |                 |                 |                \n",
      "     d, she may |      o too, may |                 |                 |                 |                \n",
      "     And he may |     O:\\nYou may |                 |                 |                 |                \n",
      "     ad: so may |      but we may |                 |                 |                 |                \n",
      "     t, you may |      vato,' may |                 |                 |                 |                \n",
      "     ! thou may |      best I may |                 |                 |                 |                \n",
      "     a, you may |     m:\\nshe may |                 |                 |                 |                \n",
      "     rust I may |      t so I may |                 |                 |                 |                \n",
      "     o too, may |      . If I may |                 |                 |                 |                \n",
      "    O:\\nYou may |     s,\\nShe may |                 |                 |                 |                \n",
      "     but we may |      e more may |                 |                 |                 |                \n",
      "     It is: may |       ye we may |                 |                 |                 |                \n",
      "     vato,' may |      ? what may |                 |                 |                 |                \n",
      "     best I may |     age,\\nI may |                 |                 |                 |                \n",
      "    m:\\nshe may |      enants may |                 |                 |                 |                \n",
      "     t so I may |     :\\nWell may |                 |                 |                 |                \n",
      "     . If I may |     r\\nIron may |                 |                 |                 |                \n",
      "    s,\\nShe may |     NA:\\nSo may |                 |                 |                 |                \n",
      "     e more may |     O:\\nAnd may |                 |                 |                 |                \n",
      "      ye we may |      time I may |                 |                 |                 |                \n",
      "     ? what may |      o? how may |                 |                 |                 |                \n",
      "    age,\\nI may |     y;\\nYou may |                 |                 |                 |                \n",
      "     enants may |       woman may |                 |                 |                 |                \n",
      "    :\\nWell may |      , thou may |                 |                 |                 |                \n",
      "    r\\nIron may |     A:\\nAnd may |                 |                 |                 |                \n",
      "    NA:\\nSo may |      ell we may |                 |                 |                 |                \n",
      "    O:\\nAnd may |      ptista may |                 |                 |                 |                \n",
      "     time I may |      ds she may |                 |                 |                 |                \n",
      "     o? how may |      and so may |                 |                 |                 |                \n",
      "    y;\\nYou may |      at hap may |                 |                 |                 |                \n",
      "      woman may |      ied as may |                 |                 |                 |                \n",
      "     , thou may |      Hope I may |                 |                 |                 |                \n",
      "    A:\\nAnd may |      angers may |                 |                 |                 |                \n",
      "     ell we may |      ready; may |                 |                 |                 |                \n",
      "     ptista may |      tripes may |                 |                 |                 |                \n",
      "     ds she may |      ir, he may |                 |                 |                 |                \n",
      "     and so may |      so you may |                 |                 |                 |                \n",
      "    TIO:\\nI may |                 |                 |                 |                 |                \n",
      "     at hap may |                 |                 |                 |                 |                \n",
      "     ied as may |                 |                 |                 |                 |                \n",
      "     Hope I may |                 |                 |                 |                 |                \n",
      "     angers may |                 |                 |                 |                 |                \n",
      "     ready; may |                 |                 |                 |                 |                \n",
      "     tripes may |                 |                 |                 |                 |                \n",
      "     ir, he may |                 |                 |                 |                 |                \n",
      "     so you may |                 |                 |                 |                 |                \n"
     ]
    }
   ],
   "source": [
    "print_results(q_idx=2, query_strings=strings20k, matching_strings=matching_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query string: e,\\nplease \n",
      "    to\\nplease  |     to\\nplease  |     to\\nplease  |     to\\nplease  |     to\\nplease  |     to\\nplease \n",
      "      increase  |       increase  |     t\\ndisease  |       increase  |      ather see  |     e,\\nplease \n",
      "    t\\ndisease  |     t\\ndisease  |     e,\\nplease  |     t\\ndisease  |     t\\ndisease  |      't please \n",
      "     hese base  |      why cease  |      e: please  |      r, praise  |     e,\\nplease  |      ou please \n",
      "     why cease  |      d. Please  |      't please  |      d. Please  |      ll praise  |      ld please \n",
      "     d. Please  |     e,\\nplease  |      ou please  |      es praise  |      e: please  |      t appease \n",
      "    e,\\nplease  |      t. Please  |      ld please  |      do repose  |      o despise  |      as please \n",
      "     t. Please  |     d\\nTo ease  |      t appease  |      t purpose  |      e disease  |      ip please \n",
      "    d\\nTo ease  |      e: please  |      as please  |     e,\\nplease  |      let's see  |      ht please \n",
      "     e: please  |      e disease  |      ip please  |      at passes  |      I purpose  |      it please \n",
      "     e disease  |      a disease  |      ht please  |      t. Please  |      e, advise  |      to please \n",
      "     a disease  |       surcease  |      it please  |      ll praise  |      't please  |      so please \n",
      "     with base  |      f. Please  |      to please  |     d\\nTo ease  |      to excuse  |      st please \n",
      "      surcease  |      't please  |     n:\\nPlease  |      e: please  |      No excuse  |      od please \n",
      "     ch a case  |      ou please  |      so please  |      To oppose  |      ou please  |     r,\\nPlease \n",
      "     heir base  |      ld please  |      To please  |      e disease  |      ld please  |      ho please \n",
      "     f. Please  |      t appease  |      st please  |       surcease  |       duty see  |      at please \n",
      "     't please  |       purchase  |     \\nTo cease  |      f. Please  |     ;\\nAnd see  |      ll please \n",
      "     ou please  |      as please  |      So please  |      't please  |      ; and see  |      ay please \n",
      "     ld please  |      ip please  |     t:\\nPlease  |     \\nProvokes  |      t appease  |      er please \n",
      "     et debase  |      ht please  |      od please  |      to excuse  |      as please  |      ch please \n",
      "     t appease  |      it please  |      s: please  |      ou please  |     \\nPersuade  |     ,\\nRelease \n",
      "      purchase  |       to chase  |     r,\\nPlease  |      ld please  |      ip please  |     f,\\nPlease \n",
      "     as please  |      they ease  |      n: please  |      t appease  |      ht please  |      ss please \n",
      "     ip please  |      to please  |      ho please  |       purchase  |      it please  |                \n",
      "     ht please  |     n:\\nPlease  |      at please  |      as please  |       to chase  |                \n",
      "    s\\nTo base  |      so please  |      ll please  |      ll repose  |     d,\\nEnsues  |                \n",
      "     it please  |       and ease  |     \\nPurchase  |      t promise  |      , and see  |                \n",
      "      to chase  |      To please  |      ay please  |      ip please  |      to please  |                \n",
      "     they ease  |      and chase  |     e.\\nPlease  |      ht please  |     n:\\nPlease  |                \n",
      "     to please  |      st please  |      t? please  |      to depose  |      l espouse  |                \n",
      "    n:\\nPlease  |      h release  |      er please  |      it please  |      so please  |                \n",
      "    im;\\nAbase  |      oth cease  |     A:\\nPlease  |       to chase  |      et me see  |                \n",
      "     so please  |     \\nTo cease  |      ch please  |      d promise  |     :\\nSuppose  |                \n",
      "     d so base  |      t at ease  |     ,\\nRelease  |      to please  |     ,\\nYet see  |                \n",
      "      and ease  |      So please  |      displease  |     n:\\nPlease  |      To please  |                \n",
      "     r in base  |       nor ease  |     f,\\nPlease  |      ay praise  |      and chase  |                \n",
      "     To please  |     t:\\nPlease  |       I please  |      so please  |      ne'er see  |                \n",
      "     and chase  |      od please  |      ss please  |      d purpose  |       disperse  |                \n",
      "      the base  |      ver cease  |     N:\\nPlease  |      To please  |      d eye see  |                \n",
      "     do debase  |      s: please  |                 |     l\\nDispose  |      st please  |                \n",
      "     ou debase  |     r,\\nPlease  |                 |      and chase  |      y can see  |                \n",
      "     st please  |      n: please  |                 |      st please  |      , who see  |                \n",
      "     this base  |      ho please  |                 |      you raise  |      never see  |                \n",
      "     h release  |      at please  |                 |      h release  |      hat sense  |                \n",
      "     lory base  |      ll please  |                 |      oth cease  |      , enemies  |                \n",
      "     me a case  |     \\nPurchase  |                 |      et repose  |     ,\\nAnd see  |                \n",
      "     oth cease  |      ay please  |                 |     \\nTo cease  |      t presses  |                \n",
      "    \\nTo cease  |      h. Please  |                 |      t at ease  |      nd expire  |                \n",
      "     t at ease  |      uld chase  |                 |       paradise  |      et repose  |                \n",
      "      the case  |     e.\\nPlease  |                 |      dispraise  |     \\nTo cease  |                \n",
      "     So please  |      it Please  |                 |      So please  |      lief\\nsee  |                \n",
      "      nor ease  |      t? please  |                 |      l dispose  |      ot excuse  |                \n",
      "    t:\\nPlease  |      er please  |                 |       nor ease  |      efore use  |                \n",
      "      and case  |     A:\\nPlease  |                 |      And raise  |      s can see  |                \n",
      "     od please  |      ch please  |                 |      , promise  |      That sees  |                \n",
      "     ver cease  |      with ease  |                 |     t:\\nPlease  |      So please  |                \n",
      "    ,\\nIf case  |      he phrase  |                 |       surprise  |     rning\\nSee  |                \n",
      "     What case  |     ,\\nRelease  |                 |      od please  |       nor ease  |                \n",
      "     s: please  |      ven cease  |                 |      ot praise  |      ce serves  |                \n",
      "    r,\\nPlease  |      displease  |                 |     fe to\\nsee  |      et, arise  |                \n",
      "     n: please  |     f,\\nPlease  |                 |      s: please  |       nor sees  |                \n",
      "     ho please  |       I please  |                 |     :'\\nPraise  |      r and see  |                \n",
      "     at please  |      ss please  |                 |     r,\\nPlease  |     t:\\nPlease  |                \n",
      "     ll please  |      ere cease  |                 |      As passes  |      could see  |                \n",
      "    \\nPurchase  |     N:\\nPlease  |                 |      n: please  |     t\\nAnd see  |                \n",
      "     ay please  |                 |                 |      on praise  |       and case  |                \n",
      "     e discase  |                 |                 |      ho please  |      od please  |                \n",
      "     h my case  |                 |                 |      at please  |     !\\nSuppose  |                \n",
      "     h. Please  |                 |                 |      ll please  |      e, excuse  |                \n",
      "     uld chase  |                 |                 |     \\nPurchase  |     n\\nAnd see  |                \n",
      "    e.\\nPlease  |                 |                 |      ay please  |      ot praise  |                \n",
      "     it Please  |                 |                 |      To choose  |      er excuse  |                \n",
      "     t? please  |                 |                 |      h. Please  |       once see  |                \n",
      "     er please  |                 |                 |      uld chase  |      aven sees  |                \n",
      "    A:\\nPlease  |                 |                 |     e.\\nPlease  |      arth sees  |                \n",
      "    e\\nIn base  |                 |                 |      it Please  |      s: please  |                \n",
      "     ch please  |                 |                 |      t? please  |     r,\\nPlease  |                \n",
      "     with ease  |                 |                 |      t precise  |      l and see  |                \n",
      "     he phrase  |                 |                 |      er please  |      n: please  |                \n",
      "    ,\\nRelease  |                 |                 |      hall pose  |     e\\nAnd see  |                \n",
      "     ven cease  |                 |                 |     A:\\nPlease  |      ho please  |                \n",
      "     displease  |                 |                 |      ch please  |      ir, spare  |                \n",
      "    ce\\nUncase  |                 |                 |     ,\\nRelease  |      d but see  |                \n",
      "     wful case  |                 |                 |      ll depose  |      at please  |                \n",
      "    f,\\nPlease  |                 |                 |      to repose  |      t becomes  |                \n",
      "     this case  |                 |                 |      displease  |      first see  |                \n",
      "      I please  |                 |                 |      and raise  |      ll please  |                \n",
      "    \\nThe base  |                 |                 |     f,\\nPlease  |      ay please  |                \n",
      "     ss please  |                 |                 |       I please  |      n; kisses  |                \n",
      "      our case  |                 |                 |      ss please  |      e discase  |                \n",
      "     e! Please  |                 |                 |      e! Please  |      uld chase  |                \n",
      "     ere cease  |                 |                 |     N:\\nPlease  |      to expose  |                \n",
      "    N:\\nPlease  |                 |                 |                 |      ll desire  |                \n",
      "                |                 |                 |                 |      t? please  |                \n",
      "                |                 |                 |                 |      onour see  |                \n",
      "                |                 |                 |                 |      est sense  |                \n",
      "                |                 |                 |                 |      im, spare  |                \n",
      "                |                 |                 |                 |      er please  |                \n",
      "                |                 |                 |                 |     A:\\nPlease  |                \n",
      "                |                 |                 |                 |      do excuse  |                \n",
      "                |                 |                 |                 |      I suppose  |                \n",
      "                |                 |                 |                 |     \\nDo curse  |                \n",
      "                |                 |                 |                 |      ch please  |                \n",
      "                |                 |                 |                 |      , resides  |                \n",
      "                |                 |                 |                 |      me excuse  |                \n",
      "                |                 |                 |                 |     ,\\nRelease  |                \n",
      "                |                 |                 |                 |      ety raise  |                \n",
      "                |                 |                 |                 |      To accuse  |                \n",
      "                |                 |                 |                 |      , resists  |                \n",
      "                |                 |                 |                 |      r, advise  |                \n",
      "                |                 |                 |                 |      ven cease  |                \n",
      "                |                 |                 |                 |      e and use  |                \n",
      "                |                 |                 |                 |      displease  |                \n",
      "                |                 |                 |                 |     ce\\nUncase  |                \n",
      "                |                 |                 |                 |       ever see  |                \n",
      "                |                 |                 |                 |     f,\\nPlease  |                \n",
      "                |                 |                 |                 |       I please  |                \n",
      "                |                 |                 |                 |       and sees  |                \n",
      "                |                 |                 |                 |      ss please  |                \n",
      "                |                 |                 |                 |      sir:\\nsee  |                \n",
      "                |                 |                 |                 |      e and see  |                \n",
      "                |                 |                 |                 |       no sense  |                \n",
      "                |                 |                 |                 |      ere cease  |                \n",
      "                |                 |                 |                 |      nd serves  |                \n",
      "                |                 |                 |                 |     ,\\nAnd use  |                \n"
     ]
    }
   ],
   "source": [
    "print_results(q_idx=9, query_strings=strings20k, matching_strings=matching_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query string: LLA:\\nAnd h\n",
      "      see and h |     lead\\nAnd h |      nce, and h |      nce, and h |     hem!\\nAnd h |      nce, and h\n",
      "     Rome and h |     sce,\\nAnd h |     hem!\\nAnd h |      rds, and h |      hee; and h |     LIA:\\nBut h\n",
      "    lead\\nAnd h |     hem!\\nAnd h |      hee; and h |     sce,\\nAnd h |      try: and h |      rds, and h\n",
      "     ears and h |     ide,\\nAnd h |      try: and h |     hem!\\nAnd h |     you;\\nAnd h |     hem!\\nAnd h\n",
      "     nce, and h |     you;\\nAnd h |      yes, and h |      hee; and h |     man:\\nAnd h |      hee; and h\n",
      "     rds, and h |     ars,\\nAnd h |      aty, and h |      try: and h |      ome; and h |     Soldier:\\nH\n",
      "     mile and h |     man:\\nAnd h |     you;\\nAnd h |      yes, and h |       me: and h |      nner is, h\n",
      "    sce,\\nAnd h |      lip\\nAnd h |     man:\\nAnd h |      aty, and h |      lie: and h |      try: and h\n",
      "     luto and h |     snow\\nAnd h |      ies; and h |     ide,\\nAnd h |      ood; and h |      aty, and h\n",
      "    hem!\\nAnd h |     bune\\nAnd h |      ome; and h |     you;\\nAnd h |     rer:\\nAnd h |      advised, h\n",
      "     hee; and h |     hem,\\nAnd h |      her, and h |     man:\\nAnd h |      nce? and h |      ture is, h\n",
      "     dius and h |     ost,\\nAnd h |     snow\\nAnd h |      ies; and h |      ife; and h |     you;\\nAnd h\n",
      "     e I find h |     lain\\nAnd h |      try, and h |      ome; and h |     ucy,\\nAnd h |      se wars, h\n",
      "     ight and h |     his,\\nAnd h |      aly, and h |      her, and h |     ing.\\nAnd h |      sook me, h\n",
      "     arfs and h |     oly,\\nAnd h |       me: and h |      try, and h |     r'd;\\nAnd h |     gman:\\nAn h\n",
      "     try: and h |     oke,\\nAnd h |      ell, and h |      aly, and h |     ent;\\nAnd h |      e'll go, h\n",
      "     yes, and h |     ate,\\nAnd h |     hem,\\nAnd h |       me: and h |      ken; and h |     man:\\nAnd h\n",
      "     aty, and h |      ood; and h |      lie: and h |      ell, and h |     all;\\nAnd h |      term it, h\n",
      "      joy and h |     rer:\\nAnd h |      oth, and h |     hem,\\nAnd h |     ves.\\nAnd h |      ies; and h\n",
      "      I stand h |     ept,\\nAnd h |     lain\\nAnd h |     ost,\\nAnd h |      ght; and h |      in gold, h\n",
      "     seen and h |     her,\\nAnd h |      ous, and h |      lie: and h |      h'd: and h |      ome; and h\n",
      "    ide,\\nAnd h |     ucy,\\nAnd h |     his,\\nAnd h |      oth, and h |      ome: and h |      her, and h\n",
      "     vine and h |     res,\\nAnd h |     oly,\\nAnd h |      ous, and h |     y,--\\nAnd h |     e,\\nAlas, h\n",
      "     You find h |     ing.\\nAnd h |      ath, and h |     his,\\nAnd h |     der;\\nAnd h |      try, and h\n",
      "     m do and h |     ere,\\nAnd h |      ood; and h |     oly,\\nAnd h |     ath:\\nAnd h |      aly, and h\n",
      "     okes and h |     r'd;\\nAnd h |      out, and h |     oke,\\nAnd h |      rmy; and h |       me: and h\n",
      "     ople and h |     ent;\\nAnd h |      you, and h |      ath, and h |     RET:\\nAnd h |      ell, and h\n",
      "    you;\\nAnd h |     tent\\nAnd h |     rer:\\nAnd h |     ate,\\nAnd h |     ong;\\nAnd h |     em;\\nand, h\n",
      "      husband h |     well\\nAnd h |       me, and h |      ood; and h |     oop;\\nAnd h |      d for't, h\n",
      "    ars,\\nAnd h |     ess;\\nAnd h |      his, and h |      out, and h |     and;\\nAnd h |      as lies, h\n",
      "     nger and h |     ays,\\nAnd h |     ept,\\nAnd h |      you, and h |     oon,\\nAnd h |      lie: and h\n",
      "     son\\nand h |     nom,\\nAnd h |     her,\\nAnd h |     rer:\\nAnd h |      it;\\nAnd h |      oth, and h\n",
      "    man:\\nAnd h |     rth,\\nAnd h |      nce? and h |       me, and h |     TER:\\nAnd h |      d, hold, h\n",
      "     ther and h |     ath,\\nAnd h |      ome, and h |      his, and h |     pen;\\nAnd h |      ous, and h\n",
      "     ies; and h |     all;\\nAnd h |      ife; and h |     ept,\\nAnd h |     rcy?\\nAnd h |     his,\\nAnd h\n",
      "     mies and h |     awd;\\nAnd h |     ucy,\\nAnd h |     her,\\nAnd h |     SET:\\nAnd h |     oly,\\nAnd h\n",
      "     lip\\nAnd h |     lous\\nAnd h |     res,\\nAnd h |      nce? and h |      ust! and h |     oke,\\nAnd h\n",
      "     ome; and h |     ves.\\nAnd h |     ing.\\nAnd h |      ome, and h |     ZEL:\\nAnd h |      ath, and h\n",
      "     her, and h |     ove,\\nAnd h |      ury, and h |      ife; and h |      rth. And h |      anchors, h\n",
      "     lord and h |     man,\\nAnd h |     ere,\\nAnd h |     ucy,\\nAnd h |     \\nAy, and h |      ood; and h\n",
      "    snow\\nAnd h |      h'd: and h |     ent;\\nAnd h |     res,\\nAnd h |     nna,\\nAnd h |      out, and h\n",
      "     try, and h |     rave\\nAnd h |      rey, and h |     ing.\\nAnd h |     LLA:\\nAnd h |     rer:\\nAnd h\n",
      "     eels and h |     ift,\\nAnd h |     tent\\nAnd h |      ury, and h |      nes; and h |       me, and h\n",
      "     ioli and h |     ent,\\nAnd h |      own, and h |     ere,\\nAnd h |      low; and h |      his, and h\n",
      "     aly, and h |     per,\\nAnd h |      him, and h |     r'd;\\nAnd h |      ice; and h |     her,\\nAnd h\n",
      "      me: and h |     one,\\nAnd h |     well\\nAnd h |     ent;\\nAnd h |     ELO:\\nAnd h |      ard? and h\n",
      "     ell, and h |     y,--\\nAnd h |      ken; and h |      own, and h |      ake; and h |      nce? and h\n",
      "    bune\\nAnd h |     ied;\\nAnd h |      ife, and h |      him, and h |      nse: and h |      ome, and h\n",
      "    hem,\\nAnd h |     der;\\nAnd h |      man, and h |     well\\nAnd h |      rue: and h |       to age, h\n",
      "    ost,\\nAnd h |     ath:\\nAnd h |      ied, and h |      ken; and h |     TIO:\\nAnd h |      ife; and h\n",
      "     ords and h |     aves\\nAnd h |     nom,\\nAnd h |      ife, and h |     oon:\\nAnd h |     e,\\nThat, h\n",
      "     lie: and h |     ong;\\nAnd h |      ble, and h |     ess;\\nAnd h |     her;\\nAnd h |     ucy,\\nAnd h\n",
      "     oth, and h |     nity\\nAnd h |      les, and h |      man, and h |     aca,\\nAnd h |     hell\\nGo, h\n",
      "     oble and h |     ess?\\nAnd h |      hen, and h |     ays,\\nAnd h |      sir; and h |     ing.\\nAnd h\n",
      "    lain\\nAnd h |     ords\\nAnd h |      eir, and h |     nom,\\nAnd h |     n,--\\nAnd h |      gham and h\n",
      "     idow and h |     oop;\\nAnd h |      ity, and h |      ble, and h |     bed;\\nAnd h |      ury, and h\n",
      "     ous, and h |     nce,\\nAnd h |     all;\\nAnd h |      les, and h |     NIO:\\nAnd h |      h gapes, h\n",
      "    his,\\nAnd h |     and;\\nAnd h |      ate, and h |      hen, and h |      ell; and h |     ere,\\nAnd h\n",
      "    oly,\\nAnd h |     oon,\\nAnd h |      ove, and h |     rth,\\nAnd h |     ill;\\nAnd h |      en; but, h\n",
      "     band and h |     ife,\\nAnd h |     lous\\nAnd h |      eir, and h |     dua,\\nAnd h |     r'd;\\nAnd h\n",
      "    oke,\\nAnd h |      it;\\nAnd h |     ves.\\nAnd h |     ath,\\nAnd h |     own;\\nAnd h |     ent;\\nAnd h\n",
      "     oung and h |     TER:\\nAnd h |      hou? and h |      ity, and h |     ife;\\nAnd h |      rey, and h\n",
      "     at stand h |     pen;\\nAnd h |      ips, and h |     all;\\nAnd h |      ved; and h |      own, and h\n",
      "     ath, and h |     rcy?\\nAnd h |      son, and h |      ate, and h |      ing: and h |      him, and h\n",
      "    ate,\\nAnd h |     SET:\\nAnd h |     ove,\\nAnd h |      ove, and h |      not; and h |     well\\nAnd h\n",
      "     ueen and h |     lem,\\nAnd h |      one, and h |     awd;\\nAnd h |                 |      ken; and h\n",
      "     ood; and h |     ains\\nAnd h |      she? and h |     ves.\\nAnd h |                 |      love and h\n",
      "     out, and h |     ence\\nAnd h |      rth, and h |      mes, and h |                 |      ife, and h\n",
      "     you, and h |     ZEL:\\nAnd h |      ght; and h |      hou? and h |                 |      s goods, h\n",
      "     rkly and h |      rth. And h |     man,\\nAnd h |     und,\\nAnd h |                 |      e haste, h\n",
      "    rer:\\nAnd h |     oved\\nAnd h |      h'd: and h |      ips, and h |                 |     vant:\\nAn h\n",
      "      me, and h |     nna,\\nAnd h |     rave\\nAnd h |      son, and h |                 |     ess;\\nAnd h\n",
      "     his, and h |     LLA:\\nAnd h |      ome: and h |     ove,\\nAnd h |                 |     tor.\\nBut h\n",
      "    ept,\\nAnd h |     ild;\\nAnd h |      led, and h |      one, and h |                 |      man, and h\n",
      "    her,\\nAnd h |     ELO:\\nAnd h |     per,\\nAnd h |      she? and h |                 |      y hands, h\n",
      "     ard? and h |     TIO:\\nAnd h |     one,\\nAnd h |      rth, and h |                 |     nom,\\nAnd h\n",
      "     nce? and h |     oon:\\nAnd h |     y,--\\nAnd h |      ght; and h |                 |      ill not, h\n",
      "     nces and h |     ion,\\nAnd h |     ied;\\nAnd h |     man,\\nAnd h |                 |      thin and h\n",
      "     ome, and h |     hid,\\nAnd h |      ive, and h |      h'd: and h |                 |      ble, and h\n",
      "     alth and h |     ides\\nAnd h |      iet, and h |     rave\\nAnd h |                 |      les, and h\n",
      "     ost find h |     ear,\\nAnd h |     der;\\nAnd h |     ift,\\nAnd h |                 |      hen, and h\n",
      "      you and h |     mber\\nAnd h |     ath:\\nAnd h |      ome: and h |                 |      eir, and h\n",
      "     ife; and h |     and,\\nAnd h |      rmy; and h |     ent,\\nAnd h |                 |      ity, and h\n",
      "     ster and h |     aid,\\nAnd h |      ret, and h |     per,\\nAnd h |                 |     all;\\nAnd h\n",
      "    ucy,\\nAnd h |     her;\\nAnd h |     aves\\nAnd h |     one,\\nAnd h |                 |      ate, and h\n",
      "    res,\\nAnd h |     aca,\\nAnd h |      tay, and h |     y,--\\nAnd h |                 |      ove, and h\n",
      "     then and h |     eld,\\nAnd h |     ong;\\nAnd h |     ied;\\nAnd h |                 |     him.\\nBut h\n",
      "     itch and h |     ooks\\nAnd h |     nity\\nAnd h |      ive, and h |                 |      tate and h\n",
      "    ing.\\nAnd h |     n,--\\nAnd h |     oop;\\nAnd h |      iet, and h |                 |     awd;\\nAnd h\n",
      "     gham and h |     day,\\nAnd h |     and;\\nAnd h |     der;\\nAnd h |                 |      uoth he: h\n",
      "     ury, and h |     ing;\\nAnd h |     oon,\\nAnd h |     ath:\\nAnd h |                 |     ves.\\nAnd h\n",
      "    ings\\nAnd h |     bed;\\nAnd h |     ife,\\nAnd h |      rmy; and h |                 |      hou? and h\n",
      "    ere,\\nAnd h |     NIO:\\nAnd h |      nry, and h |      ret, and h |                 |     urse:\\nAn h\n",
      "     nity and h |     ill;\\nAnd h |      it;\\nAnd h |      tay, and h |                 |      aylight, h\n",
      "     hers and h |     dua,\\nAnd h |     TER:\\nAnd h |     RET:\\nAnd h |                 |      ips, and h\n",
      "     nour and h |     own;\\nAnd h |     pen;\\nAnd h |     ong;\\nAnd h |                 |       son and h\n",
      "    r'd;\\nAnd h |     ife;\\nAnd h |      tre, and h |     ess?\\nAnd h |                 |      in love, h\n",
      "    ent;\\nAnd h |     hape\\nAnd h |     rcy?\\nAnd h |      ing, and h |                 |      high and h\n",
      "     rey, and h |     ilan\\nAnd h |     SET:\\nAnd h |     oop;\\nAnd h |                 |      son, and h\n",
      "    tent\\nAnd h |      not; and h |      l'd, and h |     nce,\\nAnd h |                 |     ars;\\nLo, h\n",
      "     foot and h |     gain\\nAnd h |     lem,\\nAnd h |     and;\\nAnd h |                 |      tale and h\n",
      "     thousand h |                 |       so, and h |     oon,\\nAnd h |                 |      one, and h\n",
      "    \\nEngland h |                 |     ains\\nAnd h |     ife,\\nAnd h |                 |      t taken: h\n",
      "     own, and h |                 |      n't, and h |      nry, and h |                 |      mad man, h\n",
      "     eign and h |                 |      ust! and h |     ass,\\nAnd h |                 |      she? and h\n",
      "     king and h |                 |      hem; and h |      it;\\nAnd h |                 |      rth, and h\n",
      "     him, and h |                 |      hem, and h |     TER:\\nAnd h |                 |      ght; and h\n",
      "     orts and h |                 |      der, and h |     pen;\\nAnd h |                 |     wo;\\nFor, h\n",
      "    well\\nAnd h |                 |     ZEL:\\nAnd h |      tre, and h |                 |     ter too,\\nH\n",
      "     ken; and h |                 |      ses; and h |     rcy?\\nAnd h |                 |      h'd: and h\n",
      "     love and h |                 |      yea, and h |     SET:\\nAnd h |                 |      em told, h\n",
      "     ife, and h |                 |      ugh, and h |      ard, and h |                 |      ome: and h\n",
      "     r's hand h |                 |      rth. And h |      l'd, and h |                 |     oly man.\\nH\n",
      "     ters and h |                 |      ick, and h |     lem,\\nAnd h |                 |      nd amen, h\n",
      "     oney and h |                 |     \\nAy, and h |       so, and h |                 |     's cold:\\nH\n",
      "    ess;\\nAnd h |                 |     oved\\nAnd h |      n't, and h |                 |      led, and h\n",
      "     man, and h |                 |     nna,\\nAnd h |      ust! and h |                 |     y,--\\nAnd h\n",
      "     ied, and h |                 |     LLA:\\nAnd h |      hem; and h |                 |     rt poor:\\nH\n",
      "     ve found h |                 |      nes; and h |      hem, and h |                 |      end it,--h\n",
      "    ays,\\nAnd h |                 |      low; and h |      der, and h |                 |     ied;\\nAnd h\n",
      "     ower and h |                 |      ice; and h |     ZEL:\\nAnd h |                 |      ive, and h\n",
      "    nom,\\nAnd h |                 |     ild;\\nAnd h |      ses; and h |                 |      m I mad, h\n",
      "     teel and h |                 |     ELO:\\nAnd h |      yea, and h |                 |      doth so, h\n",
      "     thin and h |                 |      ake; and h |      ugh, and h |                 |     der;\\nAnd h\n",
      "     ble, and h |                 |      nse: and h |      rth. And h |                 |     ase.\\nBut h\n",
      "     dull and h |                 |      rue: and h |      ick, and h |                 |     ath:\\nAnd h\n",
      "     orth and h |                 |      nes, and h |     \\nAy, and h |                 |      rmy; and h\n",
      "     les, and h |                 |      ong, and h |      ead, and h |                 |      tay, and h\n",
      "     er'd and h |                 |       do, and h |     nna,\\nAnd h |                 |     RET:\\nAnd h\n",
      "     bear and h |                 |     TIO:\\nAnd h |     LLA:\\nAnd h |                 |     ong;\\nAnd h\n",
      "     hen, and h |                 |      ted, and h |      nes; and h |                 |     ess?\\nAnd h\n",
      "    rth,\\nAnd h |                 |     oon:\\nAnd h |      low; and h |                 |     oop;\\nAnd h\n",
      "     eir, and h |                 |      ent, and h |      ice; and h |                 |     and;\\nAnd h\n",
      "    :\\nAscend h |                 |      hal, and h |     ild;\\nAnd h |                 |     \\nAnd so, h\n",
      "    ath,\\nAnd h |                 |     ion,\\nAnd h |     ELO:\\nAnd h |                 |     ord too,\\nH\n",
      "     And send h |                 |      sir, and h |      ake; and h |                 |      atagems, h\n",
      "     ity, and h |                 |     aid,\\nAnd h |      nse: and h |                 |     ife,\\nAnd h\n",
      "    all;\\nAnd h |                 |      ord? and h |      rue: and h |                 |     er;\\nAnd, h\n",
      "     ate, and h |                 |     her;\\nAnd h |      ong, and h |                 |      nry, and h\n",
      "     ove, and h |                 |     aca,\\nAnd h |       do, and h |                 |      it;\\nAnd h\n",
      "     tate and h |                 |      sir; and h |     TIO:\\nAnd h |                 |     TER:\\nAnd h\n",
      "    awd;\\nAnd h |                 |      woo, and h |     oon:\\nAnd h |                 |     pen;\\nAnd h\n",
      "    lous\\nAnd h |                 |      ize; and h |      hal, and h |                 |      tre, and h\n",
      "    ves.\\nAnd h |                 |      uds, and h |     ion,\\nAnd h |                 |     rcy?\\nAnd h\n",
      "     mes, and h |                 |     n,--\\nAnd h |     hid,\\nAnd h |                 |     SET:\\nAnd h\n",
      "     hou? and h |                 |     ing;\\nAnd h |      sir, and h |                 |      ut this: H\n",
      "     his hand h |                 |      uty, and h |     ear,\\nAnd h |                 |      ll'd and h\n",
      "    und,\\nAnd h |                 |     bed;\\nAnd h |     and,\\nAnd h |                 |      ard, and h\n",
      "     tino and h |                 |      and, and h |     aid,\\nAnd h |                 |      l'd, and h\n",
      "     elme and h |                 |     NIO:\\nAnd h |      ord? and h |                 |       so, and h\n",
      "     ntio and h |                 |      ell; and h |     her;\\nAnd h |                 |      ak this, h\n",
      "     utio and h |                 |     ill;\\nAnd h |     aca,\\nAnd h |                 |     st Lady:\\nH\n",
      "     tock and h |                 |     dua,\\nAnd h |     eld,\\nAnd h |                 |       honest, h\n",
      "     ips, and h |                 |      t I, and h |      sir; and h |                 |      n't, and h\n",
      "      son and h |                 |      old, and h |      woo, and h |                 |     as true,\\nH\n",
      "     head and h |                 |     own;\\nAnd h |      ize; and h |                 |      l son's, h\n",
      "     fair and h |                 |     ife;\\nAnd h |      uds, and h |                 |      life and h\n",
      "     high and h |                 |      ill, and h |     n,--\\nAnd h |                 |      os name, h\n",
      "     me stand h |                 |      ved; and h |     day,\\nAnd h |                 |      ng done: h\n",
      "     help and h |                 |     hape\\nAnd h |     ing;\\nAnd h |                 |      ust! and h\n",
      "     son, and h |                 |      ing: and h |      uty, and h |                 |      hoa, ho, h\n",
      "     here and h |                 |     ilan\\nAnd h |     bed;\\nAnd h |                 |      hem; and h\n",
      "     ings and h |                 |      not; and h |      and, and h |                 |     es;\\nand, h\n",
      "     tale and h |                 |                 |     NIO:\\nAnd h |                 |      to love, h\n",
      "    ove,\\nAnd h |                 |                 |      ell; and h |                 |      der, and h\n",
      "     one, and h |                 |                 |     ill;\\nAnd h |                 |     ZEL:\\nAnd h\n",
      "    \\nO, find h |                 |                 |     dua,\\nAnd h |                 |      ses; and h\n",
      "     she? and h |                 |                 |      old, and h |                 |      yea, and h\n",
      "     rth, and h |                 |                 |     own;\\nAnd h |                 |      ough and h\n",
      "    ,\\nAscend h |                 |                 |     ife;\\nAnd h |                 |      ugh, and h\n",
      "     ght; and h |                 |                 |      ill, and h |                 |      rth. And h\n",
      "    man,\\nAnd h |                 |                 |      ved; and h |                 |      mione's, h\n",
      "     h'd: and h |                 |                 |      ght, and h |                 |      ick, and h\n",
      "    rave\\nAnd h |                 |                 |      ing: and h |                 |     \\nAy, and h\n",
      "    ift,\\nAnd h |                 |                 |      not; and h |                 |     ine;\\nBut h\n",
      "     ome: and h |                 |                 |      eep, and h |                 |      amation, h\n",
      "     led, and h |                 |                 |                 |                 |      n stone: h\n",
      "    ent,\\nAnd h |                 |                 |                 |                 |     LLA:\\nAnd h\n",
      "    per,\\nAnd h |                 |                 |                 |                 |      hand and h\n",
      "    one,\\nAnd h |                 |                 |                 |                 |      ld save, h\n",
      "    y,--\\nAnd h |                 |                 |                 |                 |      nes; and h\n",
      "    ied;\\nAnd h |                 |                 |                 |                 |     \\nto sit, h\n",
      "     ive, and h |                 |                 |                 |                 |      low; and h\n",
      "     t attend h |                 |                 |                 |                 |      ice; and h\n",
      "     iet, and h |                 |                 |                 |                 |     SABELLA:\\nH\n",
      "     we found h |                 |                 |                 |                 |     ild;\\nAnd h\n",
      "    der;\\nAnd h |                 |                 |                 |                 |     ELO:\\nAnd h\n",
      "    ath:\\nAnd h |                 |                 |                 |                 |     r.\\nThat, h\n",
      "     liet and h |                 |                 |                 |                 |      ake; and h\n",
      "     rmy; and h |                 |                 |                 |                 |      nse: and h\n",
      "     ites and h |                 |                 |                 |                 |      rue: and h\n",
      "     vest and h |                 |                 |                 |                 |      ong, and h\n",
      "     land and h |                 |                 |                 |                 |     ck away:\\nH\n",
      "      him and h |                 |                 |                 |                 |       do, and h\n",
      "     ret, and h |                 |                 |                 |                 |     lf.\\nBut, h\n",
      "    aves\\nAnd h |                 |                 |                 |                 |     TIO:\\nAnd h\n",
      "     tay, and h |                 |                 |                 |                 |      ar none: h\n",
      "     to wound h |                 |                 |                 |                 |      ted, and h\n",
      "    RET:\\nAnd h |                 |                 |                 |                 |     TIO:\\nBut h\n",
      "      to hand h |                 |                 |                 |                 |     oon:\\nAnd h\n",
      "     edom and h |                 |                 |                 |                 |      hal, and h\n",
      "     loss and h |                 |                 |                 |                 |      at home, h\n",
      "    ong;\\nAnd h |                 |                 |                 |                 |      best is, h\n",
      "    nity\\nAnd h |                 |                 |                 |                 |     hid,\\nAnd h\n",
      "    ess?\\nAnd h |                 |                 |                 |                 |     o utter:\\nH\n",
      "    ords\\nAnd h |                 |                 |                 |                 |      sir, and h\n",
      "     ing, and h |                 |                 |                 |                 |     and,\\nAnd h\n",
      "    oop;\\nAnd h |                 |                 |                 |                 |      ord? and h\n",
      "    nce,\\nAnd h |                 |                 |                 |                 |     her;\\nAnd h\n",
      "    and;\\nAnd h |                 |                 |                 |                 |     r,\\nThat, h\n",
      "      eye and h |                 |                 |                 |                 |      sir; and h\n",
      "    oon,\\nAnd h |                 |                 |                 |                 |      That is, h\n",
      "    ife,\\nAnd h |                 |                 |                 |                 |      woo, and h\n",
      "     nry, and h |                 |                 |                 |                 |     APTISTA:\\nH\n",
      "     n me and h |                 |                 |                 |                 |      ize; and h\n",
      "     Scotland h |                 |                 |                 |                 |      uds, and h\n",
      "     rrow and h |                 |                 |                 |                 |     n,--\\nAnd h\n",
      "     gery and h |                 |                 |                 |                 |     day,\\nAnd h\n",
      "    ass,\\nAnd h |                 |                 |                 |                 |     ay.\\nAnd, h\n",
      "     it;\\nAnd h |                 |                 |                 |                 |     ing;\\nAnd h\n",
      "    TER:\\nAnd h |                 |                 |                 |                 |      uty, and h\n",
      "    pen;\\nAnd h |                 |                 |                 |                 |     bed;\\nAnd h\n",
      "     wick and h |                 |                 |                 |                 |       mad and h\n",
      "     re stand h |                 |                 |                 |                 |      and, and h\n",
      "     roke and h |                 |                 |                 |                 |     NIO:\\nAnd h\n",
      "     tre, and h |                 |                 |                 |                 |     lor:\\nBut h\n",
      "    rcy?\\nAnd h |                 |                 |                 |                 |      ell; and h\n",
      "    SET:\\nAnd h |                 |                 |                 |                 |     NIO:\\nBut h\n",
      "     ll'd and h |                 |                 |                 |                 |     ill;\\nAnd h\n",
      "     ard, and h |                 |                 |                 |                 |     dua,\\nAnd h\n",
      "     l'd, and h |                 |                 |                 |                 |     uly too,\\nH\n",
      "    lem,\\nAnd h |                 |                 |                 |                 |      t I, and h\n",
      "      so, and h |                 |                 |                 |                 |      old, and h\n",
      "    ains\\nAnd h |                 |                 |                 |                 |     own;\\nAnd h\n",
      "    ence\\nAnd h |                 |                 |                 |                 |     ife;\\nAnd h\n",
      "     hums and h |                 |                 |                 |                 |       ship so h\n",
      "     esty and h |                 |                 |                 |                 |      own lie, h\n",
      "     n't, and h |                 |                 |                 |                 |     ay'd\\nAnd h\n",
      "     life and h |                 |                 |                 |                 |      y books, h\n",
      "     come and h |                 |                 |                 |                 |      ill, and h\n",
      "     ust! and h |                 |                 |                 |                 |      ved; and h\n",
      "     hem; and h |                 |                 |                 |                 |      ave sir, h\n",
      "     eman and h |                 |                 |                 |                 |      ing: and h\n",
      "     ting and h |                 |                 |                 |                 |      ag-seed, h\n",
      "     hem, and h |                 |                 |                 |                 |      not; and h\n",
      "     der, and h |                 |                 |                 |                 |      eep, and h\n",
      "    oung\\nAnd h |                 |                 |                 |                 |                \n",
      "     sury and h |                 |                 |                 |                 |                \n",
      "    ZEL:\\nAnd h |                 |                 |                 |                 |                \n",
      "     ness and h |                 |                 |                 |                 |                \n",
      "     ses; and h |                 |                 |                 |                 |                \n",
      "     yea, and h |                 |                 |                 |                 |                \n",
      "     ough and h |                 |                 |                 |                 |                \n",
      "     ugh, and h |                 |                 |                 |                 |                \n",
      "    \\nHer and h |                 |                 |                 |                 |                \n",
      "     Can send h |                 |                 |                 |                 |                \n",
      "     rone and h |                 |                 |                 |                 |                \n",
      "     rth. And h |                 |                 |                 |                 |                \n",
      "    ng\\nfound h |                 |                 |                 |                 |                \n",
      "     bark and h |                 |                 |                 |                 |                \n",
      "    \\nman and h |                 |                 |                 |                 |                \n",
      "     ick, and h |                 |                 |                 |                 |                \n",
      "    \\nAy, and h |                 |                 |                 |                 |                \n",
      "     ead, and h |                 |                 |                 |                 |                \n",
      "     oted and h |                 |                 |                 |                 |                \n",
      "     race and h |                 |                 |                 |                 |                \n",
      "     ngue and h |                 |                 |                 |                 |                \n",
      "    oved\\nAnd h |                 |                 |                 |                 |                \n",
      "    nna,\\nAnd h |                 |                 |                 |                 |                \n",
      "    LLA:\\nAnd h |                 |                 |                 |                 |                \n",
      "     ella and h |                 |                 |                 |                 |                \n",
      "     ilth and h |                 |                 |                 |                 |                \n",
      "     hand and h |                 |                 |                 |                 |                \n",
      "     nes; and h |                 |                 |                 |                 |                \n",
      "     low; and h |                 |                 |                 |                 |                \n",
      "     ding and h |                 |                 |                 |                 |                \n",
      "     ice; and h |                 |                 |                 |                 |                \n",
      "    ild;\\nAnd h |                 |                 |                 |                 |                \n",
      "    ELO:\\nAnd h |                 |                 |                 |                 |                \n",
      "     ake; and h |                 |                 |                 |                 |                \n",
      "     nse: and h |                 |                 |                 |                 |                \n",
      "     rue: and h |                 |                 |                 |                 |                \n",
      "     nes, and h |                 |                 |                 |                 |                \n",
      "     ong, and h |                 |                 |                 |                 |                \n",
      "      do, and h |                 |                 |                 |                 |                \n",
      "    TIO:\\nAnd h |                 |                 |                 |                 |                \n",
      "     ted, and h |                 |                 |                 |                 |                \n",
      "     eard and h |                 |                 |                 |                 |                \n",
      "    oon:\\nAnd h |                 |                 |                 |                 |                \n",
      "     ent, and h |                 |                 |                 |                 |                \n",
      "     hal, and h |                 |                 |                 |                 |                \n",
      "     home and h |                 |                 |                 |                 |                \n",
      "    ion,\\nAnd h |                 |                 |                 |                 |                \n",
      "     Many and h |                 |                 |                 |                 |                \n",
      "    hid,\\nAnd h |                 |                 |                 |                 |                \n",
      "     self and h |                 |                 |                 |                 |                \n",
      "     aith and h |                 |                 |                 |                 |                \n",
      "    ides\\nAnd h |                 |                 |                 |                 |                \n",
      "     sing and h |                 |                 |                 |                 |                \n",
      "     sir, and h |                 |                 |                 |                 |                \n",
      "    ear,\\nAnd h |                 |                 |                 |                 |                \n",
      "     hipt and h |                 |                 |                 |                 |                \n",
      "    mber\\nAnd h |                 |                 |                 |                 |                \n",
      "     unds and h |                 |                 |                 |                 |                \n",
      "    and,\\nAnd h |                 |                 |                 |                 |                \n",
      "    aid,\\nAnd h |                 |                 |                 |                 |                \n",
      "     ord? and h |                 |                 |                 |                 |                \n",
      "     e me and h |                 |                 |                 |                 |                \n",
      "     athe and h |                 |                 |                 |                 |                \n",
      "    her;\\nAnd h |                 |                 |                 |                 |                \n",
      "    he\\nstand h |                 |                 |                 |                 |                \n",
      "    aca,\\nAnd h |                 |                 |                 |                 |                \n",
      "    eld,\\nAnd h |                 |                 |                 |                 |                \n",
      "     auty and h |                 |                 |                 |                 |                \n",
      "     sir; and h |                 |                 |                 |                 |                \n",
      "     woo, and h |                 |                 |                 |                 |                \n",
      "     l attend h |                 |                 |                 |                 |                \n",
      "     ize; and h |                 |                 |                 |                 |                \n",
      "    ooks\\nAnd h |                 |                 |                 |                 |                \n",
      "     uds, and h |                 |                 |                 |                 |                \n",
      "    n,--\\nAnd h |                 |                 |                 |                 |                \n",
      "    day,\\nAnd h |                 |                 |                 |                 |                \n",
      "    ing;\\nAnd h |                 |                 |                 |                 |                \n",
      "     uty, and h |                 |                 |                 |                 |                \n",
      "    bed;\\nAnd h |                 |                 |                 |                 |                \n",
      "      mad and h |                 |                 |                 |                 |                \n",
      "     anca and h |                 |                 |                 |                 |                \n",
      "     and, and h |                 |                 |                 |                 |                \n",
      "    NIO:\\nAnd h |                 |                 |                 |                 |                \n",
      "     duke and h |                 |                 |                 |                 |                \n",
      "     ell; and h |                 |                 |                 |                 |                \n",
      "     hter and h |                 |                 |                 |                 |                \n",
      "    ill;\\nAnd h |                 |                 |                 |                 |                \n",
      "    dua,\\nAnd h |                 |                 |                 |                 |                \n",
      "     t I, and h |                 |                 |                 |                 |                \n",
      "     adua and h |                 |                 |                 |                 |                \n",
      "     old, and h |                 |                 |                 |                 |                \n",
      "    own;\\nAnd h |                 |                 |                 |                 |                \n",
      "    ife;\\nAnd h |                 |                 |                 |                 |                \n",
      "      command h |                 |                 |                 |                 |                \n",
      "     weet and h |                 |                 |                 |                 |                \n",
      "    ay'd\\nAnd h |                 |                 |                 |                 |                \n",
      "     ill, and h |                 |                 |                 |                 |                \n",
      "     ved; and h |                 |                 |                 |                 |                \n",
      "    ck'd\\nAnd h |                 |                 |                 |                 |                \n",
      "    hild\\nAnd h |                 |                 |                 |                 |                \n",
      "     er grand h |                 |                 |                 |                 |                \n",
      "     ived and h |                 |                 |                 |                 |                \n",
      "    hape\\nAnd h |                 |                 |                 |                 |                \n",
      "     ght, and h |                 |                 |                 |                 |                \n",
      "     ing: and h |                 |                 |                 |                 |                \n",
      "     eeps and h |                 |                 |                 |                 |                \n",
      "    ilan\\nAnd h |                 |                 |                 |                 |                \n",
      "     not; and h |                 |                 |                 |                 |                \n",
      "     oots and h |                 |                 |                 |                 |                \n",
      "    gain\\nAnd h |                 |                 |                 |                 |                \n",
      "     wall and h |                 |                 |                 |                 |                \n",
      "     s island h |                 |                 |                 |                 |                \n",
      "     eep, and h |                 |                 |                 |                 |                \n"
     ]
    }
   ],
   "source": [
    "print_results(q_idx=6, query_strings=strings20k, matching_strings=matching_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Sims for Length 256 Strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings256 = all_unique_substrings(ts.text, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "sample_size = 500\n",
    "indices256 = torch.randperm(len(strings256))[:sample_size]\n",
    "strings256_sample = [strings256[i.item()] for i in indices256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs_sample256 = get_model_outputs(strings256_sample, encoding_helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_exp256 = BlockInternalsExperiment(encoding_helpers, accessors, strings256_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSimilaritiesForFinalFFWDExperiment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        exp: FinalFFWDExperiment,\n",
    "        output_folder: Path,\n",
    "    ):\n",
    "        self.exp = exp\n",
    "        self.output_folder = output_folder\n",
    "\n",
    "        self.n_batches = exp.n_batches\n",
    "\n",
    "    def ffwd_out_sims_filename(self, batch_idx: int, block_idx: int):\n",
    "        return self.output_folder / f'ffwds_out_sims_{batch_idx:04d}_{block_idx:02d}.pt'\n",
    "\n",
    "    def generate_ffwd_out_sims(self, get_queries: Callable[[int], torch.Tensor], disable_progress_bar=False):\n",
    "        block_idx = n_layer - 1\n",
    "        for batch_idx in tqdm(range(self.exp.n_batches), disable=disable_progress_bar):\n",
    "            queries = get_queries(block_idx)\n",
    "            assert queries.dim() == 2\n",
    "            n_queries = queries.shape[0]\n",
    "\n",
    "            ffwd_out_batch = torch.load(str(self.exp._ffwd_output_filename(batch_idx=batch_idx, block_idx=block_idx)), mmap=True)\n",
    "            batch_size = ffwd_out_batch.shape[0]\n",
    "            sims = F.cosine_similarity(\n",
    "                ffwd_out_batch.reshape(batch_size, 1, -1).expand(-1, n_queries, -1),\n",
    "                queries,\n",
    "                dim=-1\n",
    "            )\n",
    "            torch.save(sims, str(self.ffwd_out_sims_filename(batch_idx=batch_idx, block_idx=block_idx)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffwd_exp256 = FinalFFWDExperiment(\n",
    "    eh=encoding_helpers,\n",
    "    accessors=accessors,\n",
    "    strings=strings256,\n",
    "    output_dir=environment.data_root / 'block_internals_results/large_files/slen256',\n",
    "    batch_size=400,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = ffwd_exp256.output_dir / 'cosine_sims'\n",
    "output_folder.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_exp256 = CosineSimilaritiesForFinalFFWDExperiment(ffwd_exp256, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa11ce5d335947219d9e20bb0294dc00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2788 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cos_exp256.generate_ffwd_out_sims(get_queries=lambda block_idx: prompts_exp256.ffwd_output(block_idx=block_idx)[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_map256 = build_next_token_map(ts.text, 256, tokenizer.vocab_size, tokenizer.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min': 1, 'max': 38780, 'mean': 1561.9, 'std': 4162.8430373964375}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_idx = n_layer - 1\n",
    "ffwd256 = filter_across_batches(\n",
    "    get_batch=lambda batch_idx: torch.load(str(cos_exp256.ffwd_out_sims_filename(batch_idx=batch_idx, block_idx=block_idx)), mmap=True),\n",
    "    n_batches=cos_exp256.n_batches,\n",
    "    filter_fn=lambda batch: batch > 0.91,\n",
    "    n_queries=sample_size,\n",
    ")\n",
    "filter_result_stats(ffwd256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffwd256_strings = get_matching_strings(ffwd256, strings256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffwd256_freqs = [\n",
    "    torch.stack([\n",
    "        next_token_map256[matching_string]\n",
    "        for matching_string in matching_strings\n",
    "    ]).sum(dim=0)\n",
    "    for matching_strings in ffwd256_strings\n",
    "]\n",
    "ffwd256_probs = [\n",
    "    freqs / freqs.sum()\n",
    "    for freqs in ffwd256_freqs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 matches: 0.802\n",
      "Top 1 matches (any order): 0.802\n",
      "Top 2 matches: 0.362\n",
      "Top 2 matches (any order): 0.412\n",
      "Top 3 matches: 0.246\n",
      "Top 3 matches (any order): 0.298\n",
      "Top 4 matches: 0.200\n",
      "Top 4 matches (any order): 0.250\n",
      "Top 5 matches: 0.156\n",
      "Top 5 matches (any order): 0.154\n",
      "Top 6 matches: 0.128\n",
      "Top 6 matches (any order): 0.148\n",
      "Top 7 matches: 0.070\n",
      "Top 7 matches (any order): 0.086\n",
      "Top 8 matches: 0.044\n",
      "Top 8 matches (any order): 0.068\n",
      "Top 9 matches: 0.032\n",
      "Top 9 matches (any order): 0.038\n",
      "Top 10 matches: 0.030\n",
      "Top 10 matches (any order): 0.016\n"
     ]
    }
   ],
   "source": [
    "topn_matches, topn_matches_any_order = analyze_simulate_results(ffwd256_probs, model_outputs_sample256)\n",
    "for i in range(10):\n",
    "    print(f\"Top {i+1} matches: {topn_matches[i] / sample_size:.3f}\")\n",
    "    print(f\"Top {i+1} matches (any order): {topn_matches_any_order[i] / sample_size:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 0 (\" see my shame in him.\\nThou art a widow; yet thou art a mother,\\nAnd hast the comfort of thy children left thee:\\nBut death hath snatch'd my husband from mine arms,\\nAnd pluck'd two crutches from my feeble limbs,\\nEdward and Clarence. O, what cause have I,\\nThin\"): \n",
      "  \" so.\\n\\nSICINIUS:\\nLet them assemble,\\nAnd on a safer judgment all revoke\\nYour ignorant election; enforce his pride,\\nAnd his old hate unto you; besides, forget not\\nWith what contempt he wore the humble weed,\\nHow in his suit he scorn'd you; but your loves,\\nThin\"\n",
      "  \"I'ld have beaten him like a dog, but for\\ndisturbing the lords within.\\n\\nAUFIDIUS:\\nWhence comest thou? what wouldst thou? thy name?\\nWhy speak'st not? speak, man: what's thy name?\\n\\nCORIOLANUS:\\nIf, Tullus,\\nNot yet thou knowest me, and, seeing me, dost not\\nThin\"\n",
      "  \"e not--to save my life, for if\\nI had fear'd death, of all the men i' the world\\nI would have 'voided thee, but in mere spite,\\nTo be full quit of those my banishers,\\nStand I before thee here. Then if thou hast\\nA heart of wreak in thee, that wilt revenge\\nThin\"\n",
      "  \"t,\\nwhich should\\nMake our eyes flow with joy, hearts dance\\nwith comforts,\\nConstrains them weep and shake with fear and sorrow;\\nMaking the mother, wife and child to see\\nThe son, the husband and the father tearing\\nHis country's bowels out. And to poor we\\nThin\"\n",
      "  \" abhorr'd.' Speak to me, son:\\nThou hast affected the fine strains of honour,\\nTo imitate the graces of the gods;\\nTo tear with thunder the wide cheeks o' the air,\\nAnd yet to charge thy sulphur with a bolt\\nThat should but rive an oak. Why dost not speak?\\nThin\"\n",
      "  \"d me, in the field by Tewksbury\\nWhen Oxford had me down, he rescued me,\\nAnd said, 'Dear brother, live, and be a king'?\\nWho told me, when we both lay in the field\\nFrozen almost to death, how he did lap me\\nEven in his own garments, and gave himself,\\nAll thin\"\n",
      "  \" see my shame in him.\\nThou art a widow; yet thou art a mother,\\nAnd hast the comfort of thy children left thee:\\nBut death hath snatch'd my husband from mine arms,\\nAnd pluck'd two crutches from my feeble limbs,\\nEdward and Clarence. O, what cause have I,\\nThin\"\n",
      "  \" uncle Clarence' angry ghost:\\nMy grandam told me he was murdered there.\\n\\nPRINCE EDWARD:\\nI fear no uncles dead.\\n\\nGLOUCESTER:\\nNor none that live, I hope.\\n\\nPRINCE EDWARD:\\nAn if they live, I hope I need not fear.\\nBut come, my lord; and with a heavy heart,\\nThin\"\n",
      "  \"ard, capable\\nHe is all the mother's, from the top to toe.\\n\\nBUCKINGHAM:\\nWell, let them rest. Come hither, Catesby.\\nThou art sworn as deeply to effect what we intend\\nAs closely to conceal what we impart:\\nThou know'st our reasons urged upon the way;\\nWhat thin\"\n",
      "  \"ng that yet think not on it.\\n\\nCATESBY:\\n'Tis a vile thing to die, my gracious lord,\\nWhen men are unprepared and look not for it.\\n\\nHASTINGS:\\nO monstrous, monstrous! and so falls it out\\nWith Rivers, Vaughan, Grey: and so 'twill do\\nWith some men else, who thin\"\n",
      "  \"od morrow; good morrow, Catesby:\\nYou may jest on, but, by the holy rood,\\nI do not like these several councils, I.\\n\\nHASTINGS:\\nMy lord,\\nI hold my life as dear as you do yours;\\nAnd never in my life, I do protest,\\nWas it more precious to me than 'tis now:\\nThin\"\n",
      "  \"fitting for that royal time?\\n\\nDERBY:\\nIt is, and wants but nomination.\\n\\nBISHOP OF ELY:\\nTo-morrow, then, I judge a happy day.\\n\\nBUCKINGHAM:\\nWho knows the lord protector's mind herein?\\nWho is most inward with the royal duke?\\n\\nBISHOP OF ELY:\\nYour grace, we thin\"\n",
      "  \"e English woes will make me smile in France.\\n\\nQUEEN ELIZABETH:\\nO thou well skill'd in curses, stay awhile,\\nAnd teach me how to curse mine enemies!\\n\\nQUEEN MARGARET:\\nForbear to sleep the nights, and fast the days;\\nCompare dead happiness with living woe;\\nThin\"\n",
      "  'thy daughter,\\nAnd mean to make her queen of England.\\n\\nQUEEN ELIZABETH:\\nSay then, who dost thou mean shall be her king?\\n\\nKING RICHARD III:\\nEven he that makes her queen who should be else?\\n\\nQUEEN ELIZABETH:\\nWhat, thou?\\n\\nKING RICHARD III:\\nI, even I: what thin'\n",
      "  \"w near,\\nAnd list what with our council we have done.\\nFor that our kingdom's earth should not be soil'd\\nWith that dear blood which it hath fostered;\\nAnd for our eyes do hate the dire aspect\\nOf civil wounds plough'd up with neighbours' sword;\\nAnd for we thin\"\n",
      "  'd,\\nHaving my freedom, boast of nothing else\\nBut that I was a journeyman to grief?\\n\\nJOHN OF GAUNT:\\nAll places that the eye of heaven visits\\nAre to a wise man ports and happy havens.\\nTeach thy necessity to reason thus;\\nThere is no virtue like necessity.\\nThin'\n",
      "  \"eys-general to sue\\nHis livery, and deny his offer'd homage,\\nYou pluck a thousand dangers on your head,\\nYou lose a thousand well-disposed hearts\\nAnd prick my tender patience, to those thoughts\\nWhich honour and allegiance cannot think.\\n\\nKING RICHARD II:\\nThin\"\n",
      "  \"their complices,\\nThe caterpillars of the commonwealth,\\nWhich I have sworn to weed and pluck away.\\n\\nDUKE OF YORK:\\nIt may be I will go with you: but yet I'll pause;\\nFor I am loath to break our country's laws.\\nNor friends nor foes, to me welcome you are:\\nThin\"\n",
      "  \"ee to France\\nAnd cloister thee in some religious house:\\nOur holy lives must win a new world's crown,\\nWhich our profane hours here have stricken down.\\n\\nQUEEN:\\nWhat, is my Richard both in shape and mind\\nTransform'd and weaken'd? hath Bolingbroke deposed\\nThin\"\n",
      "  'ildly, kiss the rod,\\nAnd fawn on rage with base humility,\\nWhich art a lion and a king of beasts?\\n\\nKING RICHARD II:\\nA king of beasts, indeed; if aught but beasts,\\nI had been still a happy king of men.\\nGood sometime queen, prepare thee hence for France:\\nThin'\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(f\"Query {idx} ({repr(strings256_sample[idx])}): \")\n",
    "result = ffwd256[idx]\n",
    "for j in result[:20]:\n",
    "    print(f\"  {repr(strings256[j])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorBatchIterator:\n",
    "    def __init__(self, n_batches: int, get_batch: Callable[[int], torch.Tensor]):\n",
    "        self.n_batches = n_batches\n",
    "        self.get_batch = get_batch\n",
    "\n",
    "        self.next_batch_idx = 0\n",
    "        self.current_batch: Optional[torch.Tensor] = None\n",
    "        self.idx_within_batch = 0\n",
    "\n",
    "        self._load_next_batch()\n",
    "\n",
    "    def _load_next_batch(self):\n",
    "        if self.next_batch_idx >= self.n_batches:\n",
    "            raise StopIteration()\n",
    "\n",
    "        self.current_batch = self.get_batch(self.next_batch_idx)\n",
    "        self.idx_within_batch = 0\n",
    "        self.next_batch_idx += 1\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_batch is None:\n",
    "            raise StopIteration()\n",
    "\n",
    "        if self.idx_within_batch >= self.current_batch.shape[0]:\n",
    "            self._load_next_batch()\n",
    "            if self.current_batch is None:\n",
    "                raise StopIteration()\n",
    "\n",
    "        result = self.current_batch[self.idx_within_batch, :]\n",
    "        self.idx_within_batch += 1\n",
    "        return result\n",
    "\n",
    "class EmbeddingCosineSims:\n",
    "    def __init__(self, exp: CosineSimilaritiesExperiment):\n",
    "        self.exp = exp\n",
    "\n",
    "    def __iter__(self):\n",
    "        return TensorBatchIterator(\n",
    "            n_batches=self.exp.n_batches,\n",
    "            get_batch=lambda batch_idx: torch.load(str(self.exp.embedding_sims_filename(batch_idx=batch_idx)), mmap=True)\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sims = EmbeddingCosineSims(cos_exp)\n",
    "for i, sim in enumerate(emb_sims):\n",
    "    print(i, sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = next(iter(emb_sims))\n",
    "sims.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
