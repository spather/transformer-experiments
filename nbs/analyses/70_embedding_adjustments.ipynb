{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Adjustments\n",
    "\n",
    "> In this notebook, I investigate the adjustments to the input embedding that occur within a block and their impact on the output logits/probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Optional, Iterable, Sequence, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "import math\n",
    "from matplotlib.axes import Axes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "from transformer_experiments.common.substring_generator import all_unique_substrings\n",
    "from transformer_experiments.common.svd_helpers import adjust_singular_vector_sign\n",
    "from transformer_experiments.common.text_analysis import (\n",
    "    build_next_token_map,\n",
    ")\n",
    "from transformer_experiments.dataset_split import split_text_dataset\n",
    "from transformer_experiments.datasets.tinyshakespeare import (\n",
    "    TinyShakespeareDataSet,\n",
    ")\n",
    "from transformer_experiments.environments import get_environment\n",
    "from transformer_experiments.models.transformer import (\n",
    "    block_size,\n",
    "    n_embed,\n",
    "    n_layer,\n",
    "    TransformerLanguageModel\n",
    ")\n",
    "from transformer_experiments.models.transformer_helpers import (\n",
    "    unsqueeze_emb,\n",
    "    EncodingHelpers,\n",
    "    LogitsWrapper,\n",
    "    InputOutputAccessor,\n",
    "    TransformerAccessors\n",
    ")\n",
    "from transformer_experiments.trained_models.tinyshakespeare_transformer import (\n",
    "    create_model_and_tokenizer,\n",
    "    FilenameForToken,\n",
    ")\n",
    "from transformer_experiments.experiments.cosine_sims import (\n",
    "    filter_on_prefiltered_results,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment is local_mac\n"
     ]
    }
   ],
   "source": [
    "environment = get_environment()\n",
    "print(f\"environment is {environment.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ts = TinyShakespeareDataSet(cache_file=environment.code_root / 'nbs/artifacts/input.txt')\n",
    "m, tokenizer = create_model_and_tokenizer(\n",
    "    saved_model_filename=environment.code_root / 'nbs/artifacts/shakespeare-20231112.pt',\n",
    "    dataset=ts,\n",
    "    device=device,\n",
    ")\n",
    "_, val_data = split_text_dataset(ts.text, tokenizer, train_pct=0.9, device=device)\n",
    "encoding_helpers = EncodingHelpers(tokenizer, device)\n",
    "accessors = TransformerAccessors(m, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the unique substrings of length 10 from the text\n",
    "strings10 = all_unique_substrings(text=ts.text, substring_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is dreams,', 'by present', 's eyes may', 'eart of ho', ' man, as I']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1442)\n",
    "n_prompts = 20000\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "indices = torch.randperm(len(strings10))[:n_prompts]\n",
    "prompts = [strings10[i.item()] for i in indices]\n",
    "prompts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the strings\n",
    "tokens = encoding_helpers.tokenize_strings(prompts)\n",
    "\n",
    "# Embed the tokens\n",
    "accessors = TransformerAccessors(m, device)\n",
    "embeddings = accessors.embed_tokens(tokens)\n",
    "\n",
    "# Run them through the model with hooks attached that let us look at\n",
    "# intermediate values\n",
    "_, io_accessors = accessors.run_model(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Some Helpful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_from_block_output(block_output: torch.Tensor, block_idx: int) -> torch.Tensor:\n",
    "    # If we're not at the last layer, run the rest of the model, otherwise\n",
    "    # just get the logits from the embedding.\n",
    "    if block_idx < n_layer - 1:\n",
    "        logits, _ = accessors.run_model_from_block_n(block_output, block_idx+1)\n",
    "    else:\n",
    "        logits = accessors.logits_from_embedding(block_output)\n",
    "    return logits\n",
    "\n",
    "def probs_from_logits(logits: torch.Tensor) -> torch.Tensor:\n",
    "    return F.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intermediates(block_idx: int, io_accessors: Sequence[InputOutputAccessor]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    block_input = io_accessors[block_idx].input('.')\n",
    "    sa_output = io_accessors[block_idx].output('sa')\n",
    "    ffwd_output = io_accessors[block_idx].output('ffwd')\n",
    "    block_output = io_accessors[block_idx].output('.')\n",
    "\n",
    "    # Sanity check: block output is the sum of the block input, sa_output,\n",
    "    # and ffwd_output.\n",
    "    assert torch.all(block_output == block_input + sa_output + ffwd_output)\n",
    "\n",
    "    # Sanity check: the block output is the same as the next block's input\n",
    "    if block_idx < n_layer - 1:\n",
    "        assert torch.all(block_output == io_accessors[block_idx+1].input('.'))\n",
    "\n",
    "    return sa_output, ffwd_output, block_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hellinger_distance(\n",
    "    p: torch.Tensor,\n",
    "    q: torch.Tensor,\n",
    "):\n",
    "    return ((p.sqrt() - q.sqrt())**2).sum(dim=-1).sqrt() / math.sqrt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within a block, the input embedding is adjusted twice: first by adding the output of the self-attention module, then by adding the output of the feed-forward network to that sum:\n",
    "\n",
    "```python\n",
    "class Block(nn.Module):\n",
    "    \"\"\"One transformer block\"\"\"\n",
    "\n",
    "    ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # The `x +` part is a skip connection\n",
    "        x = x + self.ffwd(self.ln2(x)) # The `x +` part is a skip connection\n",
    "\n",
    "        return x\n",
    "```\n",
    "\n",
    "This is equivalent to the following code, which, by using some intermediate local variables, makes the two adjustments a little clearer:\n",
    "\n",
    "```python\n",
    "    def forward(self, x):\n",
    "        sa_output = self.sa(self.ln1(x))\n",
    "        ffwd_output = self.ffwd(self.ln2(x + sa_output))\n",
    "\n",
    "        return x + sa_output + ffwd_output\n",
    "```\n",
    "\n",
    "The output of the block is the original embedding, plus the self-attention output, plus the feed-forward network output. \n",
    "\n",
    "In this analysis, we'll look at the impact of each of the two adjustments on the final output of the model. Specifically, compute the output logits from the original output of the block and compare this to the output logits that the model would have produced had the self-attention output not been added (i.e. if the block output had been just `x + ffwd_output`) and if the feed-forward network output had not been added (`x + sa_output`). We'll also look at the impact on output probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_intermediate_impact(\n",
    "    block_idx: int,\n",
    "    sa_output: torch.Tensor,\n",
    "    ffwd_output: torch.Tensor,\n",
    "    block_output: torch.Tensor,\n",
    "):\n",
    "    print(f\"Block {block_idx}\")\n",
    "\n",
    "    # Get the logits that would have been produced by the normal block\n",
    "    # output. This is the baseline we're comparing against.\n",
    "    logits = logits_from_block_output(block_output[:, -1, :].unsqueeze(1), block_idx)\n",
    "\n",
    "    # Get the logits that would have been produced by the block output\n",
    "    # minus the self-attention output.\n",
    "    logits_no_sa = logits_from_block_output(\n",
    "        (block_output - sa_output)[:, -1, :].unsqueeze(1), block_idx\n",
    "    )\n",
    "\n",
    "    # Get the logits that would have been produced by the block output\n",
    "    # minus the feed-forward network output.\n",
    "    logits_no_ffwd = logits_from_block_output(\n",
    "        (block_output - ffwd_output)[:, -1, :].unsqueeze(1), block_idx\n",
    "    )\n",
    "\n",
    "    # Subtract the modified logits from the baseline logits and get the norms.\n",
    "    no_sa_diff_norms = (logits - logits_no_sa).norm(dim=-1)\n",
    "    no_ffwd_diff_norms = (logits - logits_no_ffwd).norm(dim=-1)\n",
    "\n",
    "    # Print stats\n",
    "    print(\n",
    "        f\"                                 Average norm of logits: {logits.norm(dim=-1).mean():>5.2f} ± {logits.norm(dim=-1).std():.2f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"                Average norm of (logits - logits_no_sa): {no_sa_diff_norms.mean():>5.2f} ± {no_sa_diff_norms.std():.2f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"              Average norm of (logits - logits_no_ffwd): {no_ffwd_diff_norms.mean():>5.2f} ± {no_ffwd_diff_norms.std():.2f}\"\n",
    "    )\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Compute probabilities from the baseline logits and the modified logits.\n",
    "    # Then compute the Hellinger distance between the two sets of probabilities.\n",
    "    h_no_sa = hellinger_distance(\n",
    "        probs_from_logits(logits), probs_from_logits(logits_no_sa)\n",
    "    )\n",
    "    h_no_ffwd = hellinger_distance(\n",
    "        probs_from_logits(logits), probs_from_logits(logits_no_ffwd)\n",
    "    )\n",
    "\n",
    "    # Print stats\n",
    "    print(\n",
    "        f\"  Average h(probs_from_logits, probs_from_logits_no_sa): {h_no_sa.mean():>5.2f} ± {h_no_sa.std():.2f}, \"\n",
    "    )\n",
    "    print(\n",
    "        f\"Average h(probs_from_logits, probs_from_logits_no_ffwd): {h_no_ffwd.mean():>5.2f} ± {h_no_ffwd.std():.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0\n",
      "                                 Average norm of logits: 26.64 ± 4.02\n",
      "                Average norm of (logits - logits_no_sa):  3.31 ± 1.23\n",
      "              Average norm of (logits - logits_no_ffwd): 20.65 ± 5.88\n",
      "\n",
      "  Average h(probs_from_logits, probs_from_logits_no_sa):  0.11 ± 0.07, \n",
      "Average h(probs_from_logits, probs_from_logits_no_ffwd):  0.70 ± 0.17\n"
     ]
    }
   ],
   "source": [
    "block_idx = 0\n",
    "sa_output, ffwd_output, block_output = get_intermediates(block_idx, io_accessors)\n",
    "analyze_intermediate_impact(block_idx, sa_output, ffwd_output, block_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 1\n",
      "                                 Average norm of logits: 28.01 ± 4.02\n",
      "                Average norm of (logits - logits_no_sa):  2.08 ± 0.75\n",
      "              Average norm of (logits - logits_no_ffwd):  5.43 ± 1.72\n",
      "\n",
      "  Average h(probs_from_logits, probs_from_logits_no_sa):  0.07 ± 0.04, \n",
      "Average h(probs_from_logits, probs_from_logits_no_ffwd):  0.19 ± 0.11\n"
     ]
    }
   ],
   "source": [
    "block_idx = 1\n",
    "sa_output, ffwd_output, block_output = get_intermediates(block_idx, io_accessors)\n",
    "analyze_intermediate_impact(block_idx, sa_output, ffwd_output, block_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 2\n",
      "                                 Average norm of logits: 29.60 ± 4.22\n",
      "                Average norm of (logits - logits_no_sa):  2.91 ± 1.39\n",
      "              Average norm of (logits - logits_no_ffwd):  4.68 ± 1.61\n",
      "\n",
      "  Average h(probs_from_logits, probs_from_logits_no_sa):  0.09 ± 0.07, \n",
      "Average h(probs_from_logits, probs_from_logits_no_ffwd):  0.15 ± 0.10\n"
     ]
    }
   ],
   "source": [
    "block_idx = 2\n",
    "sa_output, ffwd_output, block_output = get_intermediates(block_idx, io_accessors)\n",
    "analyze_intermediate_impact(block_idx, sa_output, ffwd_output, block_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 3\n",
      "                                 Average norm of logits: 30.64 ± 4.15\n",
      "                Average norm of (logits - logits_no_sa):  2.70 ± 1.13\n",
      "              Average norm of (logits - logits_no_ffwd):  4.72 ± 1.83\n",
      "\n",
      "  Average h(probs_from_logits, probs_from_logits_no_sa):  0.06 ± 0.05, \n",
      "Average h(probs_from_logits, probs_from_logits_no_ffwd):  0.13 ± 0.10\n"
     ]
    }
   ],
   "source": [
    "block_idx = 3\n",
    "sa_output, ffwd_output, block_output = get_intermediates(block_idx, io_accessors)\n",
    "analyze_intermediate_impact(block_idx, sa_output, ffwd_output, block_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 4\n",
      "                                 Average norm of logits: 31.22 ± 4.18\n",
      "                Average norm of (logits - logits_no_sa):  2.07 ± 0.71\n",
      "              Average norm of (logits - logits_no_ffwd):  5.45 ± 2.13\n",
      "\n",
      "  Average h(probs_from_logits, probs_from_logits_no_sa):  0.04 ± 0.03, \n",
      "Average h(probs_from_logits, probs_from_logits_no_ffwd):  0.14 ± 0.10\n"
     ]
    }
   ],
   "source": [
    "block_idx = 4\n",
    "sa_output, ffwd_output, block_output = get_intermediates(block_idx, io_accessors)\n",
    "analyze_intermediate_impact(block_idx, sa_output, ffwd_output, block_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 5\n",
      "                                 Average norm of logits: 31.61 ± 4.21\n",
      "                Average norm of (logits - logits_no_sa):  1.84 ± 0.61\n",
      "              Average norm of (logits - logits_no_ffwd):  7.55 ± 3.09\n",
      "\n",
      "  Average h(probs_from_logits, probs_from_logits_no_sa):  0.03 ± 0.03, \n",
      "Average h(probs_from_logits, probs_from_logits_no_ffwd):  0.17 ± 0.10\n"
     ]
    }
   ],
   "source": [
    "block_idx = 5\n",
    "sa_output, ffwd_output, block_output = get_intermediates(block_idx, io_accessors)\n",
    "analyze_intermediate_impact(block_idx, sa_output, ffwd_output, block_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magnitude varies per block, but it seems that the feed-forward network output has a much larger impact on the final output of the model at each block. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
