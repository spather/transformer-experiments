{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Internals of a Small Language Model's Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Iterable, Sequence, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "from fastcore.test import *\n",
    "from matplotlib.axes import Axes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "from transformer_experiments.common.substring_generator import all_unique_substrings\n",
    "from transformer_experiments.common.text_analysis import (\n",
    "    build_next_token_map,\n",
    "    SubstringFrequencyAnalysis,\n",
    "    top_nonzero_tokens\n",
    ")\n",
    "from transformer_experiments.common.utils import aggregate_by_string_key, DataWrapper\n",
    "from transformer_experiments.dataset_split import split_text_dataset\n",
    "from transformer_experiments.datasets.tinyshakespeare import (\n",
    "    TinyShakespeareDataSet,\n",
    ")\n",
    "from transformer_experiments.models.transformer import (\n",
    "    n_layer,\n",
    "    TransformerLanguageModel\n",
    ")\n",
    "from transformer_experiments.models.transformer_helpers import (\n",
    "    unsqueeze_emb,\n",
    "    EncodingHelpers,\n",
    "    LogitsWrapper,\n",
    "    TransformerAccessors\n",
    ")\n",
    "from transformer_experiments.trained_models.tinyshakespeare_transformer import (\n",
    "    create_model_and_tokenizer\n",
    ")\n",
    "from transformer_experiments.experiments.block_internals import (\n",
    "    BlockInternalsAccessors,\n",
    "    BlockInternalsExperiment,\n",
    "    BatchedBlockInternalsExperiment,\n",
    "    BatchedBlockInternalsExperimentSlicer,\n",
    "    BlockInternalsAnalysis,\n",
    ")\n",
    "from transformer_experiments.experiments.logit_lens import LogitLens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "Early this past summer, I trained a small language model. I then spent the rest of the summer taking it apart and trying to figure out how it works. This post is a summary of what I've learned so far. \n",
    "\n",
    "I started by watching [Andrej Karpathy](https://karpathy.ai/)'s excellent video, [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY). In that video, he starts from a blank Jupyter notebook and, just under 2 hours later, ends with a functional transformer model (it's one of the best explanatory videos I've ever seen and highly recommend it). \n",
    "\n",
    "I want to make one thing super clear at the start: The code for the language model I trained came entirely from the video. It's Andrej's, not mine. I typed in the code by copying what I saw on the screen as I watched the video. For things that weren't clear onscreen, I referenced the [GitHub repo for the video](https://github.com/karpathy/ng-video-lecture) and the [nanoGPT repo](https://github.com/karpathy/nanoGPT). After getting it working, I made only minor changes to make it work with the rest of the code in/structure of this repository, resulting in [this implementation](https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer.ipynb). In summary: the core language model is Andrej Karpathy's work, not mine. The analysis and all the supporting code behind it is mine. I was of course inspired by many others and I'll cite their work in the relevant places. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "The model I trained is a small, decoder-only [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)). It’s trained on the [TinyShakespeare data set](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) which contains 40,000 lines of Shakespeare’s plays. \n",
    "\n",
    "After about an hour of training on an A100 GPU, it is able to produce reasonable-looking fake Shakespearean text. Let’s spin it up and look at it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ts = TinyShakespeareDataSet(cache_file='../artifacts/input.txt')\n",
    "m, tokenizer = create_model_and_tokenizer(\n",
    "    saved_model_filename='../artifacts/shakespeare.pt',\n",
    "    dataset=ts,\n",
    "    device=device,\n",
    ")\n",
    "print(f\"device is {device}\")\n",
    "encoding_helpers = EncodingHelpers(tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a prompt, the model predicts what the next tokens will be. Let's start with an easy task, giving it a prompt that surely it's seen before, `ROMEO:`, and ask it to generate 500 new tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Thus he cannot Edward's sunse heart\n",
      "That any thing hath bid His temption of seems.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Nay, you are has kept another, of the queen,\n",
      "Against his noble foreign them to take;\n",
      "And with their I'll harm those insequents,\n",
      "That honoured she not black physicians;\n",
      "But what is full and a man of hoteful prince,\n",
      "And to ransom on their within the fair beds did\n",
      "But in same heaven limit out a clean.\n",
      "\n",
      "Nurse:\n",
      "England, by yond face!\n",
      "\n",
      "RATCLIFF:\n",
      "Thou dancest not kill'd with not budies.\n",
      "\n",
      "RICHARD:\n",
      "Thus say t\n"
     ]
    }
   ],
   "source": [
    "_ = torch.manual_seed(1337) # Keep the output deterministic across runs\n",
    "prompt = 'ROMEO:'\n",
    "tokens = encoding_helpers.tokenize_string(prompt)\n",
    "print(tokenizer.decode(m.generate(tokens, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't try too hard to interpret it: meaning-wise, it's nonsense. But in terms of superficial structure, it looks Shakespearean:\n",
    "\n",
    "* It's looks like a script for a play: a character name, followed by a colon, followed by a line of dialog.\n",
    "* Most of the words are valid English words. It's important to note that in this model, the tokens are individual characters, not words. So it's making words up from characters and mostly getting it right. Though there are some notable exceptions like \"sunse\", \"hoteful\", and \"insequents\". But even these exceptions don't seem too far off from real words.\n",
    "* Capitalization and punctuation are mostly correct: the first word of each line is capitalized, periods and other punctuation are used in plausible. Named characters (\"ROMEO\", \"BUCKINGHAM\") are in all-caps and unnamed characters (\"Nurse\") are not. But to be fair, all these character names appear in the training data, so that attribute is more likely a result of just copying what it's seen before vs understanding that pattern. \n",
    "* The language sounds archaic, using words like \"Thou\", \"Nay\", \"yond\", and 'dancest'. It's noteable tht \"dancest\" does not actually appear in the training data, so it's not just copying words it's seen before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When given a prompt it hasn't seen before, it still does a reasonable job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellows stand cause, Edward's sunse and justice:\n",
      "Then, tell thou this tempest malnsters should have\n",
      "Resid\n"
     ]
    }
   ],
   "source": [
    "_ = torch.manual_seed(1337) # Keep the output deterministic across runs\n",
    "prompt = 'Hello'\n",
    "tokens = encoding_helpers.tokenize_string(prompt)\n",
    "print(tokenizer.decode(m.generate(tokens, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Hello\" doesn't appear anywhere in the text, but you can see how it got to \"Hellows\" given that \"fellow\", \"yellow\", and \"mellow\" do. Even a completely gibberish prompt, like `adxed3dd`, it's able to recover and produce something reasonable looking: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adxed3ddess, and caden'd in throllows.\n",
      "Now thou dost know thy horse to set;\n",
      "Have I learn'd these friends and\n"
     ]
    }
   ],
   "source": [
    "_ = torch.manual_seed(1337) # Keep the output deterministic across runs\n",
    "prompt = 'adxed3dd'\n",
    "tokens = encoding_helpers.tokenize_string(prompt)\n",
    "print(tokenizer.decode(m.generate(tokens, max_new_tokens=100)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
