{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# databatcher\n",
    "\n",
    "> Iterable that will break a long data tensor into batches of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.test import *\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataBatcher:\n",
    "    \"\"\"Iterable that will break a long data tensor into batches of samples.\"\"\"\n",
    "    def __init__(\n",
    "        self, data: torch.Tensor, sample_len: int, max_batch_size: int, stride: int\n",
    "    ):\n",
    "        assert len(data.shape) == 1, \"Data must be a 1D tensor\"\n",
    "        assert len(data) >= sample_len, \"Data length must be at least sample_len\"\n",
    "\n",
    "        self.samples = data.unfold(0, sample_len, stride)\n",
    "        self.sample_len = sample_len\n",
    "        self.max_batch_size = max_batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of batches that will be produced.\"\"\"\n",
    "        return math.ceil(len(self.samples) / self.max_batch_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.samples), self.max_batch_size):\n",
    "            yield self.samples[i : i + self.max_batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for DataBatcher\n",
    "\n",
    "# Basic example\n",
    "data_batcher = DataBatcher(data=torch.arange(6), sample_len=3, max_batch_size=2, stride=1)\n",
    "test_eq(len(data_batcher), 2)\n",
    "test_eq(\n",
    "    list(data_batcher),\n",
    "    [\n",
    "        [[0, 1, 2], [1, 2, 3]],\n",
    "        [[2, 3, 4], [3, 4, 5]],\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Basic example with stride\n",
    "data_batcher = DataBatcher(data=torch.arange(10), sample_len=3, max_batch_size=2, stride=2)\n",
    "test_eq(len(data_batcher), 2)\n",
    "test_eq(\n",
    "    list(data_batcher),\n",
    "    [\n",
    "        [[0, 1, 2], [2, 3, 4]],\n",
    "        [[4, 5, 6], [6, 7, 8]],\n",
    "    ],\n",
    ")\n",
    "\n",
    "# No repeated elements: stride = chunk_len\n",
    "data_batcher = DataBatcher(data=torch.arange(6), sample_len=3, max_batch_size=2, stride=3)\n",
    "test_eq(len(data_batcher), 1)\n",
    "test_eq(\n",
    "    list(data_batcher),\n",
    "    [\n",
    "        [[0, 1, 2], [3, 4, 5]],\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Last batch is smaller than max_batch_size\n",
    "data_batcher = DataBatcher(data=torch.arange(7), sample_len=3, max_batch_size=2, stride=2)\n",
    "test_eq(len(data_batcher), 2)\n",
    "test_eq(\n",
    "    list(data_batcher),\n",
    "    [\n",
    "        [[0, 1, 2], [2, 3, 4]],\n",
    "        [[4, 5, 6]],\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Not even one complete batch\n",
    "data_batcher = DataBatcher(data=torch.arange(3), sample_len=3, max_batch_size=2, stride=2)\n",
    "test_eq(len(data_batcher), 1)\n",
    "test_eq(\n",
    "    list(data_batcher),\n",
    "    [\n",
    "        [[0, 1, 2]],\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Same as above but add an extra element which still doesn't make a full batch\n",
    "data_batcher = DataBatcher(data=torch.arange(8), sample_len=3, max_batch_size=2, stride=2)\n",
    "test_eq(len(data_batcher), 2)\n",
    "test_eq(\n",
    "    list(data_batcher),\n",
    "    [\n",
    "        [[0, 1, 2], [2, 3, 4]],\n",
    "        [[4, 5, 6]],\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Only one chunk can fit\n",
    "data_batcher = DataBatcher(data=torch.arange(8), sample_len=8, max_batch_size=2, stride=2)\n",
    "test_eq(len(data_batcher), 1)\n",
    "test_eq(\n",
    "    list(data_batcher),\n",
    "    [\n",
    "        [[0, 1, 2, 3, 4, 5, 6, 7]],\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Large stride makes it so that only one chunk can fit\n",
    "data_batcher = DataBatcher(data=torch.arange(10), sample_len=3, max_batch_size=2, stride=12)\n",
    "test_eq(len(data_batcher), 1)\n",
    "test_eq(\n",
    "    list(data_batcher),\n",
    "    [\n",
    "        [[0, 1, 2]],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
