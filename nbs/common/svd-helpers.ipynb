{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# svd-helpers\n",
    "\n",
    "> Helper functions related to [Singular Value Decomposition](https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/) and its applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp common.svd_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def adjust_singular_vector_sign(\n",
    "    singular_vector: torch.Tensor, original_matrix: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Depending on the algorithm used to compute the SVD, the sign of the singular\n",
    "    vectors can be flipped. This function adjusts the sign of the singular vector so\n",
    "    that it aligns with the majority of the vectors in the original matrix. Per\n",
    "    https://www.osti.gov/servlets/purl/920802, this is a valid way to resolve the\n",
    "    sign ambiguity.\"\"\"\n",
    "    assert singular_vector.ndim == 1\n",
    "    assert original_matrix.ndim == 2\n",
    "    assert singular_vector.shape[0] == original_matrix.shape[1]\n",
    "\n",
    "    n_negatives = torch.count_nonzero(\n",
    "        F.cosine_similarity(original_matrix, singular_vector.unsqueeze(dim=0)) < 0\n",
    "    )\n",
    "    sign = -1 if n_negatives > original_matrix.shape[0] / 2 else 1\n",
    "    return sign * singular_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for adjust_singular_vector_sign\n",
    "test_matrix = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "test_singular_vector = torch.tensor([-0.4797, -0.5724, -0.6651])\n",
    "\n",
    "test_eq(\n",
    "    adjust_singular_vector_sign(test_singular_vector, test_matrix),\n",
    "    -test_singular_vector,\n",
    ")\n",
    "\n",
    "test_eq(\n",
    "    adjust_singular_vector_sign(-test_singular_vector, test_matrix),\n",
    "    -test_singular_vector,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def projection_matrix_for_rank_k_approximation(\n",
    "    original_matrix: torch.Tensor, k: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Returns a projection matrix that projects onto the subspace spanned by the top\n",
    "    k singular vectors of the original matrix. Derivation of the formula:\n",
    "    https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/00e9c8f0eafedeab21a3d079a17ed3d8_MIT18_06SCF11_Ses2.2sum.pdf\"\"\"\n",
    "    assert original_matrix.ndim == 2\n",
    "    assert k > 0 and k <= original_matrix.shape[1]\n",
    "\n",
    "    _, _, V = torch.linalg.svd(original_matrix, full_matrices=True)\n",
    "    basis_vectors = []\n",
    "    for i in range(k):\n",
    "        basis_vectors.append(adjust_singular_vector_sign(V[i], original_matrix))\n",
    "\n",
    "    A = torch.stack(basis_vectors).T\n",
    "\n",
    "    return A @ (A.T @ A).inverse() @ A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for projection_matrix_for_rank_k_approximation\n",
    "\n",
    "# Make up a test matrix where the singular vectors are just the standard\n",
    "# basis vectors for R^3\n",
    "test_matrix = torch.tensor([\n",
    "    [3, 0, 0],\n",
    "    [0, 2, 0],\n",
    "    [0, 0, 1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "test_vector = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "\n",
    "# Test rank 1 approximation\n",
    "proj_matrix = projection_matrix_for_rank_k_approximation(test_matrix, 1)\n",
    "projection = proj_matrix @ test_vector\n",
    "\n",
    "# The projection should be the projection of the test vector onto the x-axis\n",
    "e_1 = torch.tensor([1, 0, 0], dtype=torch.float32)\n",
    "test_eq(projection, test_vector.dot(e_1) * e_1)\n",
    "\n",
    "# Test rank 2 approximation\n",
    "proj_matrix = projection_matrix_for_rank_k_approximation(test_matrix, 2)\n",
    "projection = proj_matrix @ test_vector\n",
    "\n",
    "# The projection should be the projection of the test vector onto the x-y plane\n",
    "e_1 = torch.tensor([1, 0, 0], dtype=torch.float32)\n",
    "e_2 = torch.tensor([0, 1, 0], dtype=torch.float32)\n",
    "test_eq(projection, test_vector.dot(e_1) * e_1 + test_vector.dot(e_2) * e_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
