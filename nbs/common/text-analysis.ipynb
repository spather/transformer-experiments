{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text-analysis\n",
    "\n",
    "> Useful code for analyzing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp common.text_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Iterable, Sequence, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_next_token_map(\n",
    "    text: str, prefix_len: int, vocab_size: int, stoi: Dict[str, int]\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"For a given body of text, build a map of all prefixes of a given\n",
    "    length to the frequencies of the next token.\"\"\"\n",
    "    next_token_map: Dict[str, torch.Tensor] = defaultdict(\n",
    "        lambda: torch.zeros(vocab_size, dtype=torch.long)\n",
    "    )\n",
    "\n",
    "    for i in range(\n",
    "        len(text) - prefix_len\n",
    "    ):  # This range ensures the last prefix has a next token\n",
    "        prefix = text[i : i + prefix_len]\n",
    "        next_token = text[i + prefix_len]\n",
    "        next_token_map[prefix][stoi[next_token]] += 1\n",
    "\n",
    "    # The loop above will have added every substring of length\n",
    "    # `prefx_len` to the map, except for the very last one,\n",
    "    # because it has no next token. But, it is useful to have\n",
    "    # this last string in the map, with zeros for all the next\n",
    "    # token counts (it is a valid substring of the right length\n",
    "    # and calling code might want to look it up). We add it here.\n",
    "    last_prefix = text[-prefix_len:]\n",
    "\n",
    "    # Adding zero ensures the entry is unchanged if it exists, but\n",
    "    # will added it (via the defaultdict's default factory) if it\n",
    "    # doesn't.\n",
    "    next_token_map[last_prefix] += 0\n",
    "\n",
    "    return next_token_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for build_next_token_map\n",
    "test_text = \"abcabcc\"\n",
    "test_prefix_len = 2\n",
    "test_vocab_size = 3\n",
    "test_stoi = {\"a\": 0, \"b\": 1, \"c\": 2}\n",
    "test_next_token_map = build_next_token_map(\n",
    "    test_text, test_prefix_len, test_vocab_size, test_stoi\n",
    ")\n",
    "test_eq(len(test_next_token_map), 4)\n",
    "test_eq(test_next_token_map[\"ab\"], torch.tensor([0, 0, 2]))\n",
    "test_eq(test_next_token_map[\"bc\"], torch.tensor([1, 0, 1]))\n",
    "test_eq(test_next_token_map[\"ca\"], torch.tensor([0, 1, 0]))\n",
    "\n",
    "# The last substring should be in the map with all zeros\n",
    "test_eq(test_next_token_map[\"cc\"], torch.tensor([0, 0, 0]))\n",
    "\n",
    "# Test the case where the last substring is already in the map.\n",
    "test_next_token_map = build_next_token_map(\n",
    "    \"abcabc\", test_prefix_len, test_vocab_size, test_stoi\n",
    ")\n",
    "test_eq(test_next_token_map[\"bc\"], torch.tensor([1, 0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def top_nonzero_tokens(freqs: torch.Tensor, itos: Dict[int, str]) -> Iterable[Tuple[str, float]]:\n",
    "    k = torch.count_nonzero(freqs).item()\n",
    "    assert isinstance(k, int) # keep mypy happy\n",
    "    topk = torch.topk(freqs, k=k)\n",
    "    return [(itos[i], freqs[i].item()) for i in topk.indices.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for top_nonzero_tokens\n",
    "itos = {0: 'a', 1: 'b', 2: 'c', 3: 'd'}\n",
    "\n",
    "# All zeros\n",
    "freqs = torch.tensor([0, 0, 0, 0])\n",
    "test_eq(top_nonzero_tokens(freqs, itos), [])\n",
    "\n",
    "# All non-zeros\n",
    "freqs = torch.tensor([1, 2, 3, 4])\n",
    "test_eq(top_nonzero_tokens(freqs, itos), [('d', 4), ('c', 3), ('b', 2), ('a', 1)])\n",
    "\n",
    "# Some zeros\n",
    "freqs = torch.tensor([0, 2, 0, 4])\n",
    "test_eq(top_nonzero_tokens(freqs, itos), [('d', 4), ('b', 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SubstringFrequencyAnalysis:\n",
    "    \"\"\"Class that performs frequency analysis on a body of text for a set of substrings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        substrs: Sequence[str],\n",
    "        next_token_map: Dict[str, torch.Tensor],\n",
    "        itos: Dict[int, str],\n",
    "    ):\n",
    "        # Need at least one string to determine the length\n",
    "        # and for this to be useful.\n",
    "        assert len(substrs) > 0\n",
    "\n",
    "        self.freq_map = {\n",
    "            s: next_token_map[s]\n",
    "            for s in substrs\n",
    "        }\n",
    "\n",
    "        # Compute the cumulative frequencies\n",
    "        vocab_size = len(next(iter(self.freq_map.values())))\n",
    "        self.cumulative_freqs = torch.zeros(vocab_size, dtype=torch.long)\n",
    "        for freqs in self.freq_map.values():\n",
    "            self.cumulative_freqs += freqs\n",
    "\n",
    "        # Normalize the cumulative frequencies\n",
    "        self.norm_cumulative_freqs = self.cumulative_freqs.float() / self.cumulative_freqs.sum()\n",
    "\n",
    "        # Figure out the top tokens for each substring\n",
    "        self.top_tokens = {\n",
    "            s: top_nonzero_tokens(freqs, itos)\n",
    "            for s, freqs in self.freq_map.items()\n",
    "        }\n",
    "        self.top_tokens_cumulative = top_nonzero_tokens(\n",
    "            self.norm_cumulative_freqs, itos\n",
    "        )\n",
    "\n",
    "    def print_summary(self):\n",
    "        print(f\"Substrings: {', '.join([repr(substr) for substr in self.freq_map.keys()])}\")\n",
    "\n",
    "        print(\"Top Tokens for each substring:\")\n",
    "        s_len = max([len(s) for s in self.freq_map.keys()])\n",
    "        for s, tokens in self.top_tokens.items():\n",
    "            print(\n",
    "                f\"{repr(s):>{2*s_len+2}}: {', '.join([f'{repr(token):>4} ({freq:>4})' for token, freq in tokens])}\"\n",
    "            )\n",
    "\n",
    "        print(\"Cumulative Top Tokens:\")\n",
    "        print(\n",
    "            ', '.join(\n",
    "                [\n",
    "                    f'{repr(token):>4} ({freq:.2f})'\n",
    "                    for token, freq in self.top_tokens_cumulative\n",
    "                ]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for SubstringFrequencyAnalysis\n",
    "itos = {0: 'a', 1: 'b', 2: 'c', 3: 'd'}\n",
    "stoi = {v: k for k, v in itos.items()}\n",
    "text = 'aaabbbcccddd'\n",
    "substrs = ['aaa', 'bbb', 'ccc', 'ddd']\n",
    "\n",
    "next_token_map = build_next_token_map(text, len(substrs[0]), len(itos), stoi)\n",
    "sfa = SubstringFrequencyAnalysis(\n",
    "    substrs=substrs, next_token_map=next_token_map, itos=itos\n",
    ")\n",
    "test_eq(sfa.freq_map['aaa'].tolist(), [0, 1, 0, 0])\n",
    "test_eq(sfa.freq_map['bbb'].tolist(), [0, 0, 1, 0])\n",
    "test_eq(sfa.freq_map['ccc'].tolist(), [0, 0, 0, 1])\n",
    "test_eq(sfa.freq_map['ddd'].tolist(), [0, 0, 0, 0])\n",
    "\n",
    "test_close(sfa.norm_cumulative_freqs.tolist(), [0, 1/3, 1/3, 1/3])\n",
    "\n",
    "test_eq(sfa.top_tokens['aaa'], [('b', 1)])\n",
    "test_eq(sfa.top_tokens['bbb'], [('c', 1)])\n",
    "test_eq(sfa.top_tokens['ccc'], [('d', 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
