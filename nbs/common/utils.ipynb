{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils\n",
    "\n",
    "> General-purpose utility functions used throughout the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp common.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Callable, Dict, Generic, Iterable, Sequence, Tuple, TypeVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "T = TypeVar('T') # Generic type that will be used in many places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def aggregate_by_string_key(\n",
    "    items: Iterable[T], key: Callable[[T], str]\n",
    ") -> Dict[str, T]:\n",
    "    \"\"\"Aggregates an iterable of items into a dictionary, where the key is the result of\n",
    "    applying the key function to the item. If multiple items have the same key, the\n",
    "    last item is used.\"\"\"\n",
    "    return {key(item): item for item in items}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for aggregate_by_string_key\n",
    "items = [('a', 1), ('b', 2), ('a', 3)]\n",
    "test_eq(aggregate_by_string_key(items, lambda x: x[0]), {'a': ('a', 3), 'b': ('b', 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class DataWrapper(Generic[T]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Sequence[T],\n",
    "        format_item_fn: Callable[[T], str] = repr,\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.format_item_fn = format_item_fn\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"DataWrapper({repr(self.data)})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return ', '.join([self.format_item_fn(d) for d in self.data])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "\n",
    "    def print(self):\n",
    "        for d in self.data:\n",
    "            print(self.format_item_fn(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for DataWrapper\n",
    "dw = DataWrapper([1, 2, 3], format_item_fn=lambda x: f\"{x}!\")\n",
    "test_eq(str(dw), '1!, 2!, 3!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def topk_across_batches(\n",
    "    n_batches: int,\n",
    "    k: int,\n",
    "    largest: bool,\n",
    "    load_batch: Callable[[int], torch.Tensor],\n",
    "    process_batch: Callable[[torch.Tensor], torch.Tensor],\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Like torch.topk, but works across multiple batches of data. Always\n",
    "    works over the batch dimension, which is assumed to be the first dimension\n",
    "    of each batch.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_batches:\n",
    "        The number of batches to process.\n",
    "    k:\n",
    "        The number of top values to return.\n",
    "    largest:\n",
    "        Whether to return the largest or smallest values.\n",
    "    load_batch:\n",
    "        A function that takes a batch index and returns a batch of data.\n",
    "    process_batch:\n",
    "        A function that takes a batch of data and returns a tensor of\n",
    "        values, with the same first dimension size as the batch. The function\n",
    "        will return the top k of these values along the batch dimension.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        A tuple of (values, indices) where indices is a list of indices into\n",
    "        the overall dataset i.e. across all batches.\n",
    "    \"\"\"\n",
    "    all_topk_values = []\n",
    "    all_topk_indices = []\n",
    "\n",
    "    batch_sizes = []\n",
    "    # Go through each batch and find the top k closest items\n",
    "    # within that batch.\n",
    "    for batch_idx in range(n_batches):\n",
    "        batch = load_batch(batch_idx)\n",
    "\n",
    "        results = process_batch(batch)\n",
    "\n",
    "        assert (\n",
    "            results.shape[0] == batch.shape[0]\n",
    "        ), f\"Batch had {batch.shape[0]} items, but results had {results.shape[0]} items.\"\n",
    "        assert batch.shape[0] >= k, f\"Batch had {batch.shape[0]} items, but k was {k}.\"\n",
    "\n",
    "        batch_sizes.append(batch.shape[0])\n",
    "\n",
    "        topk_values, topk_indices = torch.topk(results, k=k, largest=largest, dim=0)\n",
    "        all_topk_values.append(topk_values)\n",
    "        all_topk_indices.append(topk_indices)\n",
    "\n",
    "    # Combine the results from all batches.\n",
    "    all_topk_values_tensor = torch.cat(all_topk_values)\n",
    "    all_topk_indices_tensor = torch.cat(all_topk_indices)\n",
    "\n",
    "    # Find the topk items across all batches.\n",
    "    topk = torch.topk(all_topk_values_tensor, k=k, largest=largest, dim=0)\n",
    "    topk_overall_values: torch.Tensor = topk.values\n",
    "\n",
    "    # Now we have to do math to translate the indices into all_topk_distances\n",
    "    # into indices across all data items across all batches.\n",
    "\n",
    "    # First, calculate the cumulative sum of the batch sizes.\n",
    "    # Stick a zero at the front so that we can index into this\n",
    "    # with batch_idx and know how many items were in all the\n",
    "    # previous batches.\n",
    "    prev_batch_sums = torch.cat([\n",
    "        torch.tensor([0]),\n",
    "        torch.cumsum(torch.tensor(batch_sizes), dim=0)\n",
    "    ])\n",
    "\n",
    "    topk_overall_indices = []\n",
    "    for i in topk.indices:\n",
    "        # i is the index into all_topk_distances. First, let's figure\n",
    "        # out which batch it came from.\n",
    "        batch_idx = i // k\n",
    "\n",
    "        # Now we need to figure out which index into that batch it was.\n",
    "        # all_topk_indices has the indices from the topk operation on\n",
    "        # each batch.\n",
    "        index_within_batch = torch.gather(\n",
    "            all_topk_indices_tensor, dim=0, index=i.unsqueeze(0)\n",
    "        ).squeeze(0)\n",
    "\n",
    "        # The overall index is the sum of the number of items in all\n",
    "        # previous batches, plus the index within the current batch.\n",
    "        topk_overall_indices.append(prev_batch_sums[batch_idx] + index_within_batch)\n",
    "\n",
    "    return topk_overall_values, torch.stack(topk_overall_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for topk_across_batches()\n",
    "\n",
    "batches = [\n",
    "    [100, 98, 96, 94, 92],\n",
    "    [99, 97, 95, 93, 91],\n",
    "]\n",
    "\n",
    "# Test with largest=True\n",
    "values, indices = topk_across_batches(\n",
    "    n_batches=len(batches),\n",
    "    k=4,\n",
    "    largest=True,\n",
    "    load_batch=lambda i: torch.tensor(batches[i]),\n",
    "    process_batch=lambda batch: batch,\n",
    ")\n",
    "test_eq(values, torch.tensor([100, 99, 98, 97]))\n",
    "test_eq(indices, torch.tensor([0, 5, 1, 6]))\n",
    "\n",
    "# Test with largest=False\n",
    "values, indices = topk_across_batches(\n",
    "    n_batches=len(batches),\n",
    "    k=4,\n",
    "    largest=False,\n",
    "    load_batch=lambda i: torch.tensor(batches[i]),\n",
    "    process_batch=lambda batch: batch,\n",
    ")\n",
    "test_eq(values, torch.tensor([91, 92, 93, 94]))\n",
    "test_eq(indices, torch.tensor([9, 4, 8, 3]))\n",
    "\n",
    "# Test where the last batch is smaller than the others\n",
    "batches = [\n",
    "    [100, 98, 96, 94, 92],\n",
    "    [99, 97, 95, 93, 91],\n",
    "    [90, 88, 86, 84],\n",
    "]\n",
    "values, indices = topk_across_batches(\n",
    "    n_batches=len(batches),\n",
    "    k=4,\n",
    "    largest=False,\n",
    "    load_batch=lambda i: torch.tensor(batches[i]),\n",
    "    process_batch=lambda batch: batch,\n",
    ")\n",
    "test_eq(values, torch.tensor([84, 86, 88, 90]))\n",
    "test_eq(indices, torch.tensor([13, 12, 11, 10]))\n",
    "\n",
    "# Test where the all the batches are different sizes.\n",
    "batches = [\n",
    "    [100, 98, 96, 120, 160],\n",
    "    [94, 92, 130, 140],\n",
    "    [99, 97, 95, 93, 91, 109, 110],\n",
    "    [90, 101, 104, 108],\n",
    "]\n",
    "values, indices = topk_across_batches(\n",
    "    n_batches=len(batches),\n",
    "    k=4,\n",
    "    largest=False,\n",
    "    load_batch=lambda i: torch.tensor(batches[i]),\n",
    "    process_batch=lambda batch: batch,\n",
    ")\n",
    "test_eq(values, torch.tensor([90, 91, 92, 93]))\n",
    "test_eq(indices, torch.tensor([16, 13, 6, 12]))\n",
    "\n",
    "# Test where the results are 2-D tensors\n",
    "batches = [\n",
    "    [\n",
    "        [100, 98, 96, 94, 92],\n",
    "        [99, 97, 95, 93, 91],\n",
    "        [90, 88, 86, 84, 82],\n",
    "    ],\n",
    "    [\n",
    "        [200, 198, 196, 194, 192],\n",
    "        [199, 197, 195, 193, 191],\n",
    "        [190, 188, 186, 184, 182],\n",
    "    ],\n",
    "    [\n",
    "        [300, 298, 296, 294, 292],\n",
    "        [299, 297, 295, 293, 291],\n",
    "    ],\n",
    "]\n",
    "values, indices = topk_across_batches(\n",
    "    n_batches=len(batches),\n",
    "    k=2,\n",
    "    largest=False,\n",
    "    load_batch=lambda i: torch.tensor(batches[i]),\n",
    "    process_batch=lambda batch: batch,\n",
    ")\n",
    "# fmt: off\n",
    "test_eq(values, torch.tensor([\n",
    "    [90, 88, 86, 84, 82],\n",
    "    [99, 97, 95, 93, 91],\n",
    "]))\n",
    "test_eq(indices, torch.tensor([\n",
    "    [2, 2, 2, 2, 2],\n",
    "    [1, 1, 1, 1, 1]\n",
    "]))\n",
    "# fmt: on\n",
    "\n",
    "# A more interesting 2-D example\n",
    "# fmt: off\n",
    "batches = [\n",
    "    [                          # Overall index:\n",
    "        [14,  8,  1, 13, 13],  # 0\n",
    "        [18, 13, 11,  1, 10],  # 1\n",
    "        [ 8, 16, 15, 10, 14]   # 2\n",
    "    ],\n",
    "    [\n",
    "        [12, 14, 19,  3,  1],  # 3\n",
    "        [ 1, 15, 16,  3,  0],  # 4\n",
    "        [ 7, 18,  5,  0,  6]   # 5\n",
    "    ],\n",
    "    [\n",
    "        [10,  7, 16,  9,  0],  # 6\n",
    "        [ 0, 12,  1,  9,  3],  # 7\n",
    "        [14, 18, 14,  1,  8]   # 8\n",
    "    ]\n",
    "]\n",
    "# fmt: on\n",
    "values, indices = topk_across_batches(\n",
    "    n_batches=len(batches),\n",
    "    k=2,\n",
    "    largest=False,\n",
    "    load_batch=lambda i: torch.tensor(batches[i]),\n",
    "    process_batch=lambda batch: batch,\n",
    ")\n",
    "# fmt: off\n",
    "test_eq(values, torch.tensor([\n",
    "    [0, 7, 1, 0, 0],\n",
    "    [1, 8, 1, 1, 0],\n",
    "]))\n",
    "# fmt: on\n",
    "\n",
    "# The indices picked for duplicates are apparently not stable\n",
    "# across platforms. So we can't just test for equality with a\n",
    "# known indices result. Instead let's check:\n",
    "# - that the shape of the values and indices tensors are the same\n",
    "# - that the values at the given indices are the same as what was\n",
    "#   returned in the values array.\n",
    "test_eq(indices.shape, values.shape)\n",
    "\n",
    "# Cat the batches into one big tensor so we can index into it.\n",
    "all_data = torch.cat([torch.tensor(b) for b in batches])\n",
    "\n",
    "# Test that the values at the indices are the same as the returned values\n",
    "test_eq(torch.gather(all_data, dim=0, index=indices), values)\n",
    "\n",
    "# A 3-D example: 2 batches of shape (3, 3, 2)\n",
    "# fmt: off\n",
    "batches = [\n",
    "    [\n",
    "        [\n",
    "            [15, 19],\n",
    "            [19,  8],\n",
    "            [10, 12]\n",
    "        ],\n",
    "        [\n",
    "            [17,  0],\n",
    "            [ 3, 15],\n",
    "            [ 5, 15]\n",
    "        ],\n",
    "        [\n",
    "            [19, 10],\n",
    "            [ 7, 17],\n",
    "            [ 8,  0]\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        [\n",
    "            [ 9, 15],\n",
    "            [13, 11],\n",
    "            [ 8, 15]\n",
    "        ],\n",
    "        [\n",
    "            [ 5,  0],\n",
    "            [ 3,  6],\n",
    "            [10, 15]\n",
    "        ],\n",
    "        [\n",
    "            [ 6, 19],\n",
    "            [11, 15],\n",
    "            [ 2,  5]\n",
    "        ]\n",
    "    ]\n",
    "]\n",
    "# fmt: on\n",
    "values, indices = topk_across_batches(\n",
    "    n_batches=len(batches),\n",
    "    k=2,\n",
    "    largest=False,\n",
    "    load_batch=lambda i: torch.tensor(batches[i]),\n",
    "    process_batch=lambda batch: batch,\n",
    ")\n",
    "# fmt: off\n",
    "test_eq(values, torch.tensor([\n",
    "    [\n",
    "        [ 5,  0],\n",
    "        [ 3,  6],\n",
    "        [ 2,  0]\n",
    "    ],\n",
    "    [\n",
    "        [ 6,  0],\n",
    "        [ 3,  8],\n",
    "        [ 5,  5]\n",
    "    ]\n",
    "]))\n",
    "# fmt: on\n",
    "\n",
    "# Same issue as above re: duplicates\n",
    "test_eq(indices.shape, values.shape)\n",
    "\n",
    "# Cat the batches into one big tensor so we can index into it.\n",
    "all_data = torch.cat([torch.tensor(b) for b in batches])\n",
    "\n",
    "# Test that the values at the indices are the same as the returned values\n",
    "test_eq(torch.gather(all_data, dim=0, index=indices), values)\n",
    "\n",
    "# Test with processing function\n",
    "batches = [\n",
    "    [100, 98, 96, 94, 92],\n",
    "    [99, 97, 95, 93, 91],\n",
    "    [90, 88, 86, 84],\n",
    "]\n",
    "values, indices = topk_across_batches(\n",
    "    n_batches=len(batches),\n",
    "    k=4,\n",
    "    largest=False,\n",
    "    load_batch=lambda i: torch.tensor(batches[i]),\n",
    "    process_batch=lambda batch: 2 * batch,\n",
    ")\n",
    "test_eq(values, torch.tensor([168, 172, 176, 180]))\n",
    "test_eq(indices, torch.tensor([13, 12, 11, 10]))\n",
    "\n",
    "# Test that processing function can't change size of batch\n",
    "batches = [\n",
    "    [100, 98, 96, 94, 92],\n",
    "    [99, 97, 95, 93, 91],\n",
    "    [90, 88, 86, 84],\n",
    "]\n",
    "with ExceptionExpected(ex=AssertionError):\n",
    "    values, indices = topk_across_batches(\n",
    "        n_batches=len(batches),\n",
    "        k=4,\n",
    "        largest=False,\n",
    "        load_batch=lambda i: torch.tensor(batches[i]),\n",
    "        process_batch=lambda batch: batch[:3],\n",
    "    )\n",
    "\n",
    "# Test that batch can't be smaller than k\n",
    "batches = [\n",
    "    [100, 98],\n",
    "    [99, 97, 95, 93, 91],\n",
    "    [90, 88, 86, 84],\n",
    "]\n",
    "with ExceptionExpected(ex=AssertionError):\n",
    "    values, indices = topk_across_batches(\n",
    "        n_batches=len(batches),\n",
    "        k=4,\n",
    "        largest=False,\n",
    "        load_batch=lambda i: torch.tensor(batches[i]),\n",
    "        process_batch=lambda batch: batch[:3],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
