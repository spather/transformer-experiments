{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# alternate-models\n",
    "\n",
    "> In this notebook, I train three alternate versions of the model starting from different seeds and stopping when I get to approximately the same loss as the main model (train loss = 0.9334, validation loss = 1.5063). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from datetime import datetime\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "from transformer_experiments.dataset_split import split_text_dataset\n",
    "from transformer_experiments.datasets.tinyshakespeare import (\n",
    "    TinyShakespeareDataSet,\n",
    ")\n",
    "from transformer_experiments.environments import get_environment\n",
    "from transformer_experiments.models.transformer import (\n",
    "    block_size,\n",
    "    TransformerLanguageModel\n",
    ")\n",
    "from transformer_experiments.models.transformer_training import (\n",
    "    batch_size,\n",
    "    estimate_loss,\n",
    "    eval_interval,\n",
    "    eval_iters,\n",
    "    get_batch,\n",
    ")\n",
    "from transformer_experiments.tokenizers.char_tokenizer import CharacterTokenizer\n",
    "from transformer_experiments.training_utils import CheckPointer, Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment is paperspace\n"
     ]
    }
   ],
   "source": [
    "environment = get_environment()\n",
    "print(f\"environment is {environment.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TinyShakespeareDataSet(environment.code_root / 'nbs/artifacts/input.txt')\n",
    "tokenizer = CharacterTokenizer(ts.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = split_text_dataset(ts.text, tokenizer, train_pct=0.9, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_batch_func = partial(\n",
    "    get_batch,\n",
    "    batch_size=batch_size,\n",
    "    block_size=block_size,\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    device=device,\n",
    ")\n",
    "estimate_loss_func = partial(\n",
    "    estimate_loss, eval_iters=eval_iters, get_batch_func=get_batch_func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = environment.data_root / 'alternate-models'\n",
    "experiment_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_root = experiment_dir / 'model-training' / f'{datetime.now().strftime(\"%Y%m%d\")}-training'\n",
    "training_root.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = training_root / 'training_checkpoints'\n",
    "checkpoint_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dir = training_root / 'outputs'\n",
    "outputs_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1442)\n",
    "m = TransformerLanguageModel(vocab_size=tokenizer.vocab_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = m.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=m,\n",
    "    checkpointer=CheckPointer(checkpoint_dir, f'shakespeare_{iteration}_checkpoint'),\n",
    "    get_batch_func=get_batch_func,\n",
    "    estimate_loss_func=estimate_loss_func,\n",
    "    iters_trained=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(4.2780), 'val': tensor(4.2824)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a starting point\n",
    "estimate_loss_func(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78304873bc6a4237b615165ef27617ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 499: train loss 1.7921, val loss 1.9340\n",
      "step 999: train loss 1.4039, val loss 1.6217\n",
      "step 1499: train loss 1.2720, val loss 1.5374\n",
      "step 1999: train loss 1.1952, val loss 1.5002\n",
      "step 2499: train loss 1.1385, val loss 1.4993\n",
      "step 2999: train loss 1.0794, val loss 1.4871\n",
      "step 3499: train loss 1.0229, val loss 1.4950\n"
     ]
    }
   ],
   "source": [
    "# Start with a modest learning rate and train 5000 iterations\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "trainer.train(3500, optimizer, eval_interval=eval_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14bec48501e4a65af26b03c3354a535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 499: train loss 0.9652, val loss 1.4848\n",
      "step 999: train loss 0.9445, val loss 1.4929\n",
      "step 1499: train loss 0.9293, val loss 1.5038\n",
      "step 1999: train loss 0.9129, val loss 1.5034\n",
      "step 2499: train loss 0.8994, val loss 1.5133\n",
      "step 2999: train loss 0.8875, val loss 1.5220\n",
      "step 3499: train loss 0.8733, val loss 1.5258\n"
     ]
    }
   ],
   "source": [
    "# Reduce learning rate and see if we can improve without overfitting.\n",
    "learning_rate = 3e-5\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "trainer.train(3500, optimizer, eval_interval=eval_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, tensor(0.9293), tensor(1.5038))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick the checkpoint with losses closest to our target\n",
    "checkpoint = torch.load(checkpoint_dir / f'shakespeare_{iteration}_checkpoint_000009.pt', map_location=torch.device('cpu'))\n",
    "checkpoint['iters'], checkpoint['train_loss'], checkpoint['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to /storage/alternate-models/model-training/20240112-training/outputs/shakespeare-20240112-1.pt\n"
     ]
    }
   ],
   "source": [
    "# Save checkpoint\n",
    "target_filename = outputs_dir / f'shakespeare-{datetime.now().strftime(\"%Y%m%d\")}-{iteration}.pt'\n",
    "torch.save(checkpoint['model_state_dict'], target_filename)\n",
    "print(f\"Saved checkpoint to {target_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(88)\n",
    "m = TransformerLanguageModel(vocab_size=tokenizer.vocab_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = m.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=m,\n",
    "    checkpointer=CheckPointer(checkpoint_dir, f'shakespeare_{iteration}_checkpoint'),\n",
    "    get_batch_func=get_batch_func,\n",
    "    estimate_loss_func=estimate_loss_func,\n",
    "    iters_trained=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(4.3503), 'val': tensor(4.3544)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a starting point\n",
    "estimate_loss_func(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661c1b69eef64d37803e8bb04d19c948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 499: train loss 1.7591, val loss 1.9024\n",
      "step 999: train loss 1.3939, val loss 1.6083\n",
      "step 1499: train loss 1.2698, val loss 1.5196\n",
      "step 1999: train loss 1.1861, val loss 1.4866\n",
      "step 2499: train loss 1.1303, val loss 1.4812\n",
      "step 2999: train loss 1.0706, val loss 1.4865\n",
      "step 3499: train loss 1.0152, val loss 1.4986\n"
     ]
    }
   ],
   "source": [
    "# Start with a modest learning rate and train 5000 iterations\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "trainer.train(3500, optimizer, eval_interval=eval_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a62dcfe512742f584695a9ad1eb63d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 499: train loss 0.9525, val loss 1.4957\n",
      "step 999: train loss 0.9294, val loss 1.4991\n",
      "step 1499: train loss 0.9119, val loss 1.5099\n",
      "step 1999: train loss 0.8985, val loss 1.5203\n",
      "step 2499: train loss 0.8825, val loss 1.5316\n",
      "step 2999: train loss 0.8707, val loss 1.5292\n",
      "step 3499: train loss 0.8560, val loss 1.5462\n"
     ]
    }
   ],
   "source": [
    "# Reduce learning rate and see if we can improve without overfitting.\n",
    "learning_rate = 3e-5\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "trainer.train(3500, optimizer, eval_interval=eval_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, tensor(0.9294), tensor(1.4991))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick the checkpoint with losses closest to our target\n",
    "checkpoint = torch.load(checkpoint_dir / f'shakespeare_{iteration}_checkpoint_000008.pt', map_location=torch.device('cpu'))\n",
    "checkpoint['iters'], checkpoint['train_loss'], checkpoint['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to /storage/alternate-models/model-training/20240112-training/outputs/shakespeare-20240112-2.pt\n"
     ]
    }
   ],
   "source": [
    "# Save checkpoint\n",
    "target_filename = outputs_dir / f'shakespeare-{datetime.now().strftime(\"%Y%m%d\")}-{iteration}.pt'\n",
    "torch.save(checkpoint['model_state_dict'], target_filename)\n",
    "print(f\"Saved checkpoint to {target_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(99999)\n",
    "m = TransformerLanguageModel(vocab_size=tokenizer.vocab_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = m.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=m,\n",
    "    checkpointer=CheckPointer(checkpoint_dir, f'shakespeare_{iteration}_checkpoint'),\n",
    "    get_batch_func=get_batch_func,\n",
    "    estimate_loss_func=estimate_loss_func,\n",
    "    iters_trained=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(4.1027), 'val': tensor(4.1104)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a starting point\n",
    "estimate_loss_func(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1b6b699e0d47759d8c20730c28bbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 499: train loss 1.7534, val loss 1.9022\n",
      "step 999: train loss 1.3909, val loss 1.6158\n",
      "step 1499: train loss 1.2663, val loss 1.5401\n",
      "step 1999: train loss 1.1896, val loss 1.5024\n",
      "step 2499: train loss 1.1243, val loss 1.4805\n",
      "step 2999: train loss 1.0696, val loss 1.4846\n",
      "step 3499: train loss 1.0183, val loss 1.4951\n"
     ]
    }
   ],
   "source": [
    "# Start with a modest learning rate and train 5000 iterations\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "trainer.train(3500, optimizer, eval_interval=eval_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8949a181a2284202b03d07bf4486e909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 499: train loss 0.9545, val loss 1.4912\n",
      "step 999: train loss 0.9339, val loss 1.4941\n",
      "step 1499: train loss 0.9162, val loss 1.5063\n",
      "step 1999: train loss 0.9015, val loss 1.5131\n",
      "step 2499: train loss 0.8879, val loss 1.5214\n",
      "step 2999: train loss 0.8756, val loss 1.5251\n",
      "step 3499: train loss 0.8614, val loss 1.5348\n"
     ]
    }
   ],
   "source": [
    "# Reduce learning rate and see if we can improve without overfitting.\n",
    "learning_rate = 3e-5\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "trainer.train(3500, optimizer, eval_interval=eval_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, tensor(0.9339), tensor(1.4941))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick the checkpoint with losses closest to our target\n",
    "checkpoint = torch.load(checkpoint_dir / f'shakespeare_{iteration}_checkpoint_000008.pt', map_location=torch.device('cpu'))\n",
    "checkpoint['iters'], checkpoint['train_loss'], checkpoint['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to /storage/alternate-models/model-training/20240112-training/outputs/shakespeare-20240112-3.pt\n"
     ]
    }
   ],
   "source": [
    "# Save checkpoint\n",
    "target_filename = outputs_dir / f'shakespeare-{datetime.now().strftime(\"%Y%m%d\")}-{iteration}.pt'\n",
    "torch.save(checkpoint['model_state_dict'], target_filename)\n",
    "print(f\"Saved checkpoint to {target_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-experiments",
   "language": "python",
   "name": "transformer-experiments"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
