{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# block-internals\n",
    "\n",
    "> An experiment to examine the internals of a self-attention block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experiments.block_internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "from typing import Iterator, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import click\n",
    "import torch    \n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from transformer_experiments.common.databatcher import DataBatcher\n",
    "from transformer_experiments.dataset_split import split_text_dataset\n",
    "from transformer_experiments.datasets.tinyshakespeare import (\n",
    "    TinyShakespeareDataSet,\n",
    ")\n",
    "from transformer_experiments.models.transformer import (\n",
    "    n_embed,\n",
    "    n_layer,\n",
    "    TransformerLanguageModel\n",
    ")\n",
    "from transformer_experiments.models.transformer_helpers import (\n",
    "    EncodingHelpers,\n",
    "    TransformerAccessors\n",
    ")\n",
    "from transformer_experiments.trained_models.tinyshakespeare_transformer import (\n",
    "    create_model_and_tokenizer, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class BlockInternalsResult:\n",
    "    substring: str\n",
    "    heads_output: torch.Tensor\n",
    "    proj_output: torch.Tensor\n",
    "    ffwd_output: torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class BlockInternalsExperiment:\n",
    "    \"\"\"An experiment to run a bunch of inputs through a block and save the\n",
    "    intermediate values produced for each token.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eh: EncodingHelpers,\n",
    "        accessors: TransformerAccessors,\n",
    "        block_idx: int,\n",
    "        results_folder: Path,\n",
    "    ):\n",
    "        assert block_idx >= 0 and block_idx < n_layer\n",
    "\n",
    "        self.eh = eh\n",
    "        self.accessors = accessors\n",
    "        self.block_idx = block_idx\n",
    "        self.results_folder = results_folder\n",
    "\n",
    "    def _filename_stem(self):\n",
    "        return f'block{self.block_idx}_internals'\n",
    "\n",
    "    def _input_filename(self, batch: int):\n",
    "        return f'{self._filename_stem()}_input_{batch:03d}.pt'\n",
    "\n",
    "    def _head_output_filename(self, batch: int):\n",
    "        return f'{self._filename_stem()}_head_output_{batch:03d}.pt'\n",
    "\n",
    "    def _proj_output_filename(self, batch: int):\n",
    "        return f'{self._filename_stem()}_proj_output_{batch:03d}.pt'\n",
    "\n",
    "    def _ffwd_output_filename(self, batch: int):\n",
    "        return f'{self._filename_stem()}_ffwd_output_{batch:03d}.pt'\n",
    "\n",
    "    def run(self, data_batcher: DataBatcher):\n",
    "        for batch_idx, batch in tqdm(enumerate(data_batcher)):\n",
    "            x = self.accessors.embed_tokens(batch)\n",
    "\n",
    "            # Run the encoded batch through the blocks up to the one we're interested in\n",
    "            for i in range(self.block_idx):\n",
    "                x = self.accessors.m.blocks[i](x)\n",
    "\n",
    "            # Copy the block we're interested in\n",
    "            block, io_accessor = self.accessors.copy_block_from_model(\n",
    "                block_idx=self.block_idx\n",
    "            )\n",
    "            _ = block(x)  # Run the block\n",
    "\n",
    "            # Grab the outputs of interest\n",
    "            heads_output = io_accessor.input('sa.proj')\n",
    "            proj_output = io_accessor.output('sa.proj')\n",
    "            ffwd_output = io_accessor.output('ffwd')\n",
    "\n",
    "            torch.save(\n",
    "                batch.clone(), self.results_folder / self._input_filename(batch_idx)\n",
    "            )\n",
    "            torch.save(\n",
    "                heads_output,\n",
    "                self.results_folder / self._head_output_filename(batch_idx),\n",
    "            )\n",
    "            torch.save(\n",
    "                proj_output, self.results_folder / self._proj_output_filename(batch_idx)\n",
    "            )\n",
    "            torch.save(\n",
    "                ffwd_output, self.results_folder / self._ffwd_output_filename(batch_idx)\n",
    "            )\n",
    "\n",
    "    def load(\n",
    "        self,\n",
    "    ) -> Iterator[Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]:\n",
    "        input_files = sorted(\n",
    "            self.results_folder.glob(f'{self._filename_stem()}_input_*.pt')\n",
    "        )\n",
    "        head_output_files = sorted(\n",
    "            self.results_folder.glob(f'{self._filename_stem()}_head_output_*.pt')\n",
    "        )\n",
    "        proj_output_files = sorted(\n",
    "            self.results_folder.glob(f'{self._filename_stem()}_proj_output_*.pt')\n",
    "        )\n",
    "        ffwd_output_files = sorted(\n",
    "            self.results_folder.glob(f'{self._filename_stem()}_ffwd_output_*.pt')\n",
    "        )\n",
    "\n",
    "        assert (\n",
    "            len(input_files)\n",
    "            == len(head_output_files)\n",
    "            == len(proj_output_files)\n",
    "            == len(ffwd_output_files)\n",
    "        )\n",
    "\n",
    "        for input_file, head_output_file, proj_output_file, ffwd_output_file in zip(\n",
    "            input_files, head_output_files, proj_output_files, ffwd_output_files\n",
    "        ):\n",
    "            assert input_file.exists()\n",
    "            assert head_output_file.exists()\n",
    "            assert proj_output_file.exists()\n",
    "            assert ffwd_output_file.exists()\n",
    "\n",
    "            yield torch.load(input_file), torch.load(head_output_file), torch.load(\n",
    "                proj_output_file\n",
    "            ), torch.load(ffwd_output_file)\n",
    "\n",
    "    def raw_results(self) -> Iterator[BlockInternalsResult]:\n",
    "        for inputs, head_output, proj_output, ffwd_output in self.load():\n",
    "            n_samples, s_len = inputs.shape\n",
    "            for i in range(n_samples):\n",
    "                for j in range(s_len):\n",
    "                    substring = self.eh.stringify_tokens(inputs[i][: j + 1])\n",
    "                    yield BlockInternalsResult(\n",
    "                        substring,\n",
    "                        head_output[i][j],\n",
    "                        proj_output[i][j],\n",
    "                        ffwd_output[i][j],\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TinyShakespeareDataSet(cache_file='../artifacts/input.txt')\n",
    "m, tokenizer = create_model_and_tokenizer(\n",
    "    saved_model_filename='../artifacts/shakespeare.pt',\n",
    "    dataset=ts,\n",
    "    device=device,\n",
    ")\n",
    "_, val_data = split_text_dataset(ts.text, tokenizer, train_pct=0.9)\n",
    "encoding_helpers = EncodingHelpers(m, tokenizer, device)\n",
    "accessors = TransformerAccessors(m, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2c0f619b6a4ae0bb03c766c37eb7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test for BlockInternalsExperiment\n",
    "max_batch_size = 64\n",
    "s_len = 3\n",
    "n_full_batches = 2\n",
    "last_batch_size = 12\n",
    "data_batcher = DataBatcher(\n",
    "    data=val_data[\n",
    "        0 : n_full_batches * max_batch_size * s_len + last_batch_size * s_len\n",
    "    ],\n",
    "    sample_len=s_len,\n",
    "    max_batch_size=max_batch_size,\n",
    "    stride=s_len,\n",
    ")\n",
    "\n",
    "# Create a temp directory to store the results that will get\n",
    "# cleaned up at the end.\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    test_exp = BlockInternalsExperiment(\n",
    "        eh=encoding_helpers,\n",
    "        accessors=accessors,\n",
    "        block_idx=0,\n",
    "        results_folder=Path(tmpdirname),\n",
    "    )\n",
    "    test_exp.run(data_batcher=data_batcher)\n",
    "\n",
    "    expected_batch_shapes = [\n",
    "        (max_batch_size, s_len),\n",
    "        (max_batch_size, s_len),\n",
    "        (last_batch_size, s_len),\n",
    "    ]\n",
    "    expected_output_shapes = [\n",
    "        (max_batch_size, s_len, n_embed),\n",
    "        (max_batch_size, s_len, n_embed),\n",
    "        (last_batch_size, s_len, n_embed),\n",
    "    ]\n",
    "\n",
    "    for i, (inputs, heads_outputs, proj_outputs, ffwd_outputs) in enumerate(\n",
    "        test_exp.load()\n",
    "    ):\n",
    "        test_eq(inputs.shape, expected_batch_shapes[i])\n",
    "        test_eq(heads_outputs.shape, expected_output_shapes[i])\n",
    "        test_eq(proj_outputs.shape, expected_output_shapes[i])\n",
    "        test_eq(ffwd_outputs.shape, expected_output_shapes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@click.command()\n",
    "@click.argument('model_weights_filename', type=click.Path(exists=True))\n",
    "@click.argument('dataset_cache_filename', type=click.Path(exists=True))\n",
    "@click.argument('output_folder', type=click.Path(exists=True))\n",
    "@click.option('-b', '--block_idx', required=True, type=click.IntRange(min=0, max=n_layer, max_open=True))\n",
    "def run(\n",
    "    model_weights_filename: str,\n",
    "    dataset_cache_filename: str,\n",
    "    output_folder: str,\n",
    "    block_idx: int,\n",
    "):\n",
    "    click.echo(f\"Running block internals experiment for block {block_idx}\")\n",
    "    \n",
    "    # Instantiate the model, tokenizer, and dataset\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    click.echo(f\"device is {device}\")\n",
    "\n",
    "    ts = TinyShakespeareDataSet(cache_file=dataset_cache_filename)\n",
    "    m, tokenizer = create_model_and_tokenizer(\n",
    "        saved_model_filename=model_weights_filename,\n",
    "        dataset=ts,\n",
    "        device=device,\n",
    "    )\n",
    "    _, val_data = split_text_dataset(ts.text, tokenizer, train_pct=0.9)\n",
    "\n",
    "    encoding_helpers = EncodingHelpers(m, tokenizer, device)\n",
    "    accessors = TransformerAccessors(m, device)\n",
    "\n",
    "    # Create the experiment\n",
    "    exp = BlockInternalsExperiment(\n",
    "        encoding_helpers, accessors, block_idx, Path(output_folder)\n",
    "    )\n",
    "\n",
    "    # Run the experiment\n",
    "    data_batcher = DataBatcher(\n",
    "        data=val_data,\n",
    "        sample_len=3,\n",
    "        max_batch_size=64,\n",
    "        stride=96,\n",
    "    )\n",
    "    exp.run(data_batcher=data_batcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
