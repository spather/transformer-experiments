{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final-ffwd\n",
    "\n",
    "> Like BlockInternalsExperiment, but only saves the final t_i of the final ffwd layer. Useful for longer strings that would generate too much data otherwise. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experiments.final_ffwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from collections import defaultdict, OrderedDict\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import math\n",
    "from matplotlib.axes import Axes\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "from typing import Callable, Dict, Iterable, Iterator, List, Optional, Protocol, Sequence, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import click\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from transformer_experiments.common.databatcher import DataBatcher\n",
    "from transformer_experiments.common.substring_generator import all_unique_substrings\n",
    "from transformer_experiments.common.utils import topk_across_batches\n",
    "from transformer_experiments.dataset_split import split_text_dataset\n",
    "from transformer_experiments.datasets.tinyshakespeare import (\n",
    "    TinyShakespeareDataSet,\n",
    ")\n",
    "from transformer_experiments.models.transformer import (\n",
    "    block_size,\n",
    "    n_embed,\n",
    "    n_layer,\n",
    "    TransformerLanguageModel\n",
    ")\n",
    "from transformer_experiments.models.transformer_helpers import (\n",
    "    EncodingHelpers,\n",
    "    LogitsWrapper,\n",
    "    TransformerAccessors\n",
    ")\n",
    "from transformer_experiments.tokenizers.char_tokenizer import CharacterTokenizer\n",
    "from transformer_experiments.trained_models.tinyshakespeare_transformer import (\n",
    "    create_model_and_tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TinyShakespeareDataSet(cache_file='../artifacts/input.txt')\n",
    "m, tokenizer = create_model_and_tokenizer(\n",
    "    saved_model_filename='../artifacts/shakespeare.pt',\n",
    "    dataset=ts,\n",
    "    device=device,\n",
    ")\n",
    "_, val_data = split_text_dataset(ts.text, tokenizer, train_pct=0.9, device=device)\n",
    "encoding_helpers = EncodingHelpers(tokenizer, device)\n",
    "accessors = TransformerAccessors(m, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class FinalFFWDExperiment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        eh: EncodingHelpers,\n",
    "        accessors: TransformerAccessors,\n",
    "        strings: Sequence[str],\n",
    "        output_dir: Path,\n",
    "        batch_size: int = 10000,\n",
    "    ):\n",
    "        self.eh = eh\n",
    "        self.accessors = accessors\n",
    "        self.strings = strings\n",
    "        self.output_dir = output_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Create a map of string to index to enable fast lookup.\n",
    "        self.idx_map = OrderedDict((s, idx) for idx, s in enumerate(self.strings))\n",
    "\n",
    "        self.n_batches = math.ceil(len(self.strings) / self.batch_size)\n",
    "\n",
    "    def sample_length(self) -> int:\n",
    "        return len(self.strings[0])\n",
    "\n",
    "    def run(self, disable_progress_bars: bool = False):\n",
    "        for batch_idx in tqdm(range(self.n_batches), disable=disable_progress_bars):\n",
    "            start_idx = batch_idx * self.batch_size\n",
    "            end_idx = start_idx + self.batch_size\n",
    "            batch_strings = self.strings[start_idx:end_idx]\n",
    "            self._run_batch(batch_idx, batch_strings)\n",
    "\n",
    "    def _ffwd_output_filename(self, batch_idx: int, block_idx: int) -> Path:\n",
    "        return self.output_dir / f'ffwd_output-{batch_idx:04d}-{block_idx:02d}.pt'\n",
    "\n",
    "    def _run_batch(self, batch_idx: int, batch_strings: Sequence[str]):\n",
    "        tokens = self.eh.tokenize_strings(batch_strings)\n",
    "        embeddings = self.accessors.embed_tokens(tokens)\n",
    "\n",
    "        # Run the embeddings through the model.\n",
    "        _, io_accessors = self.accessors.run_model(embeddings)\n",
    "\n",
    "        # Write the result of the final block's final t_i to disk.\n",
    "        block_idx = n_layer - 1\n",
    "        torch.save(\n",
    "            io_accessors[block_idx].output('ffwd')[:, -1, :].clone(),\n",
    "            self._ffwd_output_filename(batch_idx, block_idx),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for FinalFFWDExperiment\n",
    "s_len = 3\n",
    "strings = all_unique_substrings(ts.text[:100], s_len)\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    output_dir = Path(tmpdirname)\n",
    "    experiment = FinalFFWDExperiment(\n",
    "        encoding_helpers, accessors, strings, output_dir=output_dir, batch_size=10\n",
    "    )\n",
    "    test_eq(experiment.sample_length(), s_len)\n",
    "    experiment.run(disable_progress_bars=True)\n",
    "\n",
    "    # Test the expected files exist\n",
    "    block_idx = n_layer - 1\n",
    "    for batch_idx in range(experiment.n_batches):\n",
    "        test_eq(experiment._ffwd_output_filename(batch_idx, block_idx).exists(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@click.command()\n",
    "@click.argument(\"model_weights_filename\", type=click.Path(exists=True))\n",
    "@click.argument(\"dataset_cache_filename\", type=click.Path(exists=True))\n",
    "@click.argument(\"output_folder\", type=click.Path(exists=True))\n",
    "@click.option(\n",
    "    \"-s\",\n",
    "    \"--sample_len\",\n",
    "    required=True,\n",
    "    type=click.IntRange(min=1, max=block_size),\n",
    ")\n",
    "@click.option(\n",
    "    \"-m\",\n",
    "    \"--max_batch_size\",\n",
    "    required=False,\n",
    "    type=click.IntRange(min=1),\n",
    "    default=10000,\n",
    ")\n",
    "def run(\n",
    "    model_weights_filename: str,\n",
    "    dataset_cache_filename: str,\n",
    "    output_folder: str,\n",
    "    sample_len: int,\n",
    "    max_batch_size: int,\n",
    "):\n",
    "    click.echo(f\"Running block internals experiment for with:\")\n",
    "    click.echo(f\"  model weights: {model_weights_filename}\")\n",
    "    click.echo(f\"  dataset cache: {dataset_cache_filename}\")\n",
    "    click.echo(f\"  output folder: {output_folder}\")\n",
    "    click.echo(f\"  sample length: {sample_len}\")\n",
    "    click.echo(f\"  max batch size: {max_batch_size}\")\n",
    "\n",
    "    # Instantiate the model, tokenizer, and dataset\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    click.echo(f\"device is {device}\")\n",
    "\n",
    "    ts = TinyShakespeareDataSet(cache_file=dataset_cache_filename)\n",
    "    m, tokenizer = create_model_and_tokenizer(\n",
    "        saved_model_filename=model_weights_filename,\n",
    "        dataset=ts,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    strings = all_unique_substrings(ts.text, sample_len)\n",
    "\n",
    "    encoding_helpers = EncodingHelpers(tokenizer, device)\n",
    "    accessors = TransformerAccessors(m, device)\n",
    "\n",
    "    # Create the experiment\n",
    "    exp = FinalFFWDExperiment(\n",
    "        encoding_helpers, accessors, strings, Path(output_folder), max_batch_size\n",
    "    )\n",
    "\n",
    "    exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
