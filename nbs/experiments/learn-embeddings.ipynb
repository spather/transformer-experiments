{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learn-embeddings\n",
    "\n",
    "> This notebook contains the code for learning embeddings at various stages of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import gc\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Optional, Iterable, Protocol, Sequence, Tuple, TypeVar, Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "import math\n",
    "from matplotlib.axes import Axes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "import tempfile\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "from transformer_experiments.common.substring_generator import all_unique_substrings\n",
    "from transformer_experiments.common.text_analysis import (\n",
    "    build_next_token_map,\n",
    "    SubstringFrequencyAnalysis,\n",
    "    top_nonzero_tokens\n",
    ")\n",
    "from transformer_experiments.common.utils import (\n",
    "    aggregate_by_string_key,\n",
    "    DataWrapper,\n",
    "    topk_across_batches,\n",
    ")\n",
    "from transformer_experiments.dataset_split import split_text_dataset\n",
    "from transformer_experiments.datasets.tinyshakespeare import (\n",
    "    TinyShakespeareDataSet,\n",
    ")\n",
    "from transformer_experiments.environments import get_environment\n",
    "from transformer_experiments.models.transformer import (\n",
    "    block_size,\n",
    "    n_embed,\n",
    "    n_layer,\n",
    "    TransformerLanguageModel\n",
    ")\n",
    "from transformer_experiments.models.transformer_helpers import (\n",
    "    unsqueeze_emb,\n",
    "    EncodingHelpers,\n",
    "    LogitsWrapper,\n",
    "    TransformerAccessors\n",
    ")\n",
    "from transformer_experiments.trained_models.tinyshakespeare_transformer import (\n",
    "    create_model_and_tokenizer,\n",
    "    FilenameForToken,\n",
    ")\n",
    "from transformer_experiments.training_utils import CheckPointer, GetBatchFunction, Trainer\n",
    "from transformer_experiments.experiments.block_internals import (\n",
    "    BlockInternalsAccessors,\n",
    "    BlockInternalsExperiment,\n",
    "    BatchedBlockInternalsExperiment,\n",
    "    BlockInternalsAnalysis,\n",
    "    batch_cosine_sim,\n",
    ")\n",
    "from transformer_experiments.experiments.cosine_sims import (\n",
    "    filter_on_prefiltered_results,\n",
    "    pre_filter_cosine_sim_results,\n",
    ")\n",
    "from transformer_experiments.experiments.final_ffwd import FinalFFWDExperiment\n",
    "from transformer_experiments.experiments.similar_strings import (\n",
    "    SimilarStringsData,\n",
    "    SimilarStringsExperiment,\n",
    "    SimilarStringsResult\n",
    ")\n",
    "from transformer_experiments.experiments.logit_lens import LogitLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment is paperspace\n"
     ]
    }
   ],
   "source": [
    "environment = get_environment()\n",
    "print(f\"environment is {environment.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ts = TinyShakespeareDataSet(cache_file=environment.code_root / 'nbs/artifacts/input.txt')\n",
    "m, tokenizer = create_model_and_tokenizer(\n",
    "    saved_model_filename=environment.code_root / 'nbs/artifacts/shakespeare-20231112.pt',\n",
    "    dataset=ts,\n",
    "    device=device,\n",
    ")\n",
    "_, val_data = split_text_dataset(ts.text, tokenizer, train_pct=0.9, device=device)\n",
    "encoding_helpers = EncodingHelpers(tokenizer, device)\n",
    "accessors = TransformerAccessors(m, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"device is {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Idea\n",
    "\n",
    "At any point in the transformer after tokens have been embedded, we can think of the remainder of the pipeline as a function that transforms from embedding space ($\\mathbb{R}^{384}$ since `n_embed = 384`) to logit space ($\\mathbb{R}^{65}$  `vocab_size=65`). For a given token, say the letter `a`, we might ask: at this point in the pipeline, what embedding will produce output logits that result in a probability very close to 1 for this token? In other words, what embedding, if provided as input at this point, will result in the transformer predicting the given token as the next token? \n",
    "\n",
    "I suspect this is difficult to determine analytically, especially for earlier parts of the pipeline where the input embedding has to go through many transformer blocks before output logits are computed. But, we can actually *learn* the embeddings that result in the right kind of logits. \n",
    "\n",
    "I say embeddings - plural - because it seems there isn't just one unique embedding for each token. Likely, there is some subspace of the full embeddings space, $\\mathbb{R}^{384}$, that corresponds to specific tokens being predicted. While I don't know of a way to find the bounds of this space directly, we can learn several embeddings within the space and then work out a good enough approximation of the space. \n",
    "\n",
    "To do this, we set up a typical deep-learning problem:\n",
    "* The portion of the transformer we're interested in is fixed; the embeddings input into it are the parameters we're optimizing and they start with random initialization. \n",
    "* We do a forward pass, computing the logits from the embeddings we have.\n",
    "* We compute negative log likelihood loss relative to the token we're trying to learn embeddings for. \n",
    "* We do a backward pass, adjusting the values of the embeddings according to the gradients.\n",
    "\n",
    "It works remarkably well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_embedding_for_char(\n",
    "    target_char: str,\n",
    "    embedding_to_logits: Callable[[torch.Tensor], torch.Tensor],\n",
    "    n_embeddings_to_learn: int = 1,\n",
    "    learning_rate: float = 3e-4,\n",
    "    minimum_loss: float = 1e-4,\n",
    "    max_iters: int = 50000,\n",
    "    device: str = device,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Given a character, learns the embedding that,\n",
    "    when given as input to the `embedding_to_logits`\n",
    "    function, produces the logits select that character\n",
    "    with probability almost 1.\"\"\"\n",
    "    assert len(target_char) == 1\n",
    "    target = torch.tensor(tokenizer.encode(target_char), device=device)\n",
    "\n",
    "    lsfm = nn.LogSoftmax(dim=-1)\n",
    "    lsfm.to(device)\n",
    "\n",
    "    x = torch.nn.Parameter(\n",
    "        torch.randn(n_embeddings_to_learn, 1, n_embed, device=device),\n",
    "        requires_grad=True,\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW([x], lr=learning_rate)\n",
    "    eval_iters = max_iters // 10\n",
    "\n",
    "    print(f\"Optimizing embedding for {repr(target_char)}\")\n",
    "    for step in range(max_iters):\n",
    "        logits = embedding_to_logits(x)\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        yhat = lsfm(logits.view(B * T, C))\n",
    "        loss = F.nll_loss(yhat, target.expand(n_embeddings_to_learn))\n",
    "\n",
    "        if loss < minimum_loss:\n",
    "            print(f\"ending training at step {step:>5}: loss {loss.item():.6f}\")\n",
    "            break\n",
    "\n",
    "        if step % eval_iters == 0:\n",
    "            print(f\"step {step:>5}: loss {loss.item():.6f}\")\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return x.data.detach(), loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that encapsulate the final output head of the transformer, after all the blocks (basically, a layer norm followed by a linear layer going from embedding space to token space):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_output_head_function(m: TransformerLanguageModel) -> Callable[[torch.Tensor], torch.Tensor]:\n",
    "    ln_f = nn.LayerNorm(n_embed)\n",
    "    lm_head = nn.Linear(n_embed, tokenizer.vocab_size)\n",
    "\n",
    "    ln_f.load_state_dict(m.ln_f.state_dict())\n",
    "    lm_head.load_state_dict(m.lm_head.state_dict())\n",
    "\n",
    "    ln_f.to(device)\n",
    "    lm_head.to(device)\n",
    "\n",
    "    return lambda x: lm_head(ln_f(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this to learn, e.g., an embedding that, at the very end of the transformer, after all the blocks, is likely to produce a next token probability for `a` of nearly 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing embedding for 'a'\n",
      "step     0: loss 2.809722\n",
      "step  5000: loss 0.009828\n",
      "step 10000: loss 0.000751\n",
      "ending training at step 14316: loss 0.000100\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "emb_a, _ = learn_embedding_for_char('a', transformer_output_head_function(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this, by sending the resulting embedding through the output head and plotting the next token probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logits = LogitsWrapper(accessors.logits_from_embedding(unsqueeze_emb(emb_a)), tokenizer)\n",
    "logits.plot_probs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was just one possible embedding that selects `a`. We can learn others. The following code snippet will learn 100 such embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing embedding for 'a'\n",
      "step     0: loss 4.480039\n",
      "step  5000: loss 0.014915\n",
      "step 10000: loss 0.001181\n",
      "step 15000: loss 0.000119\n",
      "ending training at step 15389: loss 0.000100\n"
     ]
    }
   ],
   "source": [
    "# Try learning more than one embedding\n",
    "\n",
    "torch.manual_seed(42)\n",
    "multi_emb_a, _ = learn_embedding_for_char(\n",
    "    \"a\", transformer_output_head_function(m), n_embeddings_to_learn=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 384])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_emb_a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convince ourselves that these are sufficiently varied by examining some stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_emb_stats(multi_embs: torch.Tensor):\n",
    "    norms = torch.norm(multi_embs[:, 0, :], dim=-1)\n",
    "    print(f\"norms: mean {norms.mean().item():.6f}, std {norms.std().item():.6f}\")\n",
    "    print(f\"norms: min {norms.min().item():.6f}, max {norms.max().item():.6f}\")\n",
    "\n",
    "    norm_diffs = []\n",
    "    for i in range(multi_embs.shape[0]):\n",
    "        for j in range(multi_embs.shape[0]):\n",
    "            if i != j:\n",
    "                norm_diffs.append(torch.norm(multi_embs[i, 0, :] - multi_embs[j, 0, :]))\n",
    "\n",
    "    norm_diffs = torch.tensor(norm_diffs)\n",
    "    print(f\"norm diffs: mean {norm_diffs.mean().item():.6f}, std {norm_diffs.std().item():.6f}\")\n",
    "    print(f\"norm diffs: min {norm_diffs.min().item():.6f}, max {norm_diffs.max().item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norms: mean 16.764254, std 0.759987\n",
      "norms: min 15.078418, max 18.306231\n",
      "norm diffs: mean 16.599855, std 0.760862\n",
      "norm diffs: min 13.795146, max 19.167126\n"
     ]
    }
   ],
   "source": [
    "multi_emb_stats(multi_emb_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The norms of the embeddings range from about 15 to about 18, with a mean of about 16.7. Then if we compute the differences between all the learned embeddings and take the norms of those differences, we see that they are quite large: the differences range from 13.8 to about 19, with a mean of 16.6. If we'd somehow learned 100 nearly identical vectors, the differences would be a lot smaller. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define a function that implements the portion of a the transformer pipeline from a given block onwards. E.g. If called with `n=3`, this would implement blocks index 3, 4, and 5, plus the transformer output head. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_n_on_function(m: TransformerLanguageModel, n: int) -> Callable[[torch.Tensor], torch.Tensor]:\n",
    "    assert n >= 0 and n < n_layer, \"n must be in [0, n_layer)\"\n",
    "\n",
    "    blocks, _ = zip(\n",
    "        *[\n",
    "            accessors.copy_block_from_model(block_idx=i)\n",
    "            for i in range(n, n_layer)\n",
    "        ]\n",
    "    )\n",
    "    blocks_module = torch.nn.Sequential(*blocks)\n",
    "    output_head = transformer_output_head_function(m)\n",
    "    return lambda x: output_head(blocks_module(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now learn 100 embeddings for the letter a, from block index 5 (the last block) in transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing embedding for 'a'\n",
      "step     0: loss 4.540399\n",
      "step  5000: loss 0.003825\n",
      "step 10000: loss 0.000321\n",
      "ending training at step 12689: loss 0.000100\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "multi_emb_a_5, _ = learn_embedding_for_char('a', block_n_on_function(m, n=5), n_embeddings_to_learn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norms: mean 17.498230, std 0.640826\n",
      "norms: min 15.787105, max 19.508923\n",
      "norm diffs: mean 23.514679, std 0.885002\n",
      "norm diffs: min 20.520039, max 27.281914\n"
     ]
    }
   ],
   "source": [
    "multi_emb_stats(multi_emb_a_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These show even more variation than the ones learned for the output head alone. \n",
    "\n",
    "As a spot check, let's plot an arbitrary one of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try an arbitrary embedding and plot it.\n",
    "logits = LogitsWrapper(block_n_on_function(m, n=5)(unsqueeze_emb(multi_emb_a_5[22, 0, :])).detach(), tokenizer)\n",
    "logits.plot_probs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It produces `a` with probability nearly 1, as we'd expect. I spot checked with several more examples and they all looked right. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can learn embeddings from an earlier point, say from block index 1 on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing embedding for 'a'\n",
      "step     0: loss 3.682070\n",
      "step  5000: loss 0.003850\n",
      "step 10000: loss 0.000303\n",
      "ending training at step 12545: loss 0.000100\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "multi_emb_a_1, _ = learn_embedding_for_char('a', block_n_on_function(m, n=1), n_embeddings_to_learn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norms: mean 17.507275, std 0.661216\n",
      "norms: min 15.955662, max 18.931648\n",
      "norm diffs: mean 24.441858, std 0.949635\n",
      "norm diffs: min 21.217833, max 27.614788\n"
     ]
    }
   ],
   "source": [
    "multi_emb_stats(multi_emb_a_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more variation now. Again, let's spot check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try an arbitrary embedding and plot it.\n",
    "logits = LogitsWrapper(block_n_on_function(m, n=1)(unsqueeze_emb(multi_emb_a_1[63, 0, :])).detach(), tokenizer)\n",
    "logits.plot_probs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in place, we can now learn embeddings for all the tokens and save them for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn embeddings for each token and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = environment.data_root / 'learned_embeddings'\n",
    "results_folder.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_for_token = FilenameForToken(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embeddings_to_learn = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn embeddings for just the output head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_to_logits_function = transformer_output_head_function(m)\n",
    "sub_dir = results_folder / f'no_blocks'\n",
    "sub_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae00820dd4e487e987375b37cc1b91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing embedding for '\\n'\n",
      "step     0: loss 4.873875\n",
      "step  5000: loss 0.014350\n",
      "step 10000: loss 0.001145\n",
      "step 15000: loss 0.000115\n",
      "ending training at step 15317: loss 0.000100\n",
      "Optimizing embedding for ' '\n",
      "step     0: loss 4.264826\n",
      "step  5000: loss 0.016041\n",
      "step 10000: loss 0.001354\n",
      "step 15000: loss 0.000146\n",
      "ending training at step 15895: loss 0.000100\n",
      "Optimizing embedding for '!'\n",
      "step     0: loss 4.792295\n",
      "step  5000: loss 0.012945\n",
      "step 10000: loss 0.000995\n",
      "ending training at step 14904: loss 0.000100\n",
      "Optimizing embedding for '$'\n",
      "step     0: loss 5.333682\n",
      "step  5000: loss 0.057611\n",
      "step 10000: loss 0.005992\n",
      "step 15000: loss 0.001061\n",
      "step 20000: loss 0.000405\n",
      "step 25000: loss 0.000354\n",
      "step 30000: loss 0.000354\n",
      "step 35000: loss 0.000354\n",
      "step 40000: loss 0.000354\n",
      "step 45000: loss 0.000354\n",
      "Optimizing embedding for '&'\n",
      "step     0: loss 5.297050\n",
      "step  5000: loss 0.059776\n",
      "step 10000: loss 0.006204\n",
      "step 15000: loss 0.001161\n",
      "step 20000: loss 0.000509\n",
      "step 25000: loss 0.000470\n",
      "step 30000: loss 0.000469\n",
      "step 35000: loss 0.000469\n",
      "step 40000: loss 0.000469\n",
      "step 45000: loss 0.000469\n",
      "Optimizing embedding for \"'\"\n",
      "step     0: loss 4.427784\n",
      "step  5000: loss 0.009131\n",
      "step 10000: loss 0.000668\n",
      "ending training at step 13922: loss 0.000100\n",
      "Optimizing embedding for ','\n",
      "step     0: loss 4.773814\n",
      "step  5000: loss 0.026466\n",
      "step 10000: loss 0.002318\n",
      "step 15000: loss 0.000273\n",
      "ending training at step 17611: loss 0.000100\n",
      "Optimizing embedding for '-'\n",
      "step     0: loss 4.811676\n",
      "step  5000: loss 0.010475\n",
      "step 10000: loss 0.000778\n",
      "ending training at step 14270: loss 0.000100\n",
      "Optimizing embedding for '.'\n",
      "step     0: loss 4.674299\n",
      "step  5000: loss 0.027865\n",
      "step 10000: loss 0.002514\n",
      "step 15000: loss 0.000314\n",
      "ending training at step 18179: loss 0.000100\n",
      "Optimizing embedding for '3'\n",
      "step     0: loss 5.365812\n",
      "step  5000: loss 0.034804\n",
      "step 10000: loss 0.002992\n",
      "step 15000: loss 0.000367\n",
      "ending training at step 18638: loss 0.000100\n",
      "Optimizing embedding for ':'\n",
      "step     0: loss 4.741449\n",
      "step  5000: loss 0.012953\n",
      "step 10000: loss 0.001016\n",
      "ending training at step 14978: loss 0.000100\n",
      "Optimizing embedding for ';'\n",
      "step     0: loss 4.844121\n",
      "step  5000: loss 0.027275\n",
      "step 10000: loss 0.002482\n",
      "step 15000: loss 0.000309\n",
      "ending training at step 18092: loss 0.000100\n",
      "Optimizing embedding for '?'\n",
      "step     0: loss 5.005649\n",
      "step  5000: loss 0.012930\n",
      "step 10000: loss 0.000989\n",
      "ending training at step 14877: loss 0.000100\n",
      "Optimizing embedding for 'A'\n",
      "step     0: loss 5.113842\n",
      "step  5000: loss 0.015388\n",
      "step 10000: loss 0.001204\n",
      "step 15000: loss 0.000119\n",
      "ending training at step 15387: loss 0.000100\n",
      "Optimizing embedding for 'B'\n",
      "step     0: loss 5.215371\n",
      "step  5000: loss 0.017506\n",
      "step 10000: loss 0.001359\n",
      "step 15000: loss 0.000136\n",
      "ending training at step 15706: loss 0.000100\n",
      "Optimizing embedding for 'C'\n",
      "step     0: loss 4.807229\n",
      "step  5000: loss 0.014966\n",
      "step 10000: loss 0.001172\n",
      "step 15000: loss 0.000117\n",
      "ending training at step 15346: loss 0.000100\n",
      "Optimizing embedding for 'D'\n",
      "step     0: loss 5.175806\n",
      "step  5000: loss 0.015056\n",
      "step 10000: loss 0.001147\n",
      "step 15000: loss 0.000111\n",
      "ending training at step 15225: loss 0.000100\n",
      "Optimizing embedding for 'E'\n",
      "step     0: loss 4.931046\n",
      "step  5000: loss 0.012371\n",
      "step 10000: loss 0.000935\n",
      "ending training at step 14723: loss 0.000100\n",
      "Optimizing embedding for 'F'\n",
      "step     0: loss 5.209412\n",
      "step  5000: loss 0.017060\n",
      "step 10000: loss 0.001304\n",
      "step 15000: loss 0.000128\n",
      "ending training at step 15544: loss 0.000100\n",
      "Optimizing embedding for 'G'\n",
      "step     0: loss 5.248595\n",
      "step  5000: loss 0.013182\n",
      "step 10000: loss 0.000999\n",
      "ending training at step 14870: loss 0.000100\n",
      "Optimizing embedding for 'H'\n",
      "step     0: loss 5.152567\n",
      "step  5000: loss 0.017908\n",
      "step 10000: loss 0.001409\n",
      "step 15000: loss 0.000143\n",
      "ending training at step 15819: loss 0.000100\n",
      "Optimizing embedding for 'I'\n",
      "step     0: loss 4.950461\n",
      "step  5000: loss 0.014191\n",
      "step 10000: loss 0.001090\n",
      "step 15000: loss 0.000105\n",
      "ending training at step 15105: loss 0.000100\n",
      "Optimizing embedding for 'J'\n",
      "step     0: loss 5.064102\n",
      "step  5000: loss 0.022407\n",
      "step 10000: loss 0.001807\n",
      "step 15000: loss 0.000198\n",
      "ending training at step 16662: loss 0.000100\n",
      "Optimizing embedding for 'K'\n",
      "step     0: loss 4.987291\n",
      "step  5000: loss 0.014709\n",
      "step 10000: loss 0.001141\n",
      "step 15000: loss 0.000112\n",
      "ending training at step 15245: loss 0.000100\n",
      "Optimizing embedding for 'L'\n",
      "step     0: loss 5.183823\n",
      "step  5000: loss 0.014404\n",
      "step 10000: loss 0.001087\n",
      "step 15000: loss 0.000104\n",
      "ending training at step 15082: loss 0.000100\n",
      "Optimizing embedding for 'M'\n",
      "step     0: loss 5.154094\n",
      "step  5000: loss 0.017238\n",
      "step 10000: loss 0.001342\n",
      "step 15000: loss 0.000134\n",
      "ending training at step 15652: loss 0.000100\n",
      "Optimizing embedding for 'N'\n",
      "step     0: loss 5.410346\n",
      "step  5000: loss 0.012436\n",
      "step 10000: loss 0.000936\n",
      "ending training at step 14710: loss 0.000100\n",
      "Optimizing embedding for 'O'\n",
      "step     0: loss 4.898608\n",
      "step  5000: loss 0.011437\n",
      "step 10000: loss 0.000870\n",
      "ending training at step 14554: loss 0.000100\n",
      "Optimizing embedding for 'P'\n",
      "step     0: loss 4.961571\n",
      "step  5000: loss 0.017236\n",
      "step 10000: loss 0.001346\n",
      "step 15000: loss 0.000136\n",
      "ending training at step 15711: loss 0.000100\n",
      "Optimizing embedding for 'Q'\n",
      "step     0: loss 5.146482\n",
      "step  5000: loss 0.033291\n",
      "step 10000: loss 0.002937\n",
      "step 15000: loss 0.000374\n",
      "ending training at step 18847: loss 0.000100\n",
      "Optimizing embedding for 'R'\n",
      "step     0: loss 5.044732\n",
      "step  5000: loss 0.012409\n",
      "step 10000: loss 0.000931\n",
      "ending training at step 14697: loss 0.000100\n",
      "Optimizing embedding for 'S'\n",
      "step     0: loss 4.984651\n",
      "step  5000: loss 0.014172\n",
      "step 10000: loss 0.001097\n",
      "step 15000: loss 0.000107\n",
      "ending training at step 15141: loss 0.000100\n",
      "Optimizing embedding for 'T'\n",
      "step     0: loss 5.201972\n",
      "step  5000: loss 0.017122\n",
      "step 10000: loss 0.001325\n",
      "step 15000: loss 0.000131\n",
      "ending training at step 15602: loss 0.000100\n",
      "Optimizing embedding for 'U'\n",
      "step     0: loss 5.261983\n",
      "step  5000: loss 0.011479\n",
      "step 10000: loss 0.000862\n",
      "ending training at step 14514: loss 0.000100\n",
      "Optimizing embedding for 'V'\n",
      "step     0: loss 5.383446\n",
      "step  5000: loss 0.014711\n",
      "step 10000: loss 0.001108\n",
      "step 15000: loss 0.000105\n",
      "ending training at step 15112: loss 0.000100\n",
      "Optimizing embedding for 'W'\n",
      "step     0: loss 5.273647\n",
      "step  5000: loss 0.021252\n",
      "step 10000: loss 0.001691\n",
      "step 15000: loss 0.000176\n",
      "ending training at step 16322: loss 0.000100\n",
      "Optimizing embedding for 'X'\n",
      "step     0: loss 5.354009\n",
      "step  5000: loss 0.015729\n",
      "step 10000: loss 0.001203\n",
      "step 15000: loss 0.000117\n",
      "ending training at step 15351: loss 0.000100\n",
      "Optimizing embedding for 'Y'\n",
      "step     0: loss 5.102457\n",
      "step  5000: loss 0.012912\n",
      "step 10000: loss 0.000968\n",
      "ending training at step 14792: loss 0.000100\n",
      "Optimizing embedding for 'Z'\n",
      "step     0: loss 5.203928\n",
      "step  5000: loss 0.015488\n",
      "step 10000: loss 0.001193\n",
      "step 15000: loss 0.000116\n",
      "ending training at step 15327: loss 0.000100\n",
      "Optimizing embedding for 'a'\n",
      "step     0: loss 4.273046\n",
      "step  5000: loss 0.014312\n",
      "step 10000: loss 0.001138\n",
      "step 15000: loss 0.000115\n",
      "ending training at step 15308: loss 0.000100\n",
      "Optimizing embedding for 'b'\n",
      "step     0: loss 4.682340\n",
      "step  5000: loss 0.013746\n",
      "step 10000: loss 0.001014\n",
      "ending training at step 14867: loss 0.000100\n",
      "Optimizing embedding for 'c'\n",
      "step     0: loss 4.690848\n",
      "step  5000: loss 0.011992\n",
      "step 10000: loss 0.000885\n",
      "ending training at step 14568: loss 0.000100\n",
      "Optimizing embedding for 'd'\n",
      "step     0: loss 4.549020\n",
      "step  5000: loss 0.011544\n",
      "step 10000: loss 0.000855\n",
      "ending training at step 14477: loss 0.000100\n",
      "Optimizing embedding for 'e'\n",
      "step     0: loss 4.382761\n",
      "step  5000: loss 0.011802\n",
      "step 10000: loss 0.000924\n",
      "ending training at step 14748: loss 0.000100\n",
      "Optimizing embedding for 'f'\n",
      "step     0: loss 4.842515\n",
      "step  5000: loss 0.013852\n",
      "step 10000: loss 0.001037\n",
      "ending training at step 14940: loss 0.000100\n",
      "Optimizing embedding for 'g'\n",
      "step     0: loss 4.529458\n",
      "step  5000: loss 0.010083\n",
      "step 10000: loss 0.000754\n",
      "ending training at step 14212: loss 0.000100\n",
      "Optimizing embedding for 'h'\n",
      "step     0: loss 4.441307\n",
      "step  5000: loss 0.013397\n",
      "step 10000: loss 0.001038\n",
      "step 15000: loss 0.000101\n",
      "ending training at step 15032: loss 0.000100\n",
      "Optimizing embedding for 'i'\n",
      "step     0: loss 4.120494\n",
      "step  5000: loss 0.012618\n",
      "step 10000: loss 0.000995\n",
      "ending training at step 14945: loss 0.000100\n",
      "Optimizing embedding for 'j'\n",
      "step     0: loss 4.989840\n",
      "step  5000: loss 0.014413\n",
      "step 10000: loss 0.001074\n",
      "step 15000: loss 0.000101\n",
      "ending training at step 15031: loss 0.000100\n",
      "Optimizing embedding for 'k'\n",
      "step     0: loss 4.854855\n",
      "step  5000: loss 0.009786\n",
      "step 10000: loss 0.000723\n",
      "ending training at step 14104: loss 0.000100\n",
      "Optimizing embedding for 'l'\n",
      "step     0: loss 4.350724\n",
      "step  5000: loss 0.011370\n",
      "step 10000: loss 0.000853\n",
      "ending training at step 14497: loss 0.000100\n",
      "Optimizing embedding for 'm'\n",
      "step     0: loss 4.619439\n",
      "step  5000: loss 0.012355\n",
      "step 10000: loss 0.000939\n",
      "ending training at step 14752: loss 0.000100\n",
      "Optimizing embedding for 'n'\n",
      "step     0: loss 4.462899\n",
      "step  5000: loss 0.012318\n",
      "step 10000: loss 0.000935\n",
      "ending training at step 14727: loss 0.000100\n",
      "Optimizing embedding for 'o'\n",
      "step     0: loss 4.410997\n",
      "step  5000: loss 0.014322\n",
      "step 10000: loss 0.001139\n",
      "step 15000: loss 0.000113\n",
      "ending training at step 15282: loss 0.000100\n",
      "Optimizing embedding for 'p'\n",
      "step     0: loss 4.570235\n",
      "step  5000: loss 0.011398\n",
      "step 10000: loss 0.000837\n",
      "ending training at step 14423: loss 0.000100\n",
      "Optimizing embedding for 'q'\n",
      "step     0: loss 5.127600\n",
      "step  5000: loss 0.012552\n",
      "step 10000: loss 0.000936\n",
      "ending training at step 14693: loss 0.000100\n",
      "Optimizing embedding for 'r'\n",
      "step     0: loss 4.435355\n",
      "step  5000: loss 0.011171\n",
      "step 10000: loss 0.000845\n",
      "ending training at step 14497: loss 0.000100\n",
      "Optimizing embedding for 's'\n",
      "step     0: loss 4.184333\n",
      "step  5000: loss 0.011847\n",
      "step 10000: loss 0.000928\n",
      "ending training at step 14775: loss 0.000100\n",
      "Optimizing embedding for 't'\n",
      "step     0: loss 4.536841\n",
      "step  5000: loss 0.013432\n",
      "step 10000: loss 0.001021\n",
      "ending training at step 14953: loss 0.000100\n",
      "Optimizing embedding for 'u'\n",
      "step     0: loss 4.413103\n",
      "step  5000: loss 0.010647\n",
      "step 10000: loss 0.000790\n",
      "ending training at step 14312: loss 0.000100\n",
      "Optimizing embedding for 'v'\n",
      "step     0: loss 4.691413\n",
      "step  5000: loss 0.011501\n",
      "step 10000: loss 0.000860\n",
      "ending training at step 14509: loss 0.000100\n",
      "Optimizing embedding for 'w'\n",
      "step     0: loss 4.491944\n",
      "step  5000: loss 0.013307\n",
      "step 10000: loss 0.000995\n",
      "ending training at step 14859: loss 0.000100\n",
      "Optimizing embedding for 'x'\n",
      "step     0: loss 4.986167\n",
      "step  5000: loss 0.011315\n",
      "step 10000: loss 0.000835\n",
      "ending training at step 14424: loss 0.000100\n",
      "Optimizing embedding for 'y'\n",
      "step     0: loss 4.736977\n",
      "step  5000: loss 0.011138\n",
      "step 10000: loss 0.000828\n",
      "ending training at step 14408: loss 0.000100\n",
      "Optimizing embedding for 'z'\n",
      "step     0: loss 4.985116\n",
      "step  5000: loss 0.009728\n",
      "step 10000: loss 0.000710\n",
      "ending training at step 14037: loss 0.000100\n"
     ]
    }
   ],
   "source": [
    "for target_char in tqdm(tokenizer.chars):\n",
    "    multi_embs, _ = learn_embedding_for_char(\n",
    "        target_char, embedding_to_logits_function, n_embeddings_to_learn=n_embeddings_to_learn\n",
    "    )\n",
    "    torch.save(multi_embs, sub_dir / f'{filename_for_token(target_char)}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Block\n",
    "\n",
    "Learn embeddings from the last block (block index 5 onwards):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_n_on = 5\n",
    "embedding_to_logits_function = block_n_on_function(m, n=block_n_on)\n",
    "sub_dir = results_folder / f'block_{block_n_on}'\n",
    "sub_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba503ef0b86540239823f30356eb842c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing embedding for '\\n'\n",
      "step     0: loss 5.347430\n",
      "step  5000: loss 0.007386\n",
      "step 10000: loss 0.000657\n",
      "ending training at step 14478: loss 0.000100\n",
      "Optimizing embedding for ' '\n",
      "step     0: loss 3.744530\n",
      "step  5000: loss 0.005794\n",
      "step 10000: loss 0.000533\n",
      "ending training at step 14041: loss 0.000100\n",
      "Optimizing embedding for '!'\n",
      "step     0: loss 6.476709\n",
      "step  5000: loss 0.007308\n",
      "step 10000: loss 0.000641\n",
      "ending training at step 14254: loss 0.000100\n",
      "Optimizing embedding for '$'\n",
      "step     0: loss 8.133106\n",
      "step  5000: loss 0.085841\n",
      "step 10000: loss 0.009670\n",
      "step 15000: loss 0.001712\n",
      "step 20000: loss 0.000555\n",
      "step 25000: loss 0.000398\n",
      "step 30000: loss 0.000378\n",
      "step 35000: loss 0.000371\n",
      "step 40000: loss 0.000367\n",
      "step 45000: loss 0.000365\n",
      "Optimizing embedding for '&'\n",
      "step     0: loss 7.565117\n",
      "step  5000: loss 0.093037\n",
      "step 10000: loss 0.010646\n",
      "step 15000: loss 0.002142\n",
      "step 20000: loss 0.000802\n",
      "step 25000: loss 0.000548\n",
      "step 30000: loss 0.000499\n",
      "step 35000: loss 0.000487\n",
      "step 40000: loss 0.000483\n",
      "step 45000: loss 0.000481\n",
      "Optimizing embedding for \"'\"\n",
      "step     0: loss 5.156186\n",
      "step  5000: loss 0.005823\n",
      "step 10000: loss 0.000452\n",
      "ending training at step 13204: loss 0.000100\n",
      "Optimizing embedding for ','\n",
      "step     0: loss 4.808379\n",
      "step  5000: loss 0.015074\n",
      "step 10000: loss 0.001522\n",
      "step 15000: loss 0.000231\n",
      "ending training at step 17605: loss 0.000100\n",
      "Optimizing embedding for '-'\n",
      "step     0: loss 5.895603\n",
      "step  5000: loss 0.005756\n",
      "step 10000: loss 0.000450\n",
      "ending training at step 13249: loss 0.000100\n",
      "Optimizing embedding for '.'\n",
      "step     0: loss 6.209974\n",
      "step  5000: loss 0.023172\n",
      "step 10000: loss 0.002623\n",
      "step 15000: loss 0.000436\n",
      "step 20000: loss 0.000104\n",
      "ending training at step 20149: loss 0.000100\n",
      "Optimizing embedding for '3'\n",
      "step     0: loss 7.941002\n",
      "step  5000: loss 0.044592\n",
      "step 10000: loss 0.004487\n",
      "step 15000: loss 0.000611\n",
      "step 20000: loss 0.000118\n",
      "ending training at step 20613: loss 0.000100\n",
      "Optimizing embedding for ':'\n",
      "step     0: loss 6.865605\n",
      "step  5000: loss 0.006139\n",
      "step 10000: loss 0.000501\n",
      "ending training at step 13679: loss 0.000100\n",
      "Optimizing embedding for ';'\n",
      "step     0: loss 6.623376\n",
      "step  5000: loss 0.028730\n",
      "step 10000: loss 0.002908\n",
      "step 15000: loss 0.000418\n",
      "ending training at step 19428: loss 0.000100\n",
      "Optimizing embedding for '?'\n",
      "step     0: loss 6.516698\n",
      "step  5000: loss 0.007135\n",
      "step 10000: loss 0.000624\n",
      "ending training at step 14201: loss 0.000100\n",
      "Optimizing embedding for 'A'\n",
      "step     0: loss 7.063316\n",
      "step  5000: loss 0.008427\n",
      "step 10000: loss 0.000698\n",
      "ending training at step 14472: loss 0.000100\n",
      "Optimizing embedding for 'B'\n",
      "step     0: loss 7.246926\n",
      "step  5000: loss 0.011748\n",
      "step 10000: loss 0.000978\n",
      "step 15000: loss 0.000113\n",
      "ending training at step 15292: loss 0.000100\n",
      "Optimizing embedding for 'C'\n",
      "step     0: loss 7.449971\n",
      "step  5000: loss 0.008698\n",
      "step 10000: loss 0.000758\n",
      "ending training at step 14731: loss 0.000100\n",
      "Optimizing embedding for 'D'\n",
      "step     0: loss 6.771146\n",
      "step  5000: loss 0.007700\n",
      "step 10000: loss 0.000649\n",
      "ending training at step 14250: loss 0.000100\n",
      "Optimizing embedding for 'E'\n",
      "step     0: loss 7.121366\n",
      "step  5000: loss 0.006511\n",
      "step 10000: loss 0.000531\n",
      "ending training at step 13697: loss 0.000100\n",
      "Optimizing embedding for 'F'\n",
      "step     0: loss 7.631640\n",
      "step  5000: loss 0.013424\n",
      "step 10000: loss 0.001119\n",
      "step 15000: loss 0.000125\n",
      "ending training at step 15536: loss 0.000100\n",
      "Optimizing embedding for 'G'\n",
      "step     0: loss 7.739057\n",
      "step  5000: loss 0.008573\n",
      "step 10000: loss 0.000703\n",
      "ending training at step 14341: loss 0.000100\n",
      "Optimizing embedding for 'H'\n",
      "step     0: loss 6.870086\n",
      "step  5000: loss 0.011302\n",
      "step 10000: loss 0.000955\n",
      "step 15000: loss 0.000113\n",
      "ending training at step 15317: loss 0.000100\n",
      "Optimizing embedding for 'I'\n",
      "step     0: loss 6.524743\n",
      "step  5000: loss 0.006385\n",
      "step 10000: loss 0.000516\n",
      "ending training at step 13674: loss 0.000100\n",
      "Optimizing embedding for 'J'\n",
      "step     0: loss 6.918394\n",
      "step  5000: loss 0.021348\n",
      "step 10000: loss 0.002077\n",
      "step 15000: loss 0.000270\n",
      "ending training at step 17681: loss 0.000100\n",
      "Optimizing embedding for 'K'\n",
      "step     0: loss 7.540547\n",
      "step  5000: loss 0.011649\n",
      "step 10000: loss 0.001001\n",
      "step 15000: loss 0.000115\n",
      "ending training at step 15346: loss 0.000100\n",
      "Optimizing embedding for 'L'\n",
      "step     0: loss 7.171153\n",
      "step  5000: loss 0.007544\n",
      "step 10000: loss 0.000624\n",
      "ending training at step 14132: loss 0.000100\n",
      "Optimizing embedding for 'M'\n",
      "step     0: loss 7.511839\n",
      "step  5000: loss 0.012008\n",
      "step 10000: loss 0.000992\n",
      "step 15000: loss 0.000115\n",
      "ending training at step 15342: loss 0.000100\n",
      "Optimizing embedding for 'N'\n",
      "step     0: loss 7.237311\n",
      "step  5000: loss 0.005385\n",
      "step 10000: loss 0.000439\n",
      "ending training at step 13250: loss 0.000100\n",
      "Optimizing embedding for 'O'\n",
      "step     0: loss 7.000911\n",
      "step  5000: loss 0.005379\n",
      "step 10000: loss 0.000424\n",
      "ending training at step 13159: loss 0.000100\n",
      "Optimizing embedding for 'P'\n",
      "step     0: loss 6.761536\n",
      "step  5000: loss 0.012577\n",
      "step 10000: loss 0.001089\n",
      "step 15000: loss 0.000130\n",
      "ending training at step 15663: loss 0.000100\n",
      "Optimizing embedding for 'Q'\n",
      "step     0: loss 7.574077\n",
      "step  5000: loss 0.036536\n",
      "step 10000: loss 0.003842\n",
      "step 15000: loss 0.000569\n",
      "step 20000: loss 0.000121\n",
      "ending training at step 20763: loss 0.000100\n",
      "Optimizing embedding for 'R'\n",
      "step     0: loss 7.027799\n",
      "step  5000: loss 0.007081\n",
      "step 10000: loss 0.000573\n",
      "ending training at step 13866: loss 0.000100\n",
      "Optimizing embedding for 'S'\n",
      "step     0: loss 7.332571\n",
      "step  5000: loss 0.008095\n",
      "step 10000: loss 0.000670\n",
      "ending training at step 14317: loss 0.000100\n",
      "Optimizing embedding for 'T'\n",
      "step     0: loss 7.321306\n",
      "step  5000: loss 0.007811\n",
      "step 10000: loss 0.000704\n",
      "ending training at step 14660: loss 0.000100\n",
      "Optimizing embedding for 'U'\n",
      "step     0: loss 7.908830\n",
      "step  5000: loss 0.006525\n",
      "step 10000: loss 0.000512\n",
      "ending training at step 13557: loss 0.000100\n",
      "Optimizing embedding for 'V'\n",
      "step     0: loss 7.760444\n",
      "step  5000: loss 0.009805\n",
      "step 10000: loss 0.000795\n",
      "ending training at step 14672: loss 0.000100\n",
      "Optimizing embedding for 'W'\n",
      "step     0: loss 7.053877\n",
      "step  5000: loss 0.013178\n",
      "step 10000: loss 0.001213\n",
      "step 15000: loss 0.000152\n",
      "ending training at step 16094: loss 0.000100\n",
      "Optimizing embedding for 'X'\n",
      "step     0: loss 8.160125\n",
      "step  5000: loss 0.012235\n",
      "step 10000: loss 0.001112\n",
      "step 15000: loss 0.000132\n",
      "ending training at step 15675: loss 0.000100\n",
      "Optimizing embedding for 'Y'\n",
      "step     0: loss 7.260418\n",
      "step  5000: loss 0.007429\n",
      "step 10000: loss 0.000593\n",
      "ending training at step 13927: loss 0.000100\n",
      "Optimizing embedding for 'Z'\n",
      "step     0: loss 7.773640\n",
      "step  5000: loss 0.012257\n",
      "step 10000: loss 0.001111\n",
      "step 15000: loss 0.000131\n",
      "ending training at step 15655: loss 0.000100\n",
      "Optimizing embedding for 'a'\n",
      "step     0: loss 4.141507\n",
      "step  5000: loss 0.003693\n",
      "step 10000: loss 0.000306\n",
      "ending training at step 12568: loss 0.000100\n",
      "Optimizing embedding for 'b'\n",
      "step     0: loss 5.297385\n",
      "step  5000: loss 0.004673\n",
      "step 10000: loss 0.000380\n",
      "ending training at step 12962: loss 0.000100\n",
      "Optimizing embedding for 'c'\n",
      "step     0: loss 5.074947\n",
      "step  5000: loss 0.003856\n",
      "step 10000: loss 0.000303\n",
      "ending training at step 12421: loss 0.000100\n",
      "Optimizing embedding for 'd'\n",
      "step     0: loss 5.328892\n",
      "step  5000: loss 0.003671\n",
      "step 10000: loss 0.000276\n",
      "ending training at step 12169: loss 0.000100\n",
      "Optimizing embedding for 'e'\n",
      "step     0: loss 4.090889\n",
      "step  5000: loss 0.003495\n",
      "step 10000: loss 0.000280\n",
      "ending training at step 12263: loss 0.000100\n",
      "Optimizing embedding for 'f'\n",
      "step     0: loss 5.071789\n",
      "step  5000: loss 0.005350\n",
      "step 10000: loss 0.000426\n",
      "ending training at step 13240: loss 0.000100\n",
      "Optimizing embedding for 'g'\n",
      "step     0: loss 4.969716\n",
      "step  5000: loss 0.003954\n",
      "step 10000: loss 0.000301\n",
      "ending training at step 12341: loss 0.000100\n",
      "Optimizing embedding for 'h'\n",
      "step     0: loss 4.788130\n",
      "step  5000: loss 0.004963\n",
      "step 10000: loss 0.000418\n",
      "ending training at step 13255: loss 0.000100\n",
      "Optimizing embedding for 'i'\n",
      "step     0: loss 3.821693\n",
      "step  5000: loss 0.003995\n",
      "step 10000: loss 0.000322\n",
      "ending training at step 12586: loss 0.000100\n",
      "Optimizing embedding for 'j'\n",
      "step     0: loss 6.758059\n",
      "step  5000: loss 0.013991\n",
      "step 10000: loss 0.001127\n",
      "step 15000: loss 0.000120\n",
      "ending training at step 15430: loss 0.000100\n",
      "Optimizing embedding for 'k'\n",
      "step     0: loss 6.332606\n",
      "step  5000: loss 0.005338\n",
      "step 10000: loss 0.000403\n",
      "ending training at step 12947: loss 0.000100\n",
      "Optimizing embedding for 'l'\n",
      "step     0: loss 4.604065\n",
      "step  5000: loss 0.003959\n",
      "step 10000: loss 0.000300\n",
      "ending training at step 12355: loss 0.000100\n",
      "Optimizing embedding for 'm'\n",
      "step     0: loss 5.128067\n",
      "step  5000: loss 0.005043\n",
      "step 10000: loss 0.000390\n",
      "ending training at step 12985: loss 0.000100\n",
      "Optimizing embedding for 'n'\n",
      "step     0: loss 4.884213\n",
      "step  5000: loss 0.004264\n",
      "step 10000: loss 0.000326\n",
      "ending training at step 12533: loss 0.000100\n",
      "Optimizing embedding for 'o'\n",
      "step     0: loss 4.753374\n",
      "step  5000: loss 0.005272\n",
      "step 10000: loss 0.000444\n",
      "ending training at step 13389: loss 0.000100\n",
      "Optimizing embedding for 'p'\n",
      "step     0: loss 5.534964\n",
      "step  5000: loss 0.004984\n",
      "step 10000: loss 0.000374\n",
      "ending training at step 12822: loss 0.000100\n",
      "Optimizing embedding for 'q'\n",
      "step     0: loss 6.771389\n",
      "step  5000: loss 0.009553\n",
      "step 10000: loss 0.000762\n",
      "ending training at step 14440: loss 0.000100\n",
      "Optimizing embedding for 'r'\n",
      "step     0: loss 4.422514\n",
      "step  5000: loss 0.004006\n",
      "step 10000: loss 0.000300\n",
      "ending training at step 12342: loss 0.000100\n",
      "Optimizing embedding for 's'\n",
      "step     0: loss 4.241723\n",
      "step  5000: loss 0.004333\n",
      "step 10000: loss 0.000347\n",
      "ending training at step 12766: loss 0.000100\n",
      "Optimizing embedding for 't'\n",
      "step     0: loss 4.483743\n",
      "step  5000: loss 0.003839\n",
      "step 10000: loss 0.000315\n",
      "ending training at step 12538: loss 0.000100\n",
      "Optimizing embedding for 'u'\n",
      "step     0: loss 5.174875\n",
      "step  5000: loss 0.004506\n",
      "step 10000: loss 0.000343\n",
      "ending training at step 12646: loss 0.000100\n",
      "Optimizing embedding for 'v'\n",
      "step     0: loss 5.617326\n",
      "step  5000: loss 0.005383\n",
      "step 10000: loss 0.000417\n",
      "ending training at step 13078: loss 0.000100\n",
      "Optimizing embedding for 'w'\n",
      "step     0: loss 4.738924\n",
      "step  5000: loss 0.005459\n",
      "step 10000: loss 0.000426\n",
      "ending training at step 13187: loss 0.000100\n",
      "Optimizing embedding for 'x'\n",
      "step     0: loss 7.199396\n",
      "step  5000: loss 0.007838\n",
      "step 10000: loss 0.000628\n",
      "ending training at step 14019: loss 0.000100\n",
      "Optimizing embedding for 'y'\n",
      "step     0: loss 5.467407\n",
      "step  5000: loss 0.004450\n",
      "step 10000: loss 0.000350\n",
      "ending training at step 12694: loss 0.000100\n",
      "Optimizing embedding for 'z'\n",
      "step     0: loss 7.097597\n",
      "step  5000: loss 0.006247\n",
      "step 10000: loss 0.000488\n",
      "ending training at step 13385: loss 0.000100\n"
     ]
    }
   ],
   "source": [
    "for target_char in tqdm(tokenizer.chars):\n",
    "    multi_embs, _ = learn_embedding_for_char(\n",
    "        target_char, embedding_to_logits_function, n_embeddings_to_learn=n_embeddings_to_learn\n",
    "    )\n",
    "    torch.save(multi_embs, sub_dir / f'{filename_for_token(target_char)}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With 1000 embeddings per token\n",
    "I wanted to see if we got better results with more embeddings, so let's learn 1000 embeddings / token for the last block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_n_on = 5\n",
    "n_embeddings_to_learn=1000\n",
    "embedding_to_logits_function = block_n_on_function(m, n=block_n_on)\n",
    "sub_dir = results_folder / f'block_{block_n_on}_{n_embeddings_to_learn}'\n",
    "sub_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddf0b8d388345f48df347f23690e54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing embedding for 'n'\n",
      "step     0: loss 4.559832\n",
      "step  5000: loss 0.004011\n",
      "step 10000: loss 0.000314\n",
      "ending training at step 12531: loss 0.000100\n",
      "Optimizing embedding for 'o'\n",
      "step     0: loss 4.772153\n",
      "step  5000: loss 0.005388\n",
      "step 10000: loss 0.000453\n",
      "ending training at step 13553: loss 0.000100\n",
      "Optimizing embedding for 'p'\n",
      "step     0: loss 5.451766\n",
      "step  5000: loss 0.004887\n",
      "step 10000: loss 0.000375\n",
      "ending training at step 12905: loss 0.000100\n",
      "Optimizing embedding for 'q'\n",
      "step     0: loss 6.712268\n",
      "step  5000: loss 0.009738\n",
      "step 10000: loss 0.000777\n",
      "ending training at step 14641: loss 0.000100\n",
      "Optimizing embedding for 'r'\n",
      "step     0: loss 4.504239\n",
      "step  5000: loss 0.003917\n",
      "step 10000: loss 0.000304\n",
      "ending training at step 12451: loss 0.000100\n",
      "Optimizing embedding for 's'\n",
      "step     0: loss 3.967334\n",
      "step  5000: loss 0.004284\n",
      "step 10000: loss 0.000346\n",
      "ending training at step 12817: loss 0.000100\n",
      "Optimizing embedding for 't'\n",
      "step     0: loss 4.246644\n",
      "step  5000: loss 0.003862\n",
      "step 10000: loss 0.000315\n",
      "ending training at step 12615: loss 0.000100\n",
      "Optimizing embedding for 'u'\n",
      "step     0: loss 5.164997\n",
      "step  5000: loss 0.004617\n",
      "step 10000: loss 0.000360\n",
      "ending training at step 12831: loss 0.000100\n",
      "Optimizing embedding for 'v'\n",
      "step     0: loss 5.784395\n",
      "step  5000: loss 0.005645\n",
      "step 10000: loss 0.000439\n",
      "ending training at step 13280: loss 0.000100\n",
      "Optimizing embedding for 'w'\n",
      "step     0: loss 5.032755\n",
      "step  5000: loss 0.005488\n",
      "step 10000: loss 0.000441\n",
      "ending training at step 13393: loss 0.000100\n",
      "Optimizing embedding for 'x'\n",
      "step     0: loss 7.050308\n",
      "step  5000: loss 0.007867\n",
      "step 10000: loss 0.000636\n",
      "ending training at step 14167: loss 0.000100\n",
      "Optimizing embedding for 'y'\n",
      "step     0: loss 5.264086\n",
      "step  5000: loss 0.004477\n",
      "step 10000: loss 0.000350\n",
      "ending training at step 12751: loss 0.000100\n",
      "Optimizing embedding for 'z'\n",
      "step     0: loss 7.208733\n",
      "step  5000: loss 0.006133\n",
      "step 10000: loss 0.000482\n",
      "ending training at step 13441: loss 0.000100\n"
     ]
    }
   ],
   "source": [
    "for target_char in tqdm(tokenizer.chars):\n",
    "    multi_embs, _ = learn_embedding_for_char(\n",
    "        target_char, embedding_to_logits_function, n_embeddings_to_learn=n_embeddings_to_learn\n",
    "    )\n",
    "    torch.save(multi_embs, sub_dir / f'{filename_for_token(target_char)}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With 5000 embeddings per token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if even more embeddings help: learn 5000 embeddings/token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/learned_embeddings/block_5_5000\n"
     ]
    }
   ],
   "source": [
    "block_n_on = 5\n",
    "n_embeddings_to_learn=5000\n",
    "embedding_to_logits_function = block_n_on_function(m, n=block_n_on)\n",
    "sub_dir = results_folder / f'block_{block_n_on}_{n_embeddings_to_learn}'\n",
    "sub_dir.mkdir(exist_ok=True)\n",
    "print(sub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a145d0360e4ba09016bc6d1ab92aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing embedding for '\\n'\n",
      "step     0: loss 5.594407\n",
      "step  5000: loss 0.007269\n",
      "step 10000: loss 0.000696\n",
      "step 15000: loss 0.000120\n",
      "ending training at step 15687: loss 0.000100\n",
      "Optimizing embedding for ' '\n",
      "step     0: loss 3.438382\n",
      "step  5000: loss 0.005430\n",
      "step 10000: loss 0.000537\n",
      "step 15000: loss 0.000103\n",
      "ending training at step 15140: loss 0.000100\n",
      "Optimizing embedding for '!'\n",
      "step     0: loss 6.517096\n",
      "step  5000: loss 0.007284\n",
      "step 10000: loss 0.000668\n",
      "step 15000: loss 0.000103\n",
      "ending training at step 15083: loss 0.000100\n",
      "Optimizing embedding for '$'\n",
      "step     0: loss 7.965337\n",
      "step  5000: loss 0.082970\n",
      "step 10000: loss 0.009700\n",
      "step 15000: loss 0.001987\n",
      "step 20000: loss 0.000854\n",
      "step 25000: loss 0.000615\n",
      "step 30000: loss 0.000530\n",
      "step 35000: loss 0.000489\n",
      "step 40000: loss 0.000464\n",
      "step 45000: loss 0.000448\n",
      "Optimizing embedding for '&'\n",
      "step     0: loss 7.658935\n",
      "step  5000: loss 0.095831\n",
      "step 10000: loss 0.011111\n",
      "step 15000: loss 0.002514\n",
      "step 20000: loss 0.001186\n",
      "step 25000: loss 0.000888\n",
      "step 30000: loss 0.000776\n",
      "step 35000: loss 0.000716\n",
      "step 40000: loss 0.000678\n",
      "step 45000: loss 0.000651\n",
      "Optimizing embedding for \"'\"\n",
      "step     0: loss 5.282728\n",
      "step  5000: loss 0.005876\n",
      "step 10000: loss 0.000483\n",
      "ending training at step 13854: loss 0.000100\n",
      "Optimizing embedding for ','\n",
      "step     0: loss 5.069627\n",
      "step  5000: loss 0.015742\n",
      "step 10000: loss 0.001677\n",
      "step 15000: loss 0.000330\n",
      "step 20000: loss 0.000136\n",
      "ending training at step 23151: loss 0.000100\n",
      "Optimizing embedding for '-'\n",
      "step     0: loss 5.793747\n",
      "step  5000: loss 0.005741\n",
      "step 10000: loss 0.000474\n",
      "ending training at step 13813: loss 0.000100\n",
      "Optimizing embedding for '.'\n",
      "step     0: loss 6.198037\n",
      "step  5000: loss 0.023380\n",
      "step 10000: loss 0.002735\n",
      "step 15000: loss 0.000564\n",
      "step 20000: loss 0.000232\n",
      "step 25000: loss 0.000152\n",
      "step 30000: loss 0.000118\n",
      "ending training at step 34704: loss 0.000100\n",
      "Optimizing embedding for '3'\n",
      "step     0: loss 7.771849\n",
      "step  5000: loss 0.044660\n",
      "step 10000: loss 0.004647\n",
      "step 15000: loss 0.000780\n",
      "step 20000: loss 0.000267\n",
      "step 25000: loss 0.000160\n",
      "step 30000: loss 0.000120\n",
      "ending training at step 34799: loss 0.000100\n",
      "Optimizing embedding for ':'\n",
      "step     0: loss 6.816797\n",
      "step  5000: loss 0.006008\n",
      "step 10000: loss 0.000522\n",
      "ending training at step 14318: loss 0.000100\n",
      "Optimizing embedding for ';'\n",
      "step     0: loss 6.429393\n",
      "step  5000: loss 0.027733\n",
      "step 10000: loss 0.002910\n",
      "step 15000: loss 0.000530\n",
      "step 20000: loss 0.000199\n",
      "step 25000: loss 0.000123\n",
      "ending training at step 28550: loss 0.000100\n",
      "Optimizing embedding for '?'\n",
      "step     0: loss 6.720572\n",
      "step  5000: loss 0.007352\n",
      "step 10000: loss 0.000670\n",
      "step 15000: loss 0.000103\n",
      "ending training at step 15092: loss 0.000100\n",
      "Optimizing embedding for 'A'\n",
      "step     0: loss 7.125983\n",
      "step  5000: loss 0.008442\n",
      "step 10000: loss 0.000733\n",
      "step 15000: loss 0.000111\n",
      "ending training at step 15360: loss 0.000100\n",
      "Optimizing embedding for 'B'\n",
      "step     0: loss 7.413654\n",
      "step  5000: loss 0.011906\n",
      "step 10000: loss 0.001031\n",
      "step 15000: loss 0.000155\n",
      "ending training at step 16551: loss 0.000100\n",
      "Optimizing embedding for 'C'\n",
      "step     0: loss 7.379269\n",
      "step  5000: loss 0.008486\n",
      "step 10000: loss 0.000769\n",
      "step 15000: loss 0.000120\n",
      "ending training at step 15637: loss 0.000100\n",
      "Optimizing embedding for 'D'\n",
      "step     0: loss 6.879560\n",
      "step  5000: loss 0.007789\n",
      "step 10000: loss 0.000685\n",
      "step 15000: loss 0.000103\n",
      "ending training at step 15098: loss 0.000100\n",
      "Optimizing embedding for 'E'\n",
      "step     0: loss 7.116797\n",
      "step  5000: loss 0.006545\n",
      "step 10000: loss 0.000557\n",
      "ending training at step 14334: loss 0.000100\n",
      "Optimizing embedding for 'F'\n",
      "step     0: loss 7.536552\n",
      "step  5000: loss 0.013105\n",
      "step 10000: loss 0.001142\n",
      "step 15000: loss 0.000168\n",
      "ending training at step 16820: loss 0.000100\n",
      "Optimizing embedding for 'G'\n",
      "step     0: loss 7.432269\n",
      "step  5000: loss 0.008491\n",
      "step 10000: loss 0.000720\n",
      "step 15000: loss 0.000102\n",
      "ending training at step 15066: loss 0.000100\n",
      "Optimizing embedding for 'H'\n",
      "step     0: loss 6.976287\n",
      "step  5000: loss 0.011694\n",
      "step 10000: loss 0.001028\n",
      "step 15000: loss 0.000160\n",
      "ending training at step 16740: loss 0.000100\n",
      "Optimizing embedding for 'I'\n",
      "step     0: loss 6.453108\n",
      "step  5000: loss 0.006361\n",
      "step 10000: loss 0.000535\n",
      "ending training at step 14314: loss 0.000100\n",
      "Optimizing embedding for 'J'\n",
      "step     0: loss 7.170426\n",
      "step  5000: loss 0.022446\n",
      "step 10000: loss 0.002219\n",
      "step 15000: loss 0.000368\n",
      "step 20000: loss 0.000125\n",
      "ending training at step 21722: loss 0.000100\n",
      "Optimizing embedding for 'K'\n",
      "step     0: loss 7.532418\n",
      "step  5000: loss 0.011812\n",
      "step 10000: loss 0.001049\n",
      "step 15000: loss 0.000156\n",
      "ending training at step 16562: loss 0.000100\n",
      "Optimizing embedding for 'L'\n",
      "step     0: loss 7.310787\n",
      "step  5000: loss 0.007852\n",
      "step 10000: loss 0.000672\n",
      "ending training at step 14940: loss 0.000100\n",
      "Optimizing embedding for 'M'\n",
      "step     0: loss 7.446343\n",
      "step  5000: loss 0.011834\n",
      "step 10000: loss 0.001031\n",
      "step 15000: loss 0.000156\n",
      "ending training at step 16586: loss 0.000100\n",
      "Optimizing embedding for 'N'\n",
      "step     0: loss 7.390759\n",
      "step  5000: loss 0.005521\n",
      "step 10000: loss 0.000460\n",
      "ending training at step 13744: loss 0.000100\n",
      "Optimizing embedding for 'O'\n",
      "step     0: loss 7.084028\n",
      "step  5000: loss 0.005356\n",
      "step 10000: loss 0.000442\n",
      "ending training at step 13628: loss 0.000100\n",
      "Optimizing embedding for 'P'\n",
      "step     0: loss 7.220920\n",
      "step  5000: loss 0.013608\n",
      "step 10000: loss 0.001216\n",
      "step 15000: loss 0.000188\n",
      "ending training at step 17391: loss 0.000100\n",
      "Optimizing embedding for 'Q'\n",
      "step     0: loss 7.621984\n",
      "step  5000: loss 0.036300\n",
      "step 10000: loss 0.003947\n",
      "step 15000: loss 0.000723\n",
      "step 20000: loss 0.000264\n",
      "step 25000: loss 0.000163\n",
      "step 30000: loss 0.000125\n",
      "step 35000: loss 0.000104\n",
      "ending training at step 36355: loss 0.000100\n",
      "Optimizing embedding for 'R'\n",
      "step     0: loss 6.963523\n",
      "step  5000: loss 0.007048\n",
      "step 10000: loss 0.000594\n",
      "ending training at step 14513: loss 0.000100\n",
      "Optimizing embedding for 'S'\n",
      "step     0: loss 7.216874\n",
      "step  5000: loss 0.007949\n",
      "step 10000: loss 0.000687\n",
      "step 15000: loss 0.000102\n",
      "ending training at step 15060: loss 0.000100\n",
      "Optimizing embedding for 'T'\n",
      "step     0: loss 7.100054\n",
      "step  5000: loss 0.007693\n",
      "step 10000: loss 0.000721\n",
      "step 15000: loss 0.000117\n",
      "ending training at step 15547: loss 0.000100\n",
      "Optimizing embedding for 'U'\n",
      "step     0: loss 7.833190\n",
      "step  5000: loss 0.006543\n",
      "step 10000: loss 0.000537\n",
      "ending training at step 14108: loss 0.000100\n",
      "Optimizing embedding for 'V'\n",
      "step     0: loss 7.466230\n",
      "step  5000: loss 0.009653\n",
      "step 10000: loss 0.000824\n",
      "step 15000: loss 0.000118\n",
      "ending training at step 15542: loss 0.000100\n",
      "Optimizing embedding for 'W'\n",
      "step     0: loss 6.921437\n",
      "step  5000: loss 0.013066\n",
      "step 10000: loss 0.001235\n",
      "step 15000: loss 0.000202\n",
      "ending training at step 17881: loss 0.000100\n",
      "Optimizing embedding for 'X'\n",
      "step     0: loss 7.990481\n",
      "step  5000: loss 0.012197\n",
      "step 10000: loss 0.001153\n",
      "step 15000: loss 0.000179\n",
      "ending training at step 17133: loss 0.000100\n",
      "Optimizing embedding for 'Y'\n",
      "step     0: loss 7.431907\n",
      "step  5000: loss 0.007709\n",
      "step 10000: loss 0.000641\n",
      "ending training at step 14677: loss 0.000100\n",
      "Optimizing embedding for 'Z'\n",
      "step     0: loss 8.011966\n",
      "step  5000: loss 0.012419\n",
      "step 10000: loss 0.001168\n",
      "step 15000: loss 0.000180\n",
      "ending training at step 17139: loss 0.000100\n",
      "Optimizing embedding for 'a'\n",
      "step     0: loss 4.276280\n",
      "step  5000: loss 0.003790\n",
      "step 10000: loss 0.000337\n",
      "ending training at step 13200: loss 0.000100\n",
      "Optimizing embedding for 'b'\n",
      "step     0: loss 5.436181\n",
      "step  5000: loss 0.004718\n",
      "step 10000: loss 0.000403\n",
      "ending training at step 13505: loss 0.000100\n",
      "Optimizing embedding for 'c'\n",
      "step     0: loss 5.123799\n",
      "step  5000: loss 0.004070\n",
      "step 10000: loss 0.000332\n",
      "ending training at step 12919: loss 0.000100\n",
      "Optimizing embedding for 'd'\n",
      "step     0: loss 5.108806\n",
      "step  5000: loss 0.003689\n",
      "step 10000: loss 0.000294\n",
      "ending training at step 12559: loss 0.000100\n",
      "Optimizing embedding for 'e'\n",
      "step     0: loss 3.921755\n",
      "step  5000: loss 0.003373\n",
      "step 10000: loss 0.000291\n",
      "ending training at step 12680: loss 0.000100\n",
      "Optimizing embedding for 'f'\n",
      "step     0: loss 5.214041\n",
      "step  5000: loss 0.005233\n",
      "step 10000: loss 0.000446\n",
      "ending training at step 13841: loss 0.000100\n",
      "Optimizing embedding for 'g'\n",
      "step     0: loss 5.360433\n",
      "step  5000: loss 0.004195\n",
      "step 10000: loss 0.000333\n",
      "ending training at step 12860: loss 0.000100\n",
      "Optimizing embedding for 'h'\n",
      "step     0: loss 4.889000\n",
      "step  5000: loss 0.005076\n",
      "step 10000: loss 0.000449\n",
      "ending training at step 13988: loss 0.000100\n",
      "Optimizing embedding for 'i'\n",
      "step     0: loss 3.911887\n",
      "step  5000: loss 0.004062\n",
      "step 10000: loss 0.000353\n",
      "ending training at step 13265: loss 0.000100\n",
      "Optimizing embedding for 'j'\n",
      "step     0: loss 6.693477\n",
      "step  5000: loss 0.013910\n",
      "step 10000: loss 0.001173\n",
      "step 15000: loss 0.000168\n",
      "ending training at step 16807: loss 0.000100\n",
      "Optimizing embedding for 'k'\n",
      "step     0: loss 6.102386\n",
      "step  5000: loss 0.005243\n",
      "step 10000: loss 0.000413\n",
      "ending training at step 13369: loss 0.000100\n",
      "Optimizing embedding for 'l'\n",
      "step     0: loss 4.647025\n",
      "step  5000: loss 0.003966\n",
      "step 10000: loss 0.000324\n",
      "ending training at step 12858: loss 0.000100\n",
      "Optimizing embedding for 'm'\n",
      "step     0: loss 4.994263\n",
      "step  5000: loss 0.004958\n",
      "step 10000: loss 0.000411\n",
      "ending training at step 13551: loss 0.000100\n",
      "Optimizing embedding for 'n'\n",
      "step     0: loss 4.590691\n",
      "step  5000: loss 0.004071\n",
      "step 10000: loss 0.000335\n",
      "ending training at step 12967: loss 0.000100\n",
      "Optimizing embedding for 'o'\n",
      "step     0: loss 4.697253\n",
      "step  5000: loss 0.005286\n",
      "step 10000: loss 0.000470\n",
      "ending training at step 14138: loss 0.000100\n",
      "Optimizing embedding for 'p'\n",
      "step     0: loss 5.542086\n",
      "step  5000: loss 0.004964\n",
      "step 10000: loss 0.000399\n",
      "ending training at step 13345: loss 0.000100\n",
      "Optimizing embedding for 'q'\n",
      "step     0: loss 6.669010\n",
      "step  5000: loss 0.009706\n",
      "step 10000: loss 0.000805\n",
      "step 15000: loss 0.000112\n",
      "ending training at step 15362: loss 0.000100\n",
      "Optimizing embedding for 'r'\n",
      "step     0: loss 4.539549\n",
      "step  5000: loss 0.003973\n",
      "step 10000: loss 0.000325\n",
      "ending training at step 12862: loss 0.000100\n",
      "Optimizing embedding for 's'\n",
      "step     0: loss 3.938461\n",
      "step  5000: loss 0.004231\n",
      "step 10000: loss 0.000364\n",
      "ending training at step 13304: loss 0.000100\n",
      "Optimizing embedding for 't'\n",
      "step     0: loss 4.229414\n",
      "step  5000: loss 0.003895\n",
      "step 10000: loss 0.000332\n",
      "ending training at step 13033: loss 0.000100\n",
      "Optimizing embedding for 'u'\n",
      "step     0: loss 5.161785\n",
      "step  5000: loss 0.004642\n",
      "step 10000: loss 0.000378\n",
      "ending training at step 13232: loss 0.000100\n",
      "Optimizing embedding for 'v'\n",
      "step     0: loss 5.768043\n",
      "step  5000: loss 0.005681\n",
      "step 10000: loss 0.000460\n",
      "ending training at step 13746: loss 0.000100\n",
      "Optimizing embedding for 'w'\n",
      "step     0: loss 4.945869\n",
      "step  5000: loss 0.005471\n",
      "step 10000: loss 0.000461\n",
      "ending training at step 13916: loss 0.000100\n",
      "Optimizing embedding for 'x'\n",
      "step     0: loss 7.026910\n",
      "step  5000: loss 0.007881\n",
      "step 10000: loss 0.000660\n",
      "ending training at step 14750: loss 0.000100\n",
      "Optimizing embedding for 'y'\n",
      "step     0: loss 5.312238\n",
      "step  5000: loss 0.004572\n",
      "step 10000: loss 0.000374\n",
      "ending training at step 13189: loss 0.000100\n",
      "Optimizing embedding for 'z'\n",
      "step     0: loss 7.262398\n",
      "step  5000: loss 0.006220\n",
      "step 10000: loss 0.000506\n",
      "ending training at step 13876: loss 0.000100\n"
     ]
    }
   ],
   "source": [
    "for target_char in tqdm(tokenizer.chars):\n",
    "    multi_embs, _ = learn_embedding_for_char(\n",
    "        target_char, embedding_to_logits_function, n_embeddings_to_learn=n_embeddings_to_learn\n",
    "    )\n",
    "    torch.save(multi_embs, sub_dir / f'{filename_for_token(target_char)}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Blocks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn embeddings for last 2 blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_n_on = 4\n",
    "embedding_to_logits_function = block_n_on_function(m, n=block_n_on)\n",
    "sub_dir = results_folder / f'block_{block_n_on}'\n",
    "sub_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf00ba018734d49b893f864fe00ac03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing embedding for '\\n'\n",
      "step     0: loss 5.976839\n",
      "step  5000: loss 0.008648\n",
      "step 10000: loss 0.000779\n",
      "ending training at step 14971: loss 0.000100\n",
      "Optimizing embedding for ' '\n",
      "step     0: loss 3.259852\n",
      "step  5000: loss 0.005781\n",
      "step 10000: loss 0.000530\n",
      "ending training at step 14046: loss 0.000100\n",
      "Optimizing embedding for '!'\n",
      "step     0: loss 6.406515\n",
      "step  5000: loss 0.006404\n",
      "step 10000: loss 0.000584\n",
      "ending training at step 14109: loss 0.000100\n",
      "Optimizing embedding for '$'\n",
      "step     0: loss 8.885063\n",
      "step  5000: loss 0.086875\n",
      "step 10000: loss 0.010300\n",
      "step 15000: loss 0.001904\n",
      "step 20000: loss 0.000638\n",
      "step 25000: loss 0.000440\n",
      "step 30000: loss 0.000404\n",
      "step 35000: loss 0.000392\n",
      "step 40000: loss 0.000385\n",
      "step 45000: loss 0.000380\n",
      "Optimizing embedding for '&'\n",
      "step     0: loss 8.174032\n",
      "step  5000: loss 0.101088\n",
      "step 10000: loss 0.011571\n",
      "step 15000: loss 0.002427\n",
      "step 20000: loss 0.000948\n",
      "step 25000: loss 0.000637\n",
      "step 30000: loss 0.000550\n",
      "step 35000: loss 0.000525\n",
      "step 40000: loss 0.000515\n",
      "step 45000: loss 0.000509\n",
      "Optimizing embedding for \"'\"\n",
      "step     0: loss 5.590206\n",
      "step  5000: loss 0.005999\n",
      "step 10000: loss 0.000472\n",
      "ending training at step 13324: loss 0.000100\n",
      "Optimizing embedding for ','\n",
      "step     0: loss 5.080379\n",
      "step  5000: loss 0.017123\n",
      "step 10000: loss 0.001732\n",
      "step 15000: loss 0.000253\n",
      "ending training at step 17830: loss 0.000100\n",
      "Optimizing embedding for '-'\n",
      "step     0: loss 6.024865\n",
      "step  5000: loss 0.005053\n",
      "step 10000: loss 0.000398\n",
      "ending training at step 12993: loss 0.000100\n",
      "Optimizing embedding for '.'\n",
      "step     0: loss 6.162129\n",
      "step  5000: loss 0.023845\n",
      "step 10000: loss 0.002738\n",
      "step 15000: loss 0.000466\n",
      "step 20000: loss 0.000115\n",
      "ending training at step 20629: loss 0.000100\n",
      "Optimizing embedding for '3'\n",
      "step     0: loss 8.444758\n",
      "step  5000: loss 0.045061\n",
      "step 10000: loss 0.004679\n",
      "step 15000: loss 0.000691\n",
      "step 20000: loss 0.000141\n",
      "ending training at step 21408: loss 0.000100\n",
      "Optimizing embedding for ':'\n",
      "step     0: loss 6.848844\n",
      "step  5000: loss 0.005700\n",
      "step 10000: loss 0.000439\n",
      "ending training at step 13367: loss 0.000100\n",
      "Optimizing embedding for ';'\n",
      "step     0: loss 6.361838\n",
      "step  5000: loss 0.028255\n",
      "step 10000: loss 0.003009\n",
      "step 15000: loss 0.000456\n",
      "ending training at step 19825: loss 0.000100\n",
      "Optimizing embedding for '?'\n",
      "step     0: loss 7.329531\n",
      "step  5000: loss 0.007494\n",
      "step 10000: loss 0.000675\n",
      "ending training at step 14452: loss 0.000100\n",
      "Optimizing embedding for 'A'\n",
      "step     0: loss 7.462622\n",
      "step  5000: loss 0.006849\n",
      "step 10000: loss 0.000583\n",
      "ending training at step 14158: loss 0.000100\n",
      "Optimizing embedding for 'B'\n",
      "step     0: loss 7.803836\n",
      "step  5000: loss 0.010118\n",
      "step 10000: loss 0.000860\n",
      "step 15000: loss 0.000106\n",
      "ending training at step 15146: loss 0.000100\n",
      "Optimizing embedding for 'C'\n",
      "step     0: loss 7.431258\n",
      "step  5000: loss 0.006722\n",
      "step 10000: loss 0.000611\n",
      "ending training at step 14388: loss 0.000100\n",
      "Optimizing embedding for 'D'\n",
      "step     0: loss 7.259895\n",
      "step  5000: loss 0.006637\n",
      "step 10000: loss 0.000560\n",
      "ending training at step 13992: loss 0.000100\n",
      "Optimizing embedding for 'E'\n",
      "step     0: loss 7.600525\n",
      "step  5000: loss 0.006000\n",
      "step 10000: loss 0.000471\n",
      "ending training at step 13459: loss 0.000100\n",
      "Optimizing embedding for 'F'\n",
      "step     0: loss 8.173978\n",
      "step  5000: loss 0.012628\n",
      "step 10000: loss 0.001058\n",
      "step 15000: loss 0.000124\n",
      "ending training at step 15532: loss 0.000100\n",
      "Optimizing embedding for 'G'\n",
      "step     0: loss 8.104610\n",
      "step  5000: loss 0.007674\n",
      "step 10000: loss 0.000619\n",
      "ending training at step 14100: loss 0.000100\n",
      "Optimizing embedding for 'H'\n",
      "step     0: loss 7.396293\n",
      "step  5000: loss 0.010146\n",
      "step 10000: loss 0.000857\n",
      "step 15000: loss 0.000108\n",
      "ending training at step 15205: loss 0.000100\n",
      "Optimizing embedding for 'I'\n",
      "step     0: loss 6.843120\n",
      "step  5000: loss 0.005367\n",
      "step 10000: loss 0.000421\n",
      "ending training at step 13261: loss 0.000100\n",
      "Optimizing embedding for 'J'\n",
      "step     0: loss 7.682026\n",
      "step  5000: loss 0.022274\n",
      "step 10000: loss 0.002175\n",
      "step 15000: loss 0.000297\n",
      "ending training at step 18032: loss 0.000100\n",
      "Optimizing embedding for 'K'\n",
      "step     0: loss 8.034242\n",
      "step  5000: loss 0.010027\n",
      "step 10000: loss 0.000878\n",
      "step 15000: loss 0.000106\n",
      "ending training at step 15157: loss 0.000100\n",
      "Optimizing embedding for 'L'\n",
      "step     0: loss 7.222985\n",
      "step  5000: loss 0.005296\n",
      "step 10000: loss 0.000452\n",
      "ending training at step 13489: loss 0.000100\n",
      "Optimizing embedding for 'M'\n",
      "step     0: loss 7.745698\n",
      "step  5000: loss 0.009881\n",
      "step 10000: loss 0.000868\n",
      "step 15000: loss 0.000109\n",
      "ending training at step 15222: loss 0.000100\n",
      "Optimizing embedding for 'N'\n",
      "step     0: loss 8.020021\n",
      "step  5000: loss 0.004542\n",
      "step 10000: loss 0.000362\n",
      "ending training at step 12857: loss 0.000100\n",
      "Optimizing embedding for 'O'\n",
      "step     0: loss 7.501211\n",
      "step  5000: loss 0.004781\n",
      "step 10000: loss 0.000361\n",
      "ending training at step 12822: loss 0.000100\n",
      "Optimizing embedding for 'P'\n",
      "step     0: loss 7.623023\n",
      "step  5000: loss 0.012417\n",
      "step 10000: loss 0.001034\n",
      "step 15000: loss 0.000127\n",
      "ending training at step 15612: loss 0.000100\n",
      "Optimizing embedding for 'Q'\n",
      "step     0: loss 8.380750\n",
      "step  5000: loss 0.035819\n",
      "step 10000: loss 0.004029\n",
      "step 15000: loss 0.000652\n",
      "step 20000: loss 0.000150\n",
      "ending training at step 21871: loss 0.000100\n",
      "Optimizing embedding for 'R'\n",
      "step     0: loss 7.295828\n",
      "step  5000: loss 0.006140\n",
      "step 10000: loss 0.000475\n",
      "ending training at step 13437: loss 0.000100\n",
      "Optimizing embedding for 'S'\n",
      "step     0: loss 7.297294\n",
      "step  5000: loss 0.005677\n",
      "step 10000: loss 0.000485\n",
      "ending training at step 13702: loss 0.000100\n",
      "Optimizing embedding for 'T'\n",
      "step     0: loss 7.263091\n",
      "step  5000: loss 0.005888\n",
      "step 10000: loss 0.000538\n",
      "ending training at step 14109: loss 0.000100\n",
      "Optimizing embedding for 'U'\n",
      "step     0: loss 7.996090\n",
      "step  5000: loss 0.005224\n",
      "step 10000: loss 0.000412\n",
      "ending training at step 13103: loss 0.000100\n",
      "Optimizing embedding for 'V'\n",
      "step     0: loss 7.998583\n",
      "step  5000: loss 0.008571\n",
      "step 10000: loss 0.000676\n",
      "ending training at step 14284: loss 0.000100\n",
      "Optimizing embedding for 'W'\n",
      "step     0: loss 7.612405\n",
      "step  5000: loss 0.011744\n",
      "step 10000: loss 0.001060\n",
      "step 15000: loss 0.000141\n",
      "ending training at step 15938: loss 0.000100\n",
      "Optimizing embedding for 'X'\n",
      "step     0: loss 8.956954\n",
      "step  5000: loss 0.011808\n",
      "step 10000: loss 0.001073\n",
      "step 15000: loss 0.000130\n",
      "ending training at step 15650: loss 0.000100\n",
      "Optimizing embedding for 'Y'\n",
      "step     0: loss 7.662042\n",
      "step  5000: loss 0.006023\n",
      "step 10000: loss 0.000485\n",
      "ending training at step 13533: loss 0.000100\n",
      "Optimizing embedding for 'Z'\n",
      "step     0: loss 8.492460\n",
      "step  5000: loss 0.011471\n",
      "step 10000: loss 0.001044\n",
      "step 15000: loss 0.000128\n",
      "ending training at step 15617: loss 0.000100\n",
      "Optimizing embedding for 'a'\n",
      "step     0: loss 3.811878\n",
      "step  5000: loss 0.003351\n",
      "step 10000: loss 0.000269\n",
      "ending training at step 12259: loss 0.000100\n",
      "Optimizing embedding for 'b'\n",
      "step     0: loss 5.239269\n",
      "step  5000: loss 0.003815\n",
      "step 10000: loss 0.000299\n",
      "ending training at step 12449: loss 0.000100\n",
      "Optimizing embedding for 'c'\n",
      "step     0: loss 5.045540\n",
      "step  5000: loss 0.003449\n",
      "step 10000: loss 0.000268\n",
      "ending training at step 12143: loss 0.000100\n",
      "Optimizing embedding for 'd'\n",
      "step     0: loss 4.871099\n",
      "step  5000: loss 0.003597\n",
      "step 10000: loss 0.000258\n",
      "ending training at step 11999: loss 0.000100\n",
      "Optimizing embedding for 'e'\n",
      "step     0: loss 3.123604\n",
      "step  5000: loss 0.002169\n",
      "step 10000: loss 0.000172\n",
      "ending training at step 11193: loss 0.000100\n",
      "Optimizing embedding for 'f'\n",
      "step     0: loss 5.064374\n",
      "step  5000: loss 0.005404\n",
      "step 10000: loss 0.000425\n",
      "ending training at step 13219: loss 0.000100\n",
      "Optimizing embedding for 'g'\n",
      "step     0: loss 5.496363\n",
      "step  5000: loss 0.003857\n",
      "step 10000: loss 0.000278\n",
      "ending training at step 12196: loss 0.000100\n",
      "Optimizing embedding for 'h'\n",
      "step     0: loss 4.893647\n",
      "step  5000: loss 0.004864\n",
      "step 10000: loss 0.000385\n",
      "ending training at step 13053: loss 0.000100\n",
      "Optimizing embedding for 'i'\n",
      "step     0: loss 3.567966\n",
      "step  5000: loss 0.003641\n",
      "step 10000: loss 0.000283\n",
      "ending training at step 12313: loss 0.000100\n",
      "Optimizing embedding for 'j'\n",
      "step     0: loss 6.891648\n",
      "step  5000: loss 0.012717\n",
      "step 10000: loss 0.001040\n",
      "step 15000: loss 0.000116\n",
      "ending training at step 15350: loss 0.000100\n",
      "Optimizing embedding for 'k'\n",
      "step     0: loss 6.694317\n",
      "step  5000: loss 0.005149\n",
      "step 10000: loss 0.000386\n",
      "ending training at step 12889: loss 0.000100\n",
      "Optimizing embedding for 'l'\n",
      "step     0: loss 3.972925\n",
      "step  5000: loss 0.003409\n",
      "step 10000: loss 0.000254\n",
      "ending training at step 12011: loss 0.000100\n",
      "Optimizing embedding for 'm'\n",
      "step     0: loss 4.716524\n",
      "step  5000: loss 0.004574\n",
      "step 10000: loss 0.000338\n",
      "ending training at step 12668: loss 0.000100\n",
      "Optimizing embedding for 'n'\n",
      "step     0: loss 3.947883\n",
      "step  5000: loss 0.003402\n",
      "step 10000: loss 0.000255\n",
      "ending training at step 12002: loss 0.000100\n",
      "Optimizing embedding for 'o'\n",
      "step     0: loss 4.209094\n",
      "step  5000: loss 0.004597\n",
      "step 10000: loss 0.000385\n",
      "ending training at step 13129: loss 0.000100\n",
      "Optimizing embedding for 'p'\n",
      "step     0: loss 5.491922\n",
      "step  5000: loss 0.004592\n",
      "step 10000: loss 0.000334\n",
      "ending training at step 12591: loss 0.000100\n",
      "Optimizing embedding for 'q'\n",
      "step     0: loss 7.071236\n",
      "step  5000: loss 0.009648\n",
      "step 10000: loss 0.000780\n",
      "ending training at step 14544: loss 0.000100\n",
      "Optimizing embedding for 'r'\n",
      "step     0: loss 4.199371\n",
      "step  5000: loss 0.003764\n",
      "step 10000: loss 0.000269\n",
      "ending training at step 12097: loss 0.000100\n",
      "Optimizing embedding for 's'\n",
      "step     0: loss 3.382303\n",
      "step  5000: loss 0.003826\n",
      "step 10000: loss 0.000292\n",
      "ending training at step 12371: loss 0.000100\n",
      "Optimizing embedding for 't'\n",
      "step     0: loss 3.480482\n",
      "step  5000: loss 0.002713\n",
      "step 10000: loss 0.000215\n",
      "ending training at step 11708: loss 0.000100\n",
      "Optimizing embedding for 'u'\n",
      "step     0: loss 4.638258\n",
      "step  5000: loss 0.004020\n",
      "step 10000: loss 0.000304\n",
      "ending training at step 12410: loss 0.000100\n",
      "Optimizing embedding for 'v'\n",
      "step     0: loss 5.588468\n",
      "step  5000: loss 0.005722\n",
      "step 10000: loss 0.000431\n",
      "ending training at step 13151: loss 0.000100\n",
      "Optimizing embedding for 'w'\n",
      "step     0: loss 5.171694\n",
      "step  5000: loss 0.005444\n",
      "step 10000: loss 0.000419\n",
      "ending training at step 13169: loss 0.000100\n",
      "Optimizing embedding for 'x'\n",
      "step     0: loss 7.298410\n",
      "step  5000: loss 0.006922\n",
      "step 10000: loss 0.000575\n",
      "ending training at step 13903: loss 0.000100\n",
      "Optimizing embedding for 'y'\n",
      "step     0: loss 4.950903\n",
      "step  5000: loss 0.004004\n",
      "step 10000: loss 0.000316\n",
      "ending training at step 12507: loss 0.000100\n",
      "Optimizing embedding for 'z'\n",
      "step     0: loss 7.318987\n",
      "step  5000: loss 0.005466\n",
      "step 10000: loss 0.000425\n",
      "ending training at step 13100: loss 0.000100\n"
     ]
    }
   ],
   "source": [
    "for target_char in tqdm(tokenizer.chars):\n",
    "    multi_embs, _ = learn_embedding_for_char(\n",
    "        target_char, embedding_to_logits_function, n_embeddings_to_learn=n_embeddings_to_learn\n",
    "    )\n",
    "    torch.save(multi_embs, sub_dir / f'{filename_for_token(target_char)}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-experiments",
   "language": "python",
   "name": "transformer-experiments"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
