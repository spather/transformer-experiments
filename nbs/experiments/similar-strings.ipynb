{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# similar-strings\n",
    "\n",
    "> Experiment to find similar strings based on various block intermediate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experiments.similar_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from collections import defaultdict, OrderedDict\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "from typing import Callable, Dict, Iterable, Iterator, List, Optional, Sequence, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import click\n",
    "import torch\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from transformer_experiments.common.substring_generator import all_unique_substrings\n",
    "from transformer_experiments.datasets.tinyshakespeare import (\n",
    "    TinyShakespeareDataSet,\n",
    ")\n",
    "from transformer_experiments.experiments.block_internals import (\n",
    "    BlockInternalsExperiment,\n",
    "    BatchedBlockInternalsExperiment,\n",
    ")\n",
    "from transformer_experiments.models.transformer import (\n",
    "    block_size,\n",
    "    n_layer,\n",
    "    TransformerLanguageModel\n",
    ")\n",
    "from transformer_experiments.models.transformer_helpers import (\n",
    "    EncodingHelpers,\n",
    "    TransformerAccessors\n",
    ")\n",
    "from transformer_experiments.tokenizers.char_tokenizer import CharacterTokenizer\n",
    "from transformer_experiments.trained_models.tinyshakespeare_transformer import (\n",
    "    create_model_and_tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TinyShakespeareDataSet(cache_file='../artifacts/input.txt')\n",
    "m, tokenizer = create_model_and_tokenizer(\n",
    "    saved_model_filename='../artifacts/shakespeare.pt',\n",
    "    dataset=ts,\n",
    "    device=device,\n",
    ")\n",
    "encoding_helpers = EncodingHelpers(tokenizer, device)\n",
    "accessors = TransformerAccessors(m, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class SimilarStringsData:\n",
    "    sim_strings: Sequence[str]\n",
    "    distances: torch.Tensor\n",
    "\n",
    "@dataclass\n",
    "class SimilarStringsResult:\n",
    "    s: str\n",
    "    embs: SimilarStringsData\n",
    "    proj_out: List[List[SimilarStringsData]] = field(default_factory=lambda: [[] for _ in range(n_layer)])\n",
    "    ffwd_out: List[List[SimilarStringsData]] = field(default_factory=lambda: [[] for _ in range(n_layer)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class SimilarStringsExperiment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dir: Path,\n",
    "        encoding_helpers: EncodingHelpers,\n",
    "    ):\n",
    "        self.output_dir = output_dir\n",
    "        self.encoding_helpers = encoding_helpers\n",
    "        self.string_to_batch_map: Optional[Dict[str, int]] = None\n",
    "\n",
    "    def _string_to_batch_map_filename(self) -> Path:\n",
    "        return self.output_dir / 'string_to_batch_map.json'\n",
    "\n",
    "    def _embs_sim_strings_filename(self, batch_idx: int) -> Path:\n",
    "        return self.output_dir / f'embs_sim_strings-{batch_idx:03d}.json'\n",
    "\n",
    "    def _proj_out_sim_strings_filename(\n",
    "        self, batch_idx: int, block_idx: int, t_i: int\n",
    "    ) -> Path:\n",
    "        return (\n",
    "            self.output_dir\n",
    "            / f'proj_out_sim_strings-{batch_idx:03d}-{block_idx:02d}-{t_i:03d}.json'\n",
    "        )\n",
    "\n",
    "    def _ffwd_out_sim_strings_filename(\n",
    "        self, batch_idx: int, block_idx: int, t_i: int\n",
    "    ) -> Path:\n",
    "        return (\n",
    "            self.output_dir\n",
    "            / f'ffwd_out_sim_strings-{batch_idx:03d}-{block_idx:02d}-{t_i:03d}.json'\n",
    "        )\n",
    "\n",
    "    def generate_string_to_batch_map(\n",
    "        self,\n",
    "        strings: Sequence[str],\n",
    "        batch_size: int = 100,\n",
    "        disable_progress_bars: bool = False,\n",
    "    ):\n",
    "        n_batches = math.ceil(len(strings) / batch_size)\n",
    "\n",
    "        string_to_batch_map: Dict[str, int] = {}\n",
    "\n",
    "        for batch_idx in tqdm(range(n_batches), disable=disable_progress_bars):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            batch_strings = strings[start_idx:end_idx]\n",
    "\n",
    "            for s in batch_strings:\n",
    "                string_to_batch_map[s] = batch_idx\n",
    "\n",
    "        self._string_to_batch_map_filename().write_text(json.dumps(string_to_batch_map, indent=2))\n",
    "\n",
    "    def generate_embeddings_files(\n",
    "        self,\n",
    "        strings: Sequence[str],\n",
    "        accessors: TransformerAccessors,\n",
    "        exp: BatchedBlockInternalsExperiment,\n",
    "        batch_size: int = 100,\n",
    "        disable_progress_bars: bool = False,\n",
    "        n_similars: int = 10,\n",
    "    ):\n",
    "        n_batches = math.ceil(len(strings) / batch_size)\n",
    "\n",
    "        for batch_idx in tqdm(range(n_batches), disable=disable_progress_bars):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            batch_strings = strings[start_idx:end_idx]\n",
    "\n",
    "            batch_exp = BlockInternalsExperiment(\n",
    "                self.encoding_helpers, accessors, batch_strings\n",
    "            )\n",
    "\n",
    "            # Compute the embedding similar strings\n",
    "            sim_strings, distances = exp.strings_with_topk_closest_embeddings(\n",
    "                queries=batch_exp.embeddings, k=n_similars, largest=False\n",
    "            )\n",
    "\n",
    "            self._embs_sim_strings_filename(batch_idx).write_text(\n",
    "                json.dumps(\n",
    "                    {\n",
    "                        'strings': {s: i for i, s in enumerate(batch_strings)},\n",
    "                        'sim_strings': sim_strings,\n",
    "                        'distances': distances.tolist(),\n",
    "                    },\n",
    "                    indent=2,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def generate_proj_out_files(\n",
    "        self,\n",
    "        strings: Sequence[str],\n",
    "        t_i: int,\n",
    "        accessors: TransformerAccessors,\n",
    "        exp: BatchedBlockInternalsExperiment,\n",
    "        batch_size: int = 100,\n",
    "        disable_progress_bars: bool = False,\n",
    "        n_similars: int = 10,\n",
    "    ):\n",
    "        filename_t_i = t_i\n",
    "        if filename_t_i < 0:\n",
    "            filename_t_i = exp.sample_length() + filename_t_i\n",
    "        assert (\n",
    "            filename_t_i >= 0\n",
    "        ), f\"converted t_i must be >= 0, was {filename_t_i}\"\n",
    "\n",
    "        n_batches = math.ceil(len(strings) / batch_size)\n",
    "\n",
    "        for batch_idx in tqdm(range(n_batches), disable=disable_progress_bars):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            batch_strings = strings[start_idx:end_idx]\n",
    "\n",
    "            batch_exp = BlockInternalsExperiment(\n",
    "                self.encoding_helpers, accessors, batch_strings\n",
    "            )\n",
    "\n",
    "            for block_idx in range(n_layer):\n",
    "                # Compute the proj_out similar strings\n",
    "                sim_strings, distances = exp.strings_with_topk_closest_proj_outputs(\n",
    "                    block_idx=block_idx,\n",
    "                    t_i=t_i,\n",
    "                    # Query is always the last token - for something else, use a shorter string\n",
    "                    queries=batch_exp.proj_output(block_idx)[:, -1, :],\n",
    "                    k=n_similars,\n",
    "                    largest=False,\n",
    "                )\n",
    "                self._proj_out_sim_strings_filename(\n",
    "                    batch_idx, block_idx, filename_t_i\n",
    "                ).write_text(\n",
    "                    json.dumps(\n",
    "                        {\n",
    "                            'strings': {s: i for i, s in enumerate(batch_strings)},\n",
    "                            'sim_strings': sim_strings,\n",
    "                            'distances': distances.tolist(),\n",
    "                        },\n",
    "                        indent=2,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def generate_ffwd_out_files(\n",
    "        self,\n",
    "        strings: Sequence[str],\n",
    "        t_i: int,\n",
    "        accessors: TransformerAccessors,\n",
    "        exp: BatchedBlockInternalsExperiment,\n",
    "        batch_size: int = 100,\n",
    "        disable_progress_bars: bool = False,\n",
    "        n_similars: int = 10,\n",
    "    ):\n",
    "        n_batches = math.ceil(len(strings) / batch_size)\n",
    "\n",
    "        filename_t_i = t_i\n",
    "        if filename_t_i < 0:\n",
    "            filename_t_i = exp.sample_length() + filename_t_i\n",
    "        assert (\n",
    "            filename_t_i >= 0\n",
    "        ), f\"converted t_i must be >= 0, was {filename_t_i}\"\n",
    "\n",
    "        for batch_idx in tqdm(range(n_batches), disable=disable_progress_bars):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            batch_strings = strings[start_idx:end_idx]\n",
    "\n",
    "            batch_exp = BlockInternalsExperiment(\n",
    "                self.encoding_helpers, accessors, batch_strings\n",
    "            )\n",
    "\n",
    "            for block_idx in range(n_layer):\n",
    "                sim_strings, distances = exp.strings_with_topk_closest_ffwd_outputs(\n",
    "                    block_idx=block_idx,\n",
    "                    t_i=t_i,\n",
    "                    # Query is always the last token - for something else, use a shorter string\n",
    "                    queries=batch_exp.ffwd_output(block_idx)[:, -1, :],\n",
    "                    k=n_similars,\n",
    "                    largest=False,\n",
    "                )\n",
    "\n",
    "                self._ffwd_out_sim_strings_filename(\n",
    "                    batch_idx, block_idx, filename_t_i\n",
    "                ).write_text(\n",
    "                    json.dumps(\n",
    "                        {\n",
    "                            'strings': {s: i for i, s in enumerate(batch_strings)},\n",
    "                            'sim_strings': sim_strings,\n",
    "                            'distances': distances.tolist(),\n",
    "                        },\n",
    "                        indent=2,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def _load_json(self, file: Path):\n",
    "        return json.loads(file.read_text())\n",
    "\n",
    "    def load_string_to_batch_map(self):\n",
    "        if self.string_to_batch_map is not None:\n",
    "            return\n",
    "\n",
    "        self.string_to_batch_map = self._load_json(self._string_to_batch_map_filename())\n",
    "\n",
    "    def load_results_for_strings(self, strings: Sequence[str], load_t_is: Sequence[int] = [-1]):\n",
    "        self.load_string_to_batch_map()\n",
    "        assert self.string_to_batch_map is not None\n",
    "\n",
    "        sample_len = len(next(iter(self.string_to_batch_map.keys())))\n",
    "        # Convert any negative t_is to positive.\n",
    "        load_t_is = [t_i if t_i >= 0 else sample_len + t_i for t_i in load_t_is]\n",
    "\n",
    "        assert all(\n",
    "            0 <= t_i < sample_len for t_i in load_t_is\n",
    "        ), f\"all t_is must be in [0, {sample_len}), were {load_t_is}\"\n",
    "\n",
    "        batch_to_strings: Dict[int, List[str]] = defaultdict(list)\n",
    "        for s in strings:\n",
    "            batch_idx = self.string_to_batch_map[s]\n",
    "            batch_to_strings[batch_idx].append(s)\n",
    "\n",
    "        string_to_results: Dict[str, SimilarStringsResult] = {}\n",
    "        for batch_idx, strings in batch_to_strings.items():\n",
    "            emb_batch = self._load_json(self._embs_sim_strings_filename(batch_idx))\n",
    "            emb_distances = torch.tensor(emb_batch['distances'], dtype=torch.float32)\n",
    "\n",
    "            for s in strings:\n",
    "                s_idx = emb_batch['strings'][s]\n",
    "                sim_strings = emb_batch['sim_strings'][s_idx]\n",
    "                distances = emb_distances[:, s_idx]\n",
    "\n",
    "                emb_data = SimilarStringsData(sim_strings, distances)\n",
    "                string_to_results[s] = SimilarStringsResult(s, emb_data)\n",
    "\n",
    "            for block_idx in range(n_layer):\n",
    "                for t_i in load_t_is:\n",
    "                    proj_batch = self._load_json(\n",
    "                        self._proj_out_sim_strings_filename(\n",
    "                            batch_idx=batch_idx, block_idx=block_idx, t_i=t_i\n",
    "                        )\n",
    "                    )\n",
    "                    proj_distances = torch.tensor(\n",
    "                        proj_batch['distances'], dtype=torch.float32\n",
    "                    )\n",
    "\n",
    "                    for s in strings:\n",
    "                        s_idx = proj_batch['strings'][s]\n",
    "                        sim_strings = proj_batch['sim_strings'][s_idx]\n",
    "                        distances = proj_distances[:, s_idx]\n",
    "                        string_to_results[s].proj_out[block_idx].append(\n",
    "                            SimilarStringsData(sim_strings, distances)\n",
    "                        )\n",
    "\n",
    "                    ffwd_batch = self._load_json(\n",
    "                        self._ffwd_out_sim_strings_filename(\n",
    "                            batch_idx=batch_idx, block_idx=block_idx, t_i=t_i\n",
    "                        )\n",
    "                    )\n",
    "                    ffwd_distances = torch.tensor(\n",
    "                        ffwd_batch['distances'], dtype=torch.float32\n",
    "                    )\n",
    "\n",
    "                    for s in strings:\n",
    "                        s_idx = ffwd_batch['strings'][s]\n",
    "                        sim_strings = ffwd_batch['sim_strings'][s_idx]\n",
    "                        distances = ffwd_distances[:, s_idx]\n",
    "                        string_to_results[s].ffwd_out[block_idx].append(\n",
    "                            SimilarStringsData(sim_strings, distances)\n",
    "                        )\n",
    "        return string_to_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for SimilarStringsExperiment\n",
    "s_len = 3\n",
    "strings = all_unique_substrings(ts.text[:100], s_len)\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    tmpdir = Path(tmpdirname)\n",
    "    experiment = BatchedBlockInternalsExperiment(\n",
    "        encoding_helpers, accessors, strings, output_dir=tmpdir, batch_size=10\n",
    "    )\n",
    "    test_eq(experiment.sample_length(), s_len)\n",
    "    experiment.run(disable_progress_bars=True)\n",
    "\n",
    "    ss_dir = tmpdir / 'similar_strings'\n",
    "    ss_dir.mkdir(exist_ok=True)\n",
    "    ssexp = SimilarStringsExperiment(ss_dir, encoding_helpers)\n",
    "    batch_size = 10\n",
    "\n",
    "    # Test that the string_to_batch_map file is generated correctly\n",
    "    ssexp.generate_string_to_batch_map(\n",
    "        strings, batch_size=batch_size, disable_progress_bars=True\n",
    "    )\n",
    "    test_eq(\n",
    "        json.loads(ssexp._string_to_batch_map_filename().read_text()),\n",
    "        {s: i // batch_size for i, s in enumerate(strings)},\n",
    "    )\n",
    "\n",
    "    # Test generating embeddings files\n",
    "    ssexp.generate_embeddings_files(\n",
    "        strings,\n",
    "        accessors,\n",
    "        experiment,\n",
    "        batch_size=batch_size,\n",
    "        n_similars=3,\n",
    "        disable_progress_bars=True,\n",
    "    )\n",
    "\n",
    "    # Test that the expected files exist\n",
    "    expected_n_batches = math.ceil(len(strings) / batch_size)\n",
    "    test_eq(len(list(ss_dir.glob('embs_sim_strings-*'))), expected_n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "# Additional tests for SimilarStringsExperiment that are slow\n",
    "s_len = 3\n",
    "strings = all_unique_substrings(ts.text[:100], s_len)\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    tmpdir = Path(tmpdirname)\n",
    "    experiment = BatchedBlockInternalsExperiment(\n",
    "        encoding_helpers, accessors, strings, output_dir=tmpdir, batch_size=10\n",
    "    )\n",
    "    test_eq(experiment.sample_length(), s_len)\n",
    "    experiment.run(disable_progress_bars=True)\n",
    "\n",
    "    ss_dir = tmpdir / 'similar_strings'\n",
    "    ss_dir.mkdir(exist_ok=True)\n",
    "    ssexp = SimilarStringsExperiment(ss_dir, encoding_helpers)\n",
    "    batch_size = 10\n",
    "\n",
    "    ssexp.generate_string_to_batch_map(\n",
    "        strings, batch_size=batch_size, disable_progress_bars=True\n",
    "    )\n",
    "    ssexp.generate_embeddings_files(\n",
    "        strings,\n",
    "        accessors,\n",
    "        experiment,\n",
    "        batch_size=batch_size,\n",
    "        n_similars=3,\n",
    "        disable_progress_bars=True,\n",
    "    )\n",
    "\n",
    "    # Test generating proj_out files\n",
    "    t_is = [1, 2]\n",
    "    for t_i in t_is:\n",
    "        ssexp.generate_proj_out_files(\n",
    "            strings,\n",
    "            t_i,\n",
    "            accessors,\n",
    "            experiment,\n",
    "            batch_size=batch_size,\n",
    "            n_similars=3,\n",
    "            disable_progress_bars=True,\n",
    "        )\n",
    "    test_eq(\n",
    "        len(list(ss_dir.glob('proj_out_sim_strings-*'))), expected_n_batches * n_layer * len(t_is)\n",
    "    )\n",
    "\n",
    "    # Test generating ffwd_out files\n",
    "    t_is = [1, 2]\n",
    "    for t_i in t_is:\n",
    "        ssexp.generate_ffwd_out_files(\n",
    "            strings,\n",
    "            t_i,\n",
    "            accessors,\n",
    "            experiment,\n",
    "            batch_size=batch_size,\n",
    "            n_similars=3,\n",
    "            disable_progress_bars=True,\n",
    "        )\n",
    "    test_eq(\n",
    "        len(list(ss_dir.glob('ffwd_out_sim_strings-*'))), expected_n_batches * n_layer * len(t_is)\n",
    "    )\n",
    "\n",
    "    # Test result loading\n",
    "    s_to_results = ssexp.load_results_for_strings(['Fir', 'for', 'ize'], load_t_is=t_is)\n",
    "    test_eq(len(s_to_results), 3)\n",
    "    test_eq(s_to_results['Fir'].s, 'Fir')\n",
    "    test_eq(s_to_results['Fir'].embs.sim_strings, ['Fir', 'for', 'ear'])\n",
    "    test_close(\n",
    "        s_to_results['Fir'].embs.distances, torch.tensor([0.0, 38.56662, 38.57690])\n",
    "    )\n",
    "\n",
    "    test_eq(s_to_results['ize'].s, 'ize')\n",
    "\n",
    "    # Compare proj_out for first t_i:\n",
    "    test_eq(s_to_results['ize'].proj_out[0][0].sim_strings, ['ze', 'iz', '\\nB'])\n",
    "    test_close(\n",
    "        s_to_results['ize'].proj_out[0][0].distances,\n",
    "        torch.tensor([7.21927, 10.30423, 10.80355]),\n",
    "    )\n",
    "\n",
    "    # Compare proj_out for second t_i:\n",
    "    test_eq(s_to_results['ize'].proj_out[0][1].sim_strings, ['ize', 'zen', 'Spe'])\n",
    "    test_close(\n",
    "        s_to_results['ize'].proj_out[0][1].distances,\n",
    "        torch.tensor([0.0, 7.81562, 8.25619]),\n",
    "    )\n",
    "\n",
    "    test_eq(s_to_results['for'].s, 'for')\n",
    "\n",
    "    # Compare ffwd_out for first t_i:\n",
    "    test_eq(s_to_results['for'].ffwd_out[3][0].sim_strings, ['or', 'ak', 'ar'])\n",
    "    test_close(\n",
    "        s_to_results['for'].ffwd_out[3][0].distances,\n",
    "        torch.tensor([9.19036, 10.75787, 10.82239]),\n",
    "    )\n",
    "\n",
    "    # Compare ffwd_out for second t_i:\n",
    "    test_eq(s_to_results['for'].ffwd_out[3][1].sim_strings, ['for', 'fur', 'oce'])\n",
    "    test_close(\n",
    "        s_to_results['for'].ffwd_out[3][1].distances,\n",
    "        torch.tensor([0.0, 11.16131, 11.18882]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "# CLI for generating similar strings files\n",
    "@click.group()\n",
    "@click.argument(\"dataset_cache_filename\", type=click.Path(exists=True))\n",
    "@click.argument(\"output_folder\", type=click.Path(exists=True))\n",
    "@click.option(\n",
    "    \"-b\",\n",
    "    \"--batch_size\",\n",
    "    required=False,\n",
    "    type=click.IntRange(min=1),\n",
    "    default=100,\n",
    ")\n",
    "@click.option(\n",
    "    \"-s\",\n",
    "    \"--sample_len\",\n",
    "    required=True,\n",
    "    type=click.IntRange(min=1, max=block_size),\n",
    ")\n",
    "@click.option(\n",
    "    \"-r\",\n",
    "    \"--random_seed\",\n",
    "    required=True,\n",
    "    type=click.INT,\n",
    ")\n",
    "@click.option(\n",
    "    \"--n_samples\",\n",
    "    required=True,\n",
    "    type=click.IntRange(min=1),\n",
    ")\n",
    "@click.pass_context\n",
    "def run(\n",
    "    ctx: click.Context,\n",
    "    dataset_cache_filename: str,\n",
    "    output_folder: str,\n",
    "    sample_len: int,\n",
    "    n_samples: int,\n",
    "    random_seed: int,\n",
    "    batch_size: int,\n",
    "):\n",
    "    ctx.ensure_object(dict)\n",
    "\n",
    "    click.echo(\"SimilarStringsExperiment CLI\")\n",
    "    click.echo()\n",
    "    click.echo(f\"  dataset cache: {dataset_cache_filename}\")\n",
    "    click.echo(f\"  output folder: {output_folder}\")\n",
    "\n",
    "    click.echo()\n",
    "    click.echo(f\"  sample length: {sample_len}\")\n",
    "    click.echo(f\"  n samples: {n_samples}\")\n",
    "    click.echo(f\"  random seed: {random_seed}\")\n",
    "\n",
    "    click.echo()\n",
    "    click.echo(f\"  batch size: {batch_size}\")\n",
    "    click.echo()\n",
    "\n",
    "    ctx.obj['batch_size'] = batch_size\n",
    "\n",
    "    ts = TinyShakespeareDataSet(cache_file=dataset_cache_filename)\n",
    "    ctx.obj['ts'] = ts\n",
    "\n",
    "    all_strings = all_unique_substrings(ts.text, sample_len)\n",
    "    ctx.obj['all_strings'] = all_strings\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    indices = torch.randperm(len(all_strings))[:n_samples]\n",
    "    strings = [all_strings[i.item()] for i in indices]\n",
    "\n",
    "    ctx.obj['strings'] = strings\n",
    "\n",
    "    tokenizer = CharacterTokenizer(text=ts.text)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    click.echo(f\"device is {device}\")\n",
    "    ctx.obj['device'] = device\n",
    "\n",
    "    encoding_helpers = EncodingHelpers(tokenizer, device)\n",
    "\n",
    "    ss_exp = SimilarStringsExperiment(Path(output_folder), encoding_helpers)\n",
    "    ctx.obj['ss_exp'] = ss_exp\n",
    "\n",
    "\n",
    "@run.command()\n",
    "@click.pass_context\n",
    "def generate_string_to_batch_map(ctx: click.Context):\n",
    "    click.echo(\"Generating string to batch map...\")\n",
    "    click.echo()\n",
    "\n",
    "    ss_exp = ctx.obj['ss_exp']\n",
    "    strings = ctx.obj['strings']\n",
    "\n",
    "    ss_exp.generate_string_to_batch_map(strings, batch_size=ctx.obj['batch_size'])\n",
    "\n",
    "    click.echo(f\"Wrote {ss_exp._string_to_batch_map_filename()}\")\n",
    "\n",
    "\n",
    "@run.group()\n",
    "@click.argument(\"model_weights_filename\", type=click.Path(exists=True))\n",
    "@click.option(\n",
    "    \"-o\",\n",
    "    \"--block_internals_experiment_output_folder\",\n",
    "    required=True,\n",
    "    type=click.Path(exists=True),\n",
    ")\n",
    "@click.option(\n",
    "    \"-m\",\n",
    "    \"--block_internals_experiment_max_batch_size\",\n",
    "    required=False,\n",
    "    type=click.IntRange(min=1),\n",
    "    default=10000,\n",
    ")\n",
    "@click.option(\n",
    "    \"-n\",\n",
    "    \"--n_similars\",\n",
    "    required=False,\n",
    "    type=click.IntRange(min=1),\n",
    "    default=10,\n",
    ")\n",
    "@click.pass_context\n",
    "def generate_similars(\n",
    "    ctx: click.Context,\n",
    "    model_weights_filename: str,\n",
    "    block_internals_experiment_output_folder: str,\n",
    "    block_internals_experiment_max_batch_size: int,\n",
    "    n_similars: int,\n",
    "):\n",
    "    click.echo(\"Generation parameters:\")\n",
    "\n",
    "    click.echo(f\"  model weights: {model_weights_filename}\")\n",
    "    click.echo()\n",
    "\n",
    "    click.echo(\n",
    "        f\"  block internals experiment output folder: {block_internals_experiment_output_folder}\"\n",
    "    )\n",
    "    click.echo(\n",
    "        f\"  block internals experiment max batch size: {block_internals_experiment_max_batch_size}\"\n",
    "    )\n",
    "    click.echo()\n",
    "\n",
    "    click.echo(f\"  n similars: {n_similars}\")\n",
    "    click.echo()\n",
    "\n",
    "    ctx.obj['n_similars'] = n_similars\n",
    "\n",
    "    # Instantiate the model, tokenizer, and dataset\n",
    "    device: str = ctx.obj['device']\n",
    "\n",
    "    m, tokenizer = create_model_and_tokenizer(\n",
    "        saved_model_filename=model_weights_filename,\n",
    "        dataset=ctx.obj['ts'],\n",
    "        device=device,\n",
    "    )\n",
    "    encoding_helpers = EncodingHelpers(tokenizer, device)\n",
    "    accessors = TransformerAccessors(m, device)\n",
    "    ctx.obj['accessors'] = accessors\n",
    "\n",
    "    ctx.obj['exp'] = BatchedBlockInternalsExperiment(\n",
    "        encoding_helpers,\n",
    "        accessors,\n",
    "        ctx.obj['all_strings'],\n",
    "        output_dir=Path(block_internals_experiment_output_folder),\n",
    "        batch_size=block_internals_experiment_max_batch_size,\n",
    "    )\n",
    "\n",
    "@generate_similars.command()\n",
    "@click.pass_context\n",
    "def embeddings(ctx: click.Context):\n",
    "    click.echo(\"Generating embeddings similars...\")\n",
    "\n",
    "    ss_exp: SimilarStringsExperiment = ctx.obj['ss_exp']\n",
    "    accessors: TransformerAccessors = ctx.obj['accessors']\n",
    "\n",
    "    ss_exp.generate_embeddings_files(\n",
    "        ctx.obj['strings'],\n",
    "        accessors,\n",
    "        ctx.obj['exp'],\n",
    "        batch_size=ctx.obj['batch_size'],\n",
    "        n_similars=ctx.obj['n_similars'],\n",
    "    )\n",
    "\n",
    "    click.echo(\"Generated embeddings similar strings files.\")\n",
    "\n",
    "@generate_similars.command()\n",
    "@click.option(\n",
    "    \"-t\",\n",
    "    \"--t_index\",\n",
    "    required=True,\n",
    "    type=click.IntRange(min=0),\n",
    ")\n",
    "@click.pass_context\n",
    "def proj_out(ctx: click.Context, t_index: int):\n",
    "    click.echo(\"Generating proj_out similars...\")\n",
    "    click.echo(f\"  t_index: {t_index}\")\n",
    "\n",
    "    if t_index >= ctx.obj['exp'].sample_length():\n",
    "        raise click.BadParameter(\n",
    "            f\"t_index must be less than sample length ({ctx.obj['exp'].sample_length()})\",\n",
    "            param_hint=\"t_index\",\n",
    "        )\n",
    "\n",
    "    ss_exp: SimilarStringsExperiment = ctx.obj['ss_exp']\n",
    "    accessors: TransformerAccessors = ctx.obj['accessors']\n",
    "\n",
    "    ss_exp.generate_proj_out_files(\n",
    "        ctx.obj['strings'],\n",
    "        t_index,\n",
    "        accessors,\n",
    "        ctx.obj['exp'],\n",
    "        batch_size=ctx.obj['batch_size'],\n",
    "        n_similars=ctx.obj['n_similars'],\n",
    "    )\n",
    "\n",
    "    click.echo(\"Generated proj_out similar strings files.\")\n",
    "\n",
    "@generate_similars.command()\n",
    "@click.option(\n",
    "    \"-t\",\n",
    "    \"--t_index\",\n",
    "    required=True,\n",
    "    type=click.IntRange(min=0),\n",
    ")\n",
    "@click.pass_context\n",
    "def ffwd_out(ctx: click.Context, t_index: int):\n",
    "    click.echo(\"Generating ffwd_out similars...\")\n",
    "    click.echo(f\"  t_index: {t_index}\")\n",
    "\n",
    "    if t_index >= ctx.obj['exp'].sample_length():\n",
    "        raise click.BadParameter(\n",
    "            f\"t_index must be less than sample length ({ctx.obj['exp'].sample_length()})\",\n",
    "            param_hint=\"t_index\",\n",
    "        )\n",
    "\n",
    "    ss_exp: SimilarStringsExperiment = ctx.obj['ss_exp']\n",
    "    accessors: TransformerAccessors = ctx.obj['accessors']\n",
    "\n",
    "    ss_exp.generate_ffwd_out_files(\n",
    "        ctx.obj['strings'],\n",
    "        t_index,\n",
    "        accessors,\n",
    "        ctx.obj['exp'],\n",
    "        batch_size=ctx.obj['batch_size'],\n",
    "        n_similars=ctx.obj['n_similars'],\n",
    "    )\n",
    "\n",
    "    click.echo(\"Generated ffwd_out similar strings files.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
