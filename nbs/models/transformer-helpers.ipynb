{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformer-helpers.ipynb\n",
    "\n",
    "> Code that helps run and inspect parts of the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.transformer_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.test import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "from transformer_experiments.models.transformer import (\n",
    "    block_size,\n",
    "    Block,\n",
    "    n_head,\n",
    "    n_embed,\n",
    "    TransformerLanguageModel\n",
    ")\n",
    "from transformer_experiments.tokenizers.char_tokenizer import CharacterTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not exported because only used for testing within this notebook\n",
    "from transformer_experiments.datasets.tinyshakespeare import (\n",
    "    TinyShakespeareDataSet,\n",
    ")\n",
    "from transformer_experiments.trained_models.tinyshakespeare_transformer import (\n",
    "    create_model_and_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "# Create a model for testing\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device is {device}\")\n",
    "\n",
    "ts = TinyShakespeareDataSet(cache_file='../artifacts/input.txt')\n",
    "m, tokenizer = create_model_and_tokenizer(\n",
    "    saved_model_filename='../artifacts/shakespeare.pt',\n",
    "    dataset=ts,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for encoding/decoding embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class EncodingHelpers:\n",
    "    def __init__(\n",
    "        self, m: TransformerLanguageModel, tokenizer: CharacterTokenizer, device: str\n",
    "    ):\n",
    "        self.m = m\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def tokenize_string(self, s: str) -> torch.Tensor:\n",
    "        \"\"\"Given a string, returns a tensor representing the tokenized string.\n",
    "        The returned tensor has shape (1, T) where T is the number of tokens,\n",
    "        so it works in situations that expect a batch dimension.\"\"\"\n",
    "        return torch.tensor(\n",
    "            [self.tokenizer.encode(s)], dtype=torch.long, device=self.device\n",
    "        )\n",
    "    \n",
    "    def stringify_tokens(self, tokens: torch.Tensor) -> str:    \n",
    "        \"\"\"Given a tensor of tokens, returns a string representing the tokens.\"\"\"\n",
    "        return self.tokenizer.decode(tokens.tolist())\n",
    "\n",
    "    def embed_string(self, s: str) -> torch.Tensor:\n",
    "        \"\"\"Given a string, performs the token and positional embeddings\n",
    "        done at the beginning of the model and returns the tensor that\n",
    "        would be sent into the stack of blocks.\"\"\"\n",
    "        return self.embed_tokens(self.tokenize_string(s))\n",
    "\n",
    "    def embed_tokens(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Given a tensor of tokens containing a batch of tokens (shape B, T),\n",
    "        performs the token and positional embeddings done at the beginning of\n",
    "        the model and returns the tensor that would be sent into the stack of\n",
    "        blocks.\"\"\"\n",
    "        idx = tokens[:, -block_size:]\n",
    "\n",
    "        # Logic from the model's forward() function\n",
    "        B, T = idx.shape\n",
    "        token_emb = self.m.token_embedding_table(idx)\n",
    "        pos_emb = self.m.position_embedding_table(\n",
    "            torch.arange(T, device=self.device)\n",
    "        )  # (T, n_embed)\n",
    "        x = token_emb + pos_emb\n",
    "        return x.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for encoding helpers\n",
    "encoding_helpers = EncodingHelpers(m, tokenizer, device)\n",
    "tokenized = encoding_helpers.tokenize_string('hello')\n",
    "test_eq(tokenized.shape, (1, 5))\n",
    "test_eq(tokenized, torch.tensor([[46, 43, 50, 50, 53]]))\n",
    "\n",
    "stringified = encoding_helpers.stringify_tokens(tokenized[0])\n",
    "test_eq(stringified, 'hello')\n",
    "\n",
    "embedded = encoding_helpers.embed_string('hello')\n",
    "test_eq(embedded.shape, (1, 5, n_embed))\n",
    "\n",
    "embedded_from_tokens = encoding_helpers.embed_tokens(tokenized)\n",
    "test_eq(embedded_from_tokens.shape, (1, 5, n_embed))\n",
    "test_eq(embedded, embedded_from_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers for Running Selective Parts of the Model\n",
    "The functions in this section enable running pieces of the model in isolation and introspecting their intermediate results. This is useful for debugging and understanding the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class InputOutputAccessor:\n",
    "    def __init__(self, activations: Dict[str, Tuple]):\n",
    "        self.activations = activations\n",
    "\n",
    "    def inputs(self, name: str) -> Tuple[torch.Tensor]:\n",
    "        return self.activations[name][0]\n",
    "\n",
    "    def input(self, name: str) -> torch.Tensor:\n",
    "        inps = self.inputs(name)\n",
    "        assert len(inps) == 1\n",
    "        return inps[0]\n",
    "\n",
    "    def output(self, name: str) -> torch.Tensor:\n",
    "        return self.activations[name][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class TransformerAccessors:\n",
    "    \"\"\"Class that provides methods for running pieces of a `TransformerLanguageModel`\n",
    "    in isolation and introspecting their intermediate results.\"\"\"\n",
    "    def __init__(self, m: TransformerLanguageModel):\n",
    "        self.m = m\n",
    "\n",
    "    def copy_block_from_model(self, block_idx: int):\n",
    "        \"\"\"Given the index of a block in the model [0, n_layer), creates\n",
    "        a new block with identical parameters.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple\n",
    "            First element is the new block, second is an `InputOutputAccessor` that\n",
    "            provides access to the inputs and outputs of the block itself, it top-level\n",
    "            sub-modules, and children of the self-attention sub-module.\n",
    "        \"\"\"\n",
    "        block = self.m.blocks[block_idx]\n",
    "        new_block = Block(n_embed, n_head)\n",
    "        new_block.load_state_dict(block.state_dict())\n",
    "        new_block.eval()\n",
    "\n",
    "        activations = {}\n",
    "\n",
    "        def log_activation_hook(name):\n",
    "            def hook(_, input, output):\n",
    "                inputs = tuple([inp.detach() for inp in input])\n",
    "                activations[name] = (inputs, output.detach())\n",
    "\n",
    "            return hook\n",
    "\n",
    "        new_block.register_forward_hook(log_activation_hook('.'))\n",
    "        for name, module in new_block.named_children():\n",
    "            module.register_forward_hook(log_activation_hook(name))\n",
    "\n",
    "        # Register the hook for the self-attention layer's children as\n",
    "        # I will need this. Wanted this function to not have to know\n",
    "        # about the internal structure of a block (i.e. not access members\n",
    "        # by a specific name), so this is unfortunate, but the most expedient.\n",
    "        for name, module in new_block.sa.named_children():\n",
    "            module.register_forward_hook(log_activation_hook(f'sa.{name}'))\n",
    "\n",
    "        return new_block, InputOutputAccessor(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test block copying\n",
    "accessors = TransformerAccessors(m)\n",
    "block_idx = 0\n",
    "old_b = m.blocks[block_idx]\n",
    "new_b, io_accessor = accessors.copy_block_from_model(block_idx)\n",
    "test_eq(new_b is old_b, False)\n",
    "\n",
    "x = encoding_helpers.embed_string('Citizen')\n",
    "old_result = old_b(x).detach()\n",
    "new_result = new_b(x).detach()\n",
    "test_eq(old_result, new_result)\n",
    "test_eq(io_accessor.input('.'), x)\n",
    "test_eq(io_accessor.output('.'), new_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
