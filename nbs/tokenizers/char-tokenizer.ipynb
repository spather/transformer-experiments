{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# char-tokenizer.ipynb\n",
    "\n",
    "> Implementation of a character-level tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution\n",
    "The code in this notebook (`char-tokenizer.ipynb`) and the resulting module (`transformer_experiments.tokenizers.char_tokenizer`) is not mine. It comes from [Andrej Karpathy](https://karpathy.ai/)'s excellent video, [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY). I typed in the code by copying what I saw on the screen as I watched the video. For things that weren't clear onscreen, I referenced the [GitHub repo for the video](https://github.com/karpathy/ng-video-lecture) and the [nanoGPT repo](https://github.com/karpathy/nanoGPT). After getting it working, I made only minor changes to make it work with the rest of the code in/structure of this repository. In summary: this module is Andrej Karpathy's work, not mine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tokenizers.char_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "from typing import Callable, Dict, Iterable, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "SToI = Dict[str, int]\n",
    "IToS = Dict[int, str]\n",
    "EncodeFn = Callable[[str], Iterable[int]]\n",
    "DecodeFn = Callable[[Iterable[int]], str]\n",
    "\n",
    "\n",
    "def create_character_tokenizer(\n",
    "    text: str,\n",
    ") -> Tuple[Iterable[str], int, SToI, IToS, EncodeFn, DecodeFn]:\n",
    "    \"\"\"Create a character tokenizer from text.\"\"\"\n",
    "    chars = sorted(list(set(text)))\n",
    "    vocab_size = len(chars)\n",
    "    stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "    itos = {i: ch for i, ch in enumerate(chars)}\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "    return chars, vocab_size, stoi, itos, encode, decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for create_character_tokenizer\n",
    "chars, vocab_size, stoi, itos, encode, decode = create_character_tokenizer('abcabc')\n",
    "test_eq(chars, ['a', 'b', 'c'])\n",
    "test_eq(vocab_size, 3)\n",
    "test_eq(stoi, {'a': 0, 'b': 1, 'c': 2})\n",
    "test_eq(itos, {0: 'a', 1: 'b', 2: 'c'})\n",
    "test_eq(encode('cab'), [2, 0, 1])\n",
    "test_eq(decode([2, 1, 0]), 'cba')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
