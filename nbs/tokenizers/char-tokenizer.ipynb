{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# char-tokenizer.ipynb\n",
    "\n",
    "> Implementation of a character-level tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution\n",
    "The code in this notebook (`char-tokenizer.ipynb`) and the resulting module (`transformer_experiments.tokenizers.char_tokenizer`) is not mine. It comes from [Andrej Karpathy](https://karpathy.ai/)'s excellent video, [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY). I typed in the code by copying what I saw on the screen as I watched the video. For things that weren't clear onscreen, I referenced the [GitHub repo for the video](https://github.com/karpathy/ng-video-lecture) and the [nanoGPT repo](https://github.com/karpathy/nanoGPT). After getting it working, I made only minor changes to make it work with the rest of the code in/structure of this repository. In summary: this module is Andrej Karpathy's work, not mine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tokenizers.char_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Callable, Dict, Iterable, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class CharacterTokenizer:\n",
    "    def __init__(self, text: str):\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.stoi = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(self.chars)}\n",
    "\n",
    "    def encode(self, s: str) -> Iterable[int]:\n",
    "        return [self.stoi[c] for c in s]\n",
    "\n",
    "    def decode(self, l: Iterable[int]) -> str:\n",
    "        return ''.join([self.itos[i] for i in l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for CharacterTokenizer\n",
    "tokenizer = CharacterTokenizer('abcabc')\n",
    "test_eq(tokenizer.chars, ['a', 'b', 'c'])\n",
    "test_eq(tokenizer.vocab_size, 3)\n",
    "test_eq(tokenizer.stoi, {'a': 0, 'b': 1, 'c': 2})\n",
    "test_eq(tokenizer.itos, {0: 'a', 1: 'b', 2: 'c'})\n",
    "test_eq(tokenizer.encode('cab'), [2, 0, 1])\n",
    "test_eq(tokenizer.decode([2, 1, 0]), 'cba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
