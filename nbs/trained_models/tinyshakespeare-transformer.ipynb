{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tinyshakespeare-transformer\n",
    "\n",
    "> Code for instantiating a pre-trained TinyShakespeare transformer model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp trained_models.tinyshakespeare_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from typing import Callable, Dict, Iterable, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from transformer_experiments.datasets.tinyshakespeare import (\n",
    "    TinyShakespeareDataSet,\n",
    ")\n",
    "from transformer_experiments.models.transformer import TransformerLanguageModel\n",
    "from transformer_experiments.tokenizers.char_tokenizer import (\n",
    "    CharacterTokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_experiments.environments import get_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def create_model_and_tokenizer(\n",
    "    saved_model_filename: str, dataset: TinyShakespeareDataSet, device: str\n",
    ") -> Tuple[\n",
    "    TransformerLanguageModel, CharacterTokenizer\n",
    "]:\n",
    "    \"\"\"Instantiates a pre-trained TinyShakespeare model: creates transformer model,\n",
    "    loads the model params from a saved file, and creates a tokenizer from the dataset's text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a tokenizer from the dataset's text\n",
    "    tokenizer = CharacterTokenizer(dataset.text)\n",
    "\n",
    "    # Create the model\n",
    "    m = TransformerLanguageModel(vocab_size=tokenizer.vocab_size, device=device)\n",
    "    m.to(device)\n",
    "\n",
    "    # Load the model params from a saved file\n",
    "    m.load_state_dict(\n",
    "        torch.load(saved_model_filename, map_location=torch.device(device))\n",
    "    )\n",
    "    m.eval()\n",
    "\n",
    "    return m, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Define names for special characters in the TinyShakespeare dataset\n",
    "# that can be used as filenames.\n",
    "special_char_names = {\n",
    "    '\\n': 'newline',\n",
    "    ' ': 'space',\n",
    "    '!': 'exclamation',\n",
    "    '$': 'dollar',\n",
    "    '&': 'ampersand',\n",
    "    '\\'': 'single_quote',\n",
    "    ',': 'comma',\n",
    "    '-': 'dash',\n",
    "    ':': 'colon',\n",
    "    ';': 'semicolon',\n",
    "    '.': 'period',\n",
    "    '?': 'question',\n",
    "    '3': 'three',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class FilenameForToken:\n",
    "    def __init__(self, tokenizer: CharacterTokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, token: str) -> str:\n",
    "        \"\"\"Given a character, returns a safe filename representing that character.\"\"\"\n",
    "        if token not in self.tokenizer.chars:\n",
    "            raise ValueError(f'unknown character {token}')\n",
    "\n",
    "        if token in special_char_names:\n",
    "            return special_char_names[token]\n",
    "\n",
    "        i = self.tokenizer.stoi[token]\n",
    "        if i >= self.tokenizer.stoi['A'] and i <= self.tokenizer.stoi['Z']:\n",
    "            return f'capital_{token.lower()}'\n",
    "        elif i >= self.tokenizer.stoi['a'] and i <= self.tokenizer.stoi['z']:\n",
    "            return f'lower_{token}'\n",
    "\n",
    "        # Ensure that there is not some character in chars we didn't specifically handle.\n",
    "        raise ValueError(f'unknown character {token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment is local_mac\n"
     ]
    }
   ],
   "source": [
    "environment = get_environment()\n",
    "print(f\"environment is {environment.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TinyShakespeareDataSet(cache_file=environment.code_root / 'nbs/artifacts/input.txt')\n",
    "tokenizer = CharacterTokenizer(ts.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for FilenameForToken\n",
    "filename_for_token = FilenameForToken(tokenizer)\n",
    "\n",
    "test_eq(filename_for_token('A'), 'capital_a')\n",
    "test_eq(filename_for_token('a'), 'lower_a')\n",
    "test_eq(filename_for_token(' '), 'space')\n",
    "test_eq(filename_for_token('\\n'), 'newline')\n",
    "test_eq(filename_for_token('!'), 'exclamation')\n",
    "\n",
    "# Test that we never get the ValueError exception at the end of the function for any character in chars.\n",
    "for token in tokenizer.chars:\n",
    "    filename_for_token(token)\n",
    "\n",
    "# Test that we do get an exception for an unknown character.\n",
    "with ExceptionExpected(ex=ValueError):\n",
    "    filename_for_token('ðŸ¤”')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
