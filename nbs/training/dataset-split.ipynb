{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset-split\n",
    "\n",
    "> Functions to help split a dataset into training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from transformer_experiments.tokenizers.char_tokenizer import (\n",
    "    CharacterTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_text_dataset(text: str, tokenizer: CharacterTokenizer, train_pct: float) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "    n = int(train_pct*len(data))\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for split_text_dataset\n",
    "text_len = 122\n",
    "text = ''.join(['a' for _ in range(text_len)])\n",
    "tokenizer = CharacterTokenizer(text)\n",
    "\n",
    "split_pct = 0.7\n",
    "train_data, val_data = split_text_dataset(text, tokenizer, split_pct)\n",
    "\n",
    "test_eq(len(train_data), int(split_pct*text_len))\n",
    "test_eq(len(val_data), text_len - int(split_pct*text_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
