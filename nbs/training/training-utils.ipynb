{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training-utils.ipynb\n",
    "\n",
    "> Utilities related to model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "from typing import Dict, Generic, List, Protocol, Tuple, TypeVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CheckPointer:\n",
    "    def __init__(self, output_dir: Path, filename_stem: str, start_num: int = 0):\n",
    "        self.output_dir = output_dir\n",
    "        self.filename_stem = filename_stem\n",
    "        self.num = start_num\n",
    "\n",
    "    def filename(self):\n",
    "        return self.output_dir / f'{self.filename_stem}_{self.num:06d}.pt'\n",
    "\n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        iters: int,\n",
    "        model: torch.nn.Module,\n",
    "        train_loss: float,\n",
    "        val_loss: float\n",
    "    ) -> Path:\n",
    "        filename = self.filename()\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"iters\": iters,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "            },\n",
    "            filename,\n",
    "        )\n",
    "        self.num += 1\n",
    "        return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tests for CheckPointer\n",
    "class TestModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.some_param = torch.nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.some_param(x)\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    output_dir = Path(tmpdirname)\n",
    "    checkpointer = CheckPointer(output_dir, 'test')\n",
    "\n",
    "    test_model = TestModule()\n",
    "    test_train_loss = 0.123\n",
    "    test_val_loss = 0.125\n",
    "\n",
    "    filename = checkpointer.save_checkpoint(10, test_model, test_train_loss, test_val_loss)\n",
    "    test_eq(filename, output_dir / 'test_000000.pt')\n",
    "    test_eq(filename.exists(), True)\n",
    "\n",
    "    checkpoint = torch.load(str(output_dir / 'test_000000.pt'))\n",
    "    test_eq(checkpoint['iters'], 10)\n",
    "    test_eq(checkpoint['train_loss'], test_train_loss)\n",
    "    test_eq(checkpoint['val_loss'], test_val_loss)\n",
    "    test_eq('model_state_dict' in checkpoint, True)\n",
    "\n",
    "    filename = checkpointer.save_checkpoint(11, test_model, test_train_loss, test_val_loss)\n",
    "    test_eq(filename, output_dir / 'test_000001.pt')\n",
    "    test_eq(filename.exists(), True)\n",
    "\n",
    "    # Test start_num\n",
    "    checkpointer = CheckPointer(output_dir, 'test', start_num=14)\n",
    "    filename = checkpointer.save_checkpoint(11, test_model, test_train_loss, test_val_loss)\n",
    "    test_eq(filename, output_dir / 'test_000014.pt')\n",
    "    test_eq(filename.exists(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "TModel = TypeVar(\"TModel\", bound=torch.nn.Module, contravariant=True)\n",
    "\n",
    "\n",
    "class EstimateLossFunction(Protocol[TModel]):\n",
    "    def __call__(self, model: TModel) -> Dict[str, float]:\n",
    "        ...\n",
    "\n",
    "\n",
    "class GetBatchFunction(Protocol):\n",
    "    def __call__(self, split: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        ...\n",
    "\n",
    "\n",
    "class OnBatchTrainedHandler(Protocol):\n",
    "    def __call__(self, iters_trained: int, batch: torch.Tensor) -> None:\n",
    "        ...\n",
    "\n",
    "\n",
    "class OnCheckpointSavedHandler(Protocol):\n",
    "    def __call__(self, iters_trained: int, checkpoint_file: Path) -> None:\n",
    "        ...\n",
    "\n",
    "\n",
    "class Trainer(Generic[TModel]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: TModel,\n",
    "        checkpointer: CheckPointer,\n",
    "        get_batch_func: GetBatchFunction,\n",
    "        estimate_loss_func: EstimateLossFunction[TModel],\n",
    "        iters_trained: int = 0,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.checkpointer = checkpointer\n",
    "        self.get_batch_func = get_batch_func\n",
    "        self.estimate_loss_func = estimate_loss_func\n",
    "        self.iters_trained = iters_trained\n",
    "        self.on_batch_trained_handlers: List[OnBatchTrainedHandler] = []\n",
    "        self.on_checkpoint_saved_handlers: List[OnCheckpointSavedHandler] = []\n",
    "\n",
    "    def add_on_batch_trained_handler(self, handler: OnBatchTrainedHandler):\n",
    "        self.on_batch_trained_handlers.append(handler)\n",
    "\n",
    "    def add_on_checkpoint_saved_handler(self, handler: OnCheckpointSavedHandler):\n",
    "        self.on_checkpoint_saved_handlers.append(handler)\n",
    "\n",
    "    def fire_on_batch_trained(self, batch: torch.Tensor):\n",
    "        for handler in self.on_batch_trained_handlers:\n",
    "            handler(self.iters_trained, batch)\n",
    "\n",
    "    def fire_on_checkpoint_saved(self, checkpoint_file: Path):\n",
    "        for handler in self.on_checkpoint_saved_handlers:\n",
    "            handler(self.iters_trained, checkpoint_file)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        n_iters: int,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        eval_interval: int = 500,\n",
    "        disable_progress_bar: bool = False,\n",
    "        disable_output: bool = False,\n",
    "    ):\n",
    "        self.model.train()\n",
    "        for steps in tqdm(range(n_iters), disable=disable_progress_bar):\n",
    "            xb, yb = self.get_batch_func(split=\"train\")\n",
    "\n",
    "            _, loss = self.model(xb, yb)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            self.iters_trained += 1\n",
    "            self.fire_on_batch_trained(xb)\n",
    "\n",
    "            if self.iters_trained % eval_interval == 0:\n",
    "                losses = self.estimate_loss_func(self.model)\n",
    "                if not disable_output:\n",
    "                    print(\n",
    "                        f\"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
    "                    )\n",
    "                checkpoint_filename = self.checkpointer.save_checkpoint(\n",
    "                    self.iters_trained,\n",
    "                    self.model,\n",
    "                    losses[\"train\"],\n",
    "                    losses[\"val\"],\n",
    "                )\n",
    "                self.fire_on_checkpoint_saved(checkpoint_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for Trainer\n",
    "class TestModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.some_param = torch.nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.some_param(x), torch.randn(1, 1, requires_grad=True)\n",
    "\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    output_dir = Path(tmpdirname)\n",
    "    checkpointer = CheckPointer(output_dir, \"test\")\n",
    "\n",
    "    test_model = TestModule()\n",
    "\n",
    "    xbs = []\n",
    "\n",
    "    def get_batch_func(split: str):\n",
    "        xb = torch.randn(1, 10)\n",
    "        xbs.append(xb)\n",
    "        yb = torch.randn(1, 1)\n",
    "\n",
    "        return xb, yb\n",
    "\n",
    "    def estimate_loss_func(model: TestModule):\n",
    "        return {\"train\": 0.123, \"val\": 0.125}\n",
    "\n",
    "    iters_trained_start = 10\n",
    "    trainer = Trainer(\n",
    "        test_model,\n",
    "        checkpointer,\n",
    "        get_batch_func,\n",
    "        estimate_loss_func,\n",
    "        iters_trained=iters_trained_start,\n",
    "    )\n",
    "\n",
    "    on_batch_trained_data = []\n",
    "    trainer.add_on_batch_trained_handler(\n",
    "        lambda iters_trained, batch: on_batch_trained_data.append(\n",
    "            (iters_trained, batch)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    on_checkpoint_saved_data = []\n",
    "    trainer.add_on_checkpoint_saved_handler(\n",
    "        lambda iters_trained, checkpoint_file: on_checkpoint_saved_data.append(\n",
    "            (iters_trained, checkpoint_file)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    trainer.train(\n",
    "        10,\n",
    "        torch.optim.Adam(test_model.parameters()),\n",
    "        eval_interval=5,\n",
    "        disable_progress_bar=True,\n",
    "        disable_output=True,\n",
    "    )\n",
    "    test_eq(len(on_batch_trained_data), 10)\n",
    "    test_eq(len(on_checkpoint_saved_data), 2)\n",
    "\n",
    "    for i, (iters_trained, batch) in enumerate(on_batch_trained_data):\n",
    "        test_eq(iters_trained, iters_trained_start + i + 1)\n",
    "        test_eq(batch.shape, (1, 10))\n",
    "        test_eq(batch, xbs[i])\n",
    "\n",
    "    for i, (iters_trained, checkpoint_file) in enumerate(on_checkpoint_saved_data):\n",
    "        test_eq(iters_trained, iters_trained_start + (i + 1) * 5)\n",
    "        test_eq(checkpoint_file, output_dir / f\"test_{i:06d}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
