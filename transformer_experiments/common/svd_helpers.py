# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/common/svd-helpers.ipynb.

# %% auto 0
__all__ = ['adjust_singular_vector_sign', 'projection_matrix_for_rank_k_approximation']

# %% ../../nbs/common/svd-helpers.ipynb 5
import torch
from torch.nn import functional as F

# %% ../../nbs/common/svd-helpers.ipynb 6
def adjust_singular_vector_sign(
    singular_vector: torch.Tensor, original_matrix: torch.Tensor
) -> torch.Tensor:
    """Depending on the algorithm used to compute the SVD, the sign of the singular
    vectors can be flipped. This function adjusts the sign of the singular vector so
    that it aligns with the majority of the vectors in the original matrix. Per
    https://www.osti.gov/servlets/purl/920802, this is a valid way to resolve the
    sign ambiguity."""
    assert singular_vector.ndim == 1
    assert original_matrix.ndim == 2
    assert singular_vector.shape[0] == original_matrix.shape[1]

    n_negatives = torch.count_nonzero(
        F.cosine_similarity(original_matrix, singular_vector.unsqueeze(dim=0)) < 0
    )
    sign = -1 if n_negatives > original_matrix.shape[0] / 2 else 1
    return sign * singular_vector

# %% ../../nbs/common/svd-helpers.ipynb 8
def projection_matrix_for_rank_k_approximation(
    original_matrix: torch.Tensor, k: int
) -> torch.Tensor:
    """Returns a projection matrix that projects onto the subspace spanned by the top
    k singular vectors of the original matrix. Derivation of the formula:
    https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/00e9c8f0eafedeab21a3d079a17ed3d8_MIT18_06SCF11_Ses2.2sum.pdf
    """
    assert original_matrix.ndim == 2
    assert k > 0 and k <= original_matrix.shape[1]

    _, _, V = torch.linalg.svd(original_matrix, full_matrices=True)
    basis_vectors = []
    for i in range(k):
        basis_vectors.append(adjust_singular_vector_sign(V[i], original_matrix))

    A = torch.stack(basis_vectors).T

    return A @ (A.T @ A).inverse() @ A.T
