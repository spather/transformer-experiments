# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/common/text-analysis.ipynb.

# %% auto 0
__all__ = ['top_nonzero_tokens', 'SubstringFrequencyAnalysis']

# %% ../../nbs/common/text-analysis.ipynb 4
from typing import Dict, Iterable, Sequence, Tuple

# %% ../../nbs/common/text-analysis.ipynb 5
import torch

# %% ../../nbs/common/text-analysis.ipynb 6
def top_nonzero_tokens(
    freqs: torch.Tensor, itos: Dict[int, str]
) -> Iterable[Tuple[str, float]]:
    k = torch.count_nonzero(freqs).item()
    assert isinstance(k, int)  # keep mypy happy
    topk = torch.topk(freqs, k=k)
    return [(itos[i], freqs[i].item()) for i in topk.indices.tolist()]

# %% ../../nbs/common/text-analysis.ipynb 8
class SubstringFrequencyAnalysis:
    """Class that performs frequency analysis on a body of text for a set of substrings."""

    def __init__(
        self,
        substrs: Sequence[str],
        text: str,
        vocab_size: int,
        itos: Dict[int, str],
        stoi: Dict[str, int],
    ):
        self.substrs = substrs
        self.text = text
        self.vocab_size = vocab_size
        self.itos = itos
        self.stoi = stoi

        # Need at least one string to determine the length
        # and for this to be useful.
        assert len(substrs) > 0
        self.s_len = len(substrs[0])

        # Build frequency map of next characters
        self.freq_map = {
            s: torch.zeros(self.vocab_size, dtype=torch.long) for s in self.substrs
        }

        for i in range(len(self.text) - self.s_len):
            s = self.text[i : i + self.s_len]
            if s in self.freq_map:
                next_char = text[i + self.s_len]
                self.freq_map[s][self.stoi[next_char]] += 1

        # Compute the normalized cumulative frequencies
        self.cumulative_freqs = torch.zeros(self.vocab_size, dtype=torch.float32)
        for freqs in self.freq_map.values():
            self.cumulative_freqs += freqs.float()
        self.cumulative_freqs /= self.cumulative_freqs.sum()

        # Figure out the top tokens for each substring
        self.top_tokens = {
            s: top_nonzero_tokens(freqs, self.itos)
            for s, freqs in self.freq_map.items()
        }
        self.top_tokens_cumulative = top_nonzero_tokens(
            self.cumulative_freqs, self.itos
        )

    def print_summary(self):
        print(f"Substrings: {', '.join([repr(substr) for substr in self.substrs])}")

        print("Top Tokens for each substring:")
        for s, tokens in self.top_tokens.items():
            print(
                f"{repr(s)}: {', '.join([f'{repr(token)} ({freq:>3})' for token, freq in tokens])}"
            )

        print("Cumulative Top Tokens:")
        print(
            ", ".join(
                [
                    f"{repr(token)} ({freq:.2f})"
                    for token, freq in self.top_tokens_cumulative
                ]
            )
        )
