# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/common/text-analysis.ipynb.

# %% auto 0
__all__ = ['build_next_token_map', 'top_nonzero_tokens', 'SubstringFrequencyAnalysis']

# %% ../../nbs/common/text-analysis.ipynb 4
from collections import defaultdict
from typing import Dict, Iterable, Sequence, Tuple

# %% ../../nbs/common/text-analysis.ipynb 5
import torch

# %% ../../nbs/common/text-analysis.ipynb 6
def build_next_token_map(
    text: str, prefix_len: int, vocab_size: int, stoi: Dict[str, int]
) -> Dict[str, torch.Tensor]:
    """For a given body of text, build a map of all prefixes of a given
    length to the frequencies of the next token."""
    next_token_map: Dict[str, torch.Tensor] = defaultdict(
        lambda: torch.zeros(vocab_size, dtype=torch.long)
    )

    for i in range(
        len(text) - prefix_len
    ):  # This range ensures the last prefix has a next token
        prefix = text[i : i + prefix_len]
        next_token = text[i + prefix_len]
        next_token_map[prefix][stoi[next_token]] += 1

    return next_token_map

# %% ../../nbs/common/text-analysis.ipynb 8
def top_nonzero_tokens(
    freqs: torch.Tensor, itos: Dict[int, str]
) -> Iterable[Tuple[str, float]]:
    k = torch.count_nonzero(freqs).item()
    assert isinstance(k, int)  # keep mypy happy
    topk = torch.topk(freqs, k=k)
    return [(itos[i], freqs[i].item()) for i in topk.indices.tolist()]

# %% ../../nbs/common/text-analysis.ipynb 10
class SubstringFrequencyAnalysis:
    """Class that performs frequency analysis on a body of text for a set of substrings."""

    def __init__(
        self,
        substrs: Sequence[str],
        next_token_map: Dict[str, torch.Tensor],
        itos: Dict[int, str],
    ):
        # Need at least one string to determine the length
        # and for this to be useful.
        assert len(substrs) > 0
        assert len(substrs[0]) == len(next(iter(next_token_map.keys())))

        self.freq_map = {s: next_token_map[s] for s in substrs}

        # Compute the cumulative frequencies
        vocab_size = len(next(iter(self.freq_map.values())))
        self.cumulative_freqs = torch.zeros(vocab_size, dtype=torch.long)
        for freqs in self.freq_map.values():
            self.cumulative_freqs += freqs

        # Normalize the cumulative frequencies
        self.norm_cumulative_freqs = (
            self.cumulative_freqs.float() / self.cumulative_freqs.sum()
        )

        # Figure out the top tokens for each substring
        self.top_tokens = {
            s: top_nonzero_tokens(freqs, itos) for s, freqs in self.freq_map.items()
        }
        self.top_tokens_cumulative = top_nonzero_tokens(
            self.norm_cumulative_freqs, itos
        )

    def print_summary(self):
        print(
            f"Substrings: {', '.join([repr(substr) for substr in self.freq_map.keys()])}"
        )

        print("Top Tokens for each substring:")
        s_len = len(next(iter(self.freq_map.keys())))
        for s, tokens in self.top_tokens.items():
            print(
                f"{repr(s):>{2*s_len+2}}: {', '.join([f'{repr(token):>4} ({freq:>4})' for token, freq in tokens])}"
            )

        print("Cumulative Top Tokens:")
        print(
            ", ".join(
                [
                    f"{repr(token):>4} ({freq:.2f})"
                    for token, freq in self.top_tokens_cumulative
                ]
            )
        )
