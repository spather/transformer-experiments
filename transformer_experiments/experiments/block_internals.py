# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/experiments/block-internals.ipynb.

# %% auto 0
__all__ = ['BlockInternalsExperiment', 'BlockInternalsAnalysis']

# %% ../../nbs/experiments/block-internals.ipynb 5
from collections import OrderedDict
from dataclasses import dataclass
from matplotlib.axes import Axes
import matplotlib.pyplot as plt
from pathlib import Path
import tempfile
from typing import Dict, Iterable, Iterator, Sequence, Tuple

# %% ../../nbs/experiments/block-internals.ipynb 6
import torch
from tqdm.auto import tqdm

# %% ../../nbs/experiments/block-internals.ipynb 7
from ..common.databatcher import DataBatcher
from ..common.substring_generator import all_unique_substrings
from ..dataset_split import split_text_dataset
from transformer_experiments.datasets.tinyshakespeare import (
    TinyShakespeareDataSet,
)
from transformer_experiments.models.transformer import (
    n_embed,
    n_layer,
    TransformerLanguageModel,
)
from transformer_experiments.models.transformer_helpers import (
    EncodingHelpers,
    LogitsWrapper,
    TransformerAccessors,
)
from ..tokenizers.char_tokenizer import CharacterTokenizer
from transformer_experiments.trained_models.tinyshakespeare_transformer import (
    create_model_and_tokenizer,
)

# %% ../../nbs/experiments/block-internals.ipynb 8
class BlockInternalsExperiment:
    """An experiment to run a bunch of inputs through the model and save the
    intermediate values produced within each block."""

    def __init__(
        self,
        eh: EncodingHelpers,
        accessors: TransformerAccessors,
        strings: Sequence[str],
    ):
        self.eh = eh
        self.accessors = accessors
        self.strings = strings

        tokens = self.eh.tokenize_strings(self.strings)
        self.embeddings = self.accessors.embed_tokens(tokens)

        # Create a map of string to index to enable fast lookup.
        self.idx_map = OrderedDict((s, idx) for idx, s in enumerate(self.strings))

        # Run the embeddings through the model.
        _, self.io_accessors = self.accessors.run_model(self.embeddings)

    def string_idx(self, s: str) -> int:
        """Returns the index of the specified string."""
        return self.idx_map[s]

    def block_input(self, block_idx: int) -> torch.Tensor:
        """Returns the input to the specified block."""
        return self.io_accessors[block_idx].input(".")

    def heads_output(self, block_idx: int) -> torch.Tensor:
        """Returns the output of the attention heads in the specified block."""
        # Heads output is the input to the self-attention proj layer.
        return self.io_accessors[block_idx].input("sa.proj")

    def proj_output(self, block_idx: int) -> torch.Tensor:
        """Returns the output of the self-attention proj layer in the specified block."""
        return self.io_accessors[block_idx].output("sa.proj")

    def ffwd_output(self, block_idx: int) -> torch.Tensor:
        """Returns the output of the feed-forward layer in the specified block."""
        return self.io_accessors[block_idx].output("ffwd")

    def block_output(self, block_idx: int) -> torch.Tensor:
        """Returns the output of the specified block."""
        return self.io_accessors[block_idx].output(".")

# %% ../../nbs/experiments/block-internals.ipynb 12
class BlockInternalsAnalysis:
    """This class performs analysis of how the next token probabilities change
    as an embedded input is passed through each of the blocks in the model"""

    def __init__(
        self,
        accessors: TransformerAccessors,
        encoding_helpers: EncodingHelpers,
        prompt: str,
    ):
        self.accessors = accessors
        self.encoding_helpers = encoding_helpers
        self.prompt = prompt

        # Run the prompt through the model
        tokens = self.encoding_helpers.tokenize_string(prompt)
        x = self.accessors.embed_tokens(tokens)
        _, io_accessors = self.accessors.run_model(x)

        tokenizer = self.encoding_helpers.tokenizer

        # The data tensor is going to be a stack of probabilities. Columns
        # correspond to tokens.
        #
        # Row index 0 is the probabilities from the input embedding.
        # Row index 1 + 2 * block_idx is the probabilities that result from
        #   adding block block_idx's self-attention output to its input.
        # Row index 1 + 2 * block_idx + 1 is the probabilities that result from
        #   adding block block_idx's feed-forward output to the previous result.
        #
        # Examples:
        #   self.data[0][tokenizer.stoi['a']] is the probability of the next
        #       token being 'a' given the input embedding.
        #   self.data[1][tokenizer.stoi['a']] is the probability of the next
        #       token being 'a' given the input embedding plus the self-attention
        #       output of the first block.
        #   self.data[2][tokenizer.stoi['a']] is the probability of the next
        #       token being 'a' given the input embedding plus the self-attention
        #       output of the first block plus the feed-forward output of the
        #       first block.
        #   self.data[3][tokenizer.stoi['a']] is the probability of the next
        #       token being 'a' given the output of the first block plus the
        #       self-attention output of the second block.
        self.data = torch.zeros(
            (1 + 2 * n_layer, tokenizer.vocab_size), dtype=torch.float32
        )
        self.data[0] = LogitsWrapper(
            self.accessors.logits_from_embedding(x), tokenizer
        ).probs()[0, -1]
        self.row_labels = ["Input"]
        for block_idx, io_accessor in enumerate(io_accessors):
            block_input = io_accessor.input(".")
            sa_output = io_accessor.output("sa")
            ffwd_output = io_accessor.output("ffwd")

            # The logic inside a block is:
            #   x = x + self.sa(self.ln1(x))
            #   x = x + self.ffwd(self.ln2(x))
            #
            # sa_adjusted_logits simulates the first line
            # and ffwd_adjusted_logits simulates the second line.

            sa_adjusted_logits = LogitsWrapper(
                self.accessors.logits_from_embedding(block_input + sa_output), tokenizer
            )
            self.data[self.idx_sa_probs(block_idx)] = sa_adjusted_logits.probs()[0, -1]
            self.row_labels.append(f"Block {block_idx} after SA")

            ffwd_adjusted_logits = LogitsWrapper(
                self.accessors.logits_from_embedding(
                    block_input + sa_output + ffwd_output
                ),
                tokenizer,
            )
            self.data[self.idx_ffwd_probs(block_idx)] = ffwd_adjusted_logits.probs()[
                0, -1
            ]
            self.row_labels.append(f"Block {block_idx} after FFWD")

    def idx_sa_probs(self, block_idx: int) -> int:
        """Returns the index into the data tensor containing the SA adjusted
        probabilities for the given block index."""
        return 1 + 2 * block_idx

    def idx_ffwd_probs(self, block_idx: int) -> int:
        """Returns the index into the data tensor containing the ffwd adjusted
        probabilities for the given block index."""
        return 1 + 2 * block_idx + 1

    def idx_input_probs(self, block_idx) -> int:
        """Returns the index into the data tensor containing the probabilities
        from the input to the given block index."""
        return self.idx_sa_probs(block_idx) - 1

    def plot(self):
        _, ax = plt.subplots(1, 1, figsize=(20, 12))
        self._plot(
            ax,
            self.data,
            self.row_labels,
            [repr(c)[1:-1] for c in self.encoding_helpers.tokenizer.chars],
        )

    def plot_subset(self, rows: Sequence[int] = [], cols: Sequence[int] = []):
        # If either rows or cols is empty, treat is as "all"
        if len(rows) == 0:
            rows = range(self.data.shape[0])
        if len(cols) == 0:
            cols = range(self.data.shape[1])

        data = torch.zeros((len(rows), len(cols)), dtype=self.data.dtype)
        for i, row in enumerate(rows):
            for j, col in enumerate(cols):
                data[i, j] = self.data[row, col]

        row_labels = [self.row_labels[row] for row in rows]
        col_labels = [
            repr(self.encoding_helpers.tokenizer.itos[col])[1:-1] for col in cols
        ]

        _, ax = plt.subplots(1, 1, figsize=(len(cols), len(rows)))
        self._plot(ax, data, row_labels, col_labels)

    def _plot(
        self,
        ax: Axes,
        data: torch.Tensor,
        row_labels: Sequence[str],
        col_labels: Sequence[str],
    ):
        im = ax.imshow(data, cmap="viridis")
        ax.set_xticks(range(len(col_labels)))
        ax.set_xticklabels(col_labels, rotation=90)

        ax.set_yticks(range(len(row_labels)))
        ax.set_yticklabels(row_labels)
        im.set_clim(0, 1.0)

        plt.colorbar(im, ax=[ax], location="top")
