# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/experiments/block-internals.ipynb.

# %% auto 0
__all__ = ['BlockInternalsResult', 'BlockInternalsExperiment', 'run', 'BlockInternalsAnalysis']

# %% ../../nbs/experiments/block-internals.ipynb 5
import argparse
from dataclasses import dataclass
import matplotlib.pyplot as plt
from pathlib import Path
import tempfile
from typing import Iterator, Tuple

# %% ../../nbs/experiments/block-internals.ipynb 6
import click
import torch
from tqdm.auto import tqdm

# %% ../../nbs/experiments/block-internals.ipynb 7
from ..common.databatcher import DataBatcher
from ..dataset_split import split_text_dataset
from transformer_experiments.datasets.tinyshakespeare import (
    TinyShakespeareDataSet,
)
from transformer_experiments.models.transformer import (
    n_embed,
    n_layer,
    TransformerLanguageModel,
)
from transformer_experiments.models.transformer_helpers import (
    EncodingHelpers,
    LogitsWrapper,
    TransformerAccessors,
)
from ..tokenizers.char_tokenizer import CharacterTokenizer
from transformer_experiments.trained_models.tinyshakespeare_transformer import (
    create_model_and_tokenizer,
)

# %% ../../nbs/experiments/block-internals.ipynb 8
@dataclass
class BlockInternalsResult:
    substring: str
    heads_output: torch.Tensor
    proj_output: torch.Tensor
    ffwd_output: torch.Tensor

# %% ../../nbs/experiments/block-internals.ipynb 9
class BlockInternalsExperiment:
    """An experiment to run a bunch of inputs through a block and save the
    intermediate values produced for each token."""

    def __init__(
        self,
        eh: EncodingHelpers,
        accessors: TransformerAccessors,
        block_idx: int,
        results_folder: Path,
    ):
        assert block_idx >= 0 and block_idx < n_layer

        self.eh = eh
        self.accessors = accessors
        self.block_idx = block_idx
        self.results_folder = results_folder

    def _filename_stem(self):
        return f"block{self.block_idx}_internals"

    def _input_filename(self, batch: int):
        return f"{self._filename_stem()}_input_{batch:03d}.pt"

    def _head_output_filename(self, batch: int):
        return f"{self._filename_stem()}_head_output_{batch:03d}.pt"

    def _proj_output_filename(self, batch: int):
        return f"{self._filename_stem()}_proj_output_{batch:03d}.pt"

    def _ffwd_output_filename(self, batch: int):
        return f"{self._filename_stem()}_ffwd_output_{batch:03d}.pt"

    def run(self, data_batcher: DataBatcher):
        for batch_idx, batch in tqdm(enumerate(data_batcher)):
            x = self.accessors.embed_tokens(batch)

            # Run the encoded batch through the blocks up to the one we're interested in
            for i in range(self.block_idx):
                x = self.accessors.m.blocks[i](x)

            # Copy the block we're interested in
            block, io_accessor = self.accessors.copy_block_from_model(
                block_idx=self.block_idx
            )
            _ = block(x)  # Run the block

            # Grab the outputs of interest
            heads_output = io_accessor.input("sa.proj")
            proj_output = io_accessor.output("sa.proj")
            ffwd_output = io_accessor.output("ffwd")

            torch.save(
                batch.clone(), self.results_folder / self._input_filename(batch_idx)
            )
            torch.save(
                heads_output,
                self.results_folder / self._head_output_filename(batch_idx),
            )
            torch.save(
                proj_output, self.results_folder / self._proj_output_filename(batch_idx)
            )
            torch.save(
                ffwd_output, self.results_folder / self._ffwd_output_filename(batch_idx)
            )

    def load(
        self,
    ) -> Iterator[Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]:
        input_files = sorted(
            self.results_folder.glob(f"{self._filename_stem()}_input_*.pt")
        )
        head_output_files = sorted(
            self.results_folder.glob(f"{self._filename_stem()}_head_output_*.pt")
        )
        proj_output_files = sorted(
            self.results_folder.glob(f"{self._filename_stem()}_proj_output_*.pt")
        )
        ffwd_output_files = sorted(
            self.results_folder.glob(f"{self._filename_stem()}_ffwd_output_*.pt")
        )

        assert (
            len(input_files)
            == len(head_output_files)
            == len(proj_output_files)
            == len(ffwd_output_files)
        )

        for input_file, head_output_file, proj_output_file, ffwd_output_file in zip(
            input_files, head_output_files, proj_output_files, ffwd_output_files
        ):
            assert input_file.exists()
            assert head_output_file.exists()
            assert proj_output_file.exists()
            assert ffwd_output_file.exists()

            yield torch.load(input_file), torch.load(head_output_file), torch.load(
                proj_output_file
            ), torch.load(ffwd_output_file)

    def raw_results(self) -> Iterator[BlockInternalsResult]:
        for inputs, head_output, proj_output, ffwd_output in self.load():
            n_samples, s_len = inputs.shape
            for i in range(n_samples):
                for j in range(s_len):
                    substring = self.eh.stringify_tokens(inputs[i][: j + 1])
                    yield BlockInternalsResult(
                        substring,
                        head_output[i][j],
                        proj_output[i][j],
                        ffwd_output[i][j],
                    )

# %% ../../nbs/experiments/block-internals.ipynb 13
@click.command()
@click.argument("model_weights_filename", type=click.Path(exists=True))
@click.argument("dataset_cache_filename", type=click.Path(exists=True))
@click.argument("output_folder", type=click.Path(exists=True))
@click.option(
    "-b",
    "--block_idx",
    required=True,
    type=click.IntRange(min=0, max=n_layer, max_open=True),
)
def run(
    model_weights_filename: str,
    dataset_cache_filename: str,
    output_folder: str,
    block_idx: int,
):
    click.echo(f"Running block internals experiment for block {block_idx}")

    # Instantiate the model, tokenizer, and dataset
    device = "cuda" if torch.cuda.is_available() else "cpu"
    click.echo(f"device is {device}")

    ts = TinyShakespeareDataSet(cache_file=dataset_cache_filename)
    m, tokenizer = create_model_and_tokenizer(
        saved_model_filename=model_weights_filename,
        dataset=ts,
        device=device,
    )
    _, val_data = split_text_dataset(ts.text, tokenizer, train_pct=0.9)

    encoding_helpers = EncodingHelpers(tokenizer, device)
    accessors = TransformerAccessors(m, device)

    # Create the experiment
    exp = BlockInternalsExperiment(
        encoding_helpers, accessors, block_idx, Path(output_folder)
    )

    # Run the experiment
    data_batcher = DataBatcher(
        data=val_data,
        sample_len=3,
        max_batch_size=64,
        stride=96,
    )
    exp.run(data_batcher=data_batcher)

# %% ../../nbs/experiments/block-internals.ipynb 14
class BlockInternalsAnalysis:
    """This class performs analysis of how the next token probabilities change
    as an embedded input is passed through each of the blocks in the model"""

    def __init__(
        self,
        accessors: TransformerAccessors,
        encoding_helpers: EncodingHelpers,
        prompt: str,
    ):
        self.accessors = accessors
        self.encoding_helpers = encoding_helpers
        self.prompt = prompt

        # Run the prompt through the model
        tokens = self.encoding_helpers.tokenize_string(prompt)
        x = self.accessors.embed_tokens(tokens)
        _, io_accessors = self.accessors.run_model(x)

        tokenizer = self.encoding_helpers.tokenizer

        # The data tensor is going to be a stack of probabilities. Columns
        # correspond to tokens.
        #
        # Row index 0 is the probabilities from the input embedding.
        # Row index 1 + 2 * block_idx is the probabilities that result from
        #   adding block block_idx's self-attention output to its input.
        # Row index 1 + 2 * block_idx + 1 is the probabilities that result from
        #   adding block block_idx's feed-forward output to the previous result.
        #
        # Examples:
        #   self.data[0][tokenizer.stoi['a']] is the probability of the next
        #       token being 'a' given the input embedding.
        #   self.data[1][tokenizer.stoi['a']] is the probability of the next
        #       token being 'a' given the input embedding plus the self-attention
        #       output of the first block.
        #   self.data[2][tokenizer.stoi['a']] is the probability of the next
        #       token being 'a' given the input embedding plus the self-attention
        #       output of the first block plus the feed-forward output of the
        #       first block.
        #   self.data[3][tokenizer.stoi['a']] is the probability of the next
        #       token being 'a' given the output of the first block plus the
        #       self-attention output of the second block.
        self.data = torch.zeros(
            (1 + 2 * n_layer, tokenizer.vocab_size), dtype=torch.float32
        )
        self.data[0] = LogitsWrapper(
            self.accessors.logits_from_embedding(x), tokenizer
        ).probs()[0, -1]
        self.row_labels = ["Input"]
        for block_idx, io_accessor in enumerate(io_accessors):
            block_input = io_accessor.input(".")
            sa_output = io_accessor.output("sa")
            ffwd_output = io_accessor.output("ffwd")

            # The logic inside a block is:
            #   x = x + self.sa(self.ln1(x))
            #   x = x + self.ffwd(self.ln2(x))
            #
            # sa_adjusted_logits simulates the first line
            # and ffwd_adjusted_logits simulates the second line.

            sa_adjusted_logits = LogitsWrapper(
                self.accessors.logits_from_embedding(block_input + sa_output), tokenizer
            )
            self.data[self.idx_sa_probs(block_idx)] = sa_adjusted_logits.probs()[0, -1]
            self.row_labels.append(f"Block {block_idx} after SA")

            ffwd_adjusted_logits = LogitsWrapper(
                self.accessors.logits_from_embedding(
                    block_input + sa_output + ffwd_output
                ),
                tokenizer,
            )
            self.data[self.idx_ffwd_probs(block_idx)] = ffwd_adjusted_logits.probs()[
                0, -1
            ]
            self.row_labels.append(f"Block {block_idx} after FFWD")

    def idx_sa_probs(self, block_idx: int) -> int:
        """Returns the index into the data tensor containing the SA adjusted
        probabilities for the given block index."""
        return 1 + 2 * block_idx

    def idx_ffwd_probs(self, block_idx: int) -> int:
        """Returns the index into the data tensor containing the ffwd adjusted
        probabilities for the given block index."""
        return 1 + 2 * block_idx + 1

    def idx_input_probs(self, block_idx) -> int:
        """Returns the index into the data tensor containing the probabilities
        from the input to the given block index."""
        return self.idx_sa_probs(block_idx) - 1

    def plot(self):
        _, ax = plt.subplots(1, 1, figsize=(20, 12))
        im = ax.imshow(self.data, cmap="viridis")
        ax.set_xticks(range(self.encoding_helpers.tokenizer.vocab_size))
        ax.set_xticklabels(
            [repr(c)[1:-1] for c in self.encoding_helpers.tokenizer.chars], rotation=90
        )

        ax.set_yticks(range(len(self.row_labels)))
        ax.set_yticklabels(self.row_labels)
        im.set_clim(0, 1.0)

        plt.colorbar(im, ax=[ax], location="top")
