# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/trained_models/tinyshakespeare-transformer.ipynb.

# %% auto 0
__all__ = ['create_model_and_tokenizer', 'FilenameForToken']

# %% ../../nbs/trained_models/tinyshakespeare-transformer.ipynb 5
from typing import Callable, Dict, Iterable, Tuple

# %% ../../nbs/trained_models/tinyshakespeare-transformer.ipynb 6
import torch

# %% ../../nbs/trained_models/tinyshakespeare-transformer.ipynb 7
from transformer_experiments.datasets.tinyshakespeare import (
    TinyShakespeareDataSet,
)
from ..models.transformer import TransformerLanguageModel
from transformer_experiments.tokenizers.char_tokenizer import (
    CharacterTokenizer,
)

# %% ../../nbs/trained_models/tinyshakespeare-transformer.ipynb 9
def create_model_and_tokenizer(
    saved_model_filename: str, dataset: TinyShakespeareDataSet, device: str
) -> Tuple[TransformerLanguageModel, CharacterTokenizer]:
    """Instantiates a pre-trained TinyShakespeare model: creates transformer model,
    loads the model params from a saved file, and creates a tokenizer from the dataset's text.
    """

    # Create a tokenizer from the dataset's text
    tokenizer = CharacterTokenizer(dataset.text)

    # Create the model
    m = TransformerLanguageModel(vocab_size=tokenizer.vocab_size, device=device)
    m.to(device)

    # Load the model params from a saved file
    m.load_state_dict(
        torch.load(saved_model_filename, map_location=torch.device(device))
    )
    m.eval()

    return m, tokenizer

# %% ../../nbs/trained_models/tinyshakespeare-transformer.ipynb 10
# Define names for special characters in the TinyShakespeare dataset
# that can be used as filenames.
special_char_names = {
    "\n": "newline",
    " ": "space",
    "!": "exclamation",
    "$": "dollar",
    "&": "ampersand",
    "'": "single_quote",
    ",": "comma",
    "-": "dash",
    ":": "colon",
    ";": "semicolon",
    ".": "period",
    "?": "question",
    "3": "three",
}

# %% ../../nbs/trained_models/tinyshakespeare-transformer.ipynb 11
class FilenameForToken:
    def __init__(self, tokenizer: CharacterTokenizer):
        self.tokenizer = tokenizer

    def __call__(self, token: str) -> str:
        """Given a character, returns a safe filename representing that character."""
        if token not in self.tokenizer.chars:
            raise ValueError(f"unknown character {token}")

        if token in special_char_names:
            return special_char_names[token]

        i = self.tokenizer.stoi[token]
        if i >= self.tokenizer.stoi["A"] and i <= self.tokenizer.stoi["Z"]:
            return f"capital_{token.lower()}"
        elif i >= self.tokenizer.stoi["a"] and i <= self.tokenizer.stoi["z"]:
            return f"lower_{token}"

        # Ensure that there is not some character in chars we didn't specifically handle.
        raise ValueError(f"unknown character {token}")
